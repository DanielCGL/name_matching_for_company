{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a34472-5d78-4765-b853-0938e5b16d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main Code\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "# A regular expression to match and replace certain symbols with spaces, including slashes, percentage signs, exclamations, punctuation, hyphens, and special symbols.\n",
    "ERASE_OR_REPLACE_TO_SPACE_SYMBOLS = re.compile(r'''[/\\\\%!！.?？^*·•⦁\\-]''')\n",
    "\n",
    "# A regular expression to match entity names that start with \"the\" followed by alphanumeric characters, case-insensitive.\n",
    "THE_SPECIAL_ENTITY_NAME_REGEX = re.compile(r'^(the\\s+[a-z0-9]+)$', re.IGNORECASE)\n",
    "\n",
    "# A regular expression to match sequences of single letters separated by spaces (e.g., \"a b c\") and find those patterns, case-insensitive.\n",
    "SINGLE_LETTER_SEQUENTIAL_REGEX = re.compile(rf'(\\b[a-z]\\b)(\\s+[a-z]\\b)+', re.IGNORECASE)\n",
    "\n",
    "# A regular expression to match single lowercase letters surrounded by spaces (e.g., \" a \").\n",
    "SINGLE_LOWERCASE_REGEX = re.compile(rf'\\s+[a-z]\\s+')\n",
    "\n",
    "# A regular expression to match and remove invalid symbols in company names, such as punctuation, special characters, and spaces around hyphens.\n",
    "REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "\n",
    "# A regular expression to match symbols that should be replaced with spaces, including ampersands, question marks, asterisks, parentheses, commas, and hyphens.\n",
    "REPLACE_TO_SPACE_SYMBOLS = re.compile(r'[&?#*|=_+()（）、,，–]')\n",
    "\n",
    "class NameNormalizer:\n",
    "    \"\"\"\n",
    "    This class handles the normalization of names, including circumflex letters and symbols.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pairing circumflex letters with their normalized counterparts\n",
    "    CIRCUMFLEX_PAIRS = (\n",
    "        (r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'),\n",
    "        (r'ðđ', 'd'), (r'ÐĐ', 'D'), (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'),\n",
    "        (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'), (r'Ł', 'L'), (r'ł', 'l'),\n",
    "        (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "        (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'),\n",
    "        (r'ŵ', 'w'), (r'ý', 'y'), (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE')\n",
    "    )\n",
    "\n",
    "    # Compile regex for circumflex replacements\n",
    "    CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "        (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize circumflex letters in the name, e.g., 'é' becomes 'e'.\n",
    "        If name is not a string, return an empty string.\n",
    "        \"\"\"\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # Replace circumflex letters using pre-compiled regex pairs\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class CompanyMatcher:\n",
    "    \"\"\"\n",
    "    This class handles company name matching using various techniques like tokenization, symbol regulation,\n",
    "    and name normalization.\n",
    "    \"\"\"\n",
    "    # Define common words for both English and Chinese\n",
    "    COMPANY_COMMON_WORDS = {\n",
    "        \"en\": {\n",
    "            \"laboratory\", \"laboratories\", \"lab\", \"labs\", \"research\", \"technology\", \"technologies\",\n",
    "            \"technical\", \"tech\", \"sci\", \"science\"\n",
    "        },\n",
    "        \"zh\": {\n",
    "            \"实验室\", \"研究院\", \"技术\", \"科技\"\n",
    "        }\n",
    "    }\n",
    "    COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "        r'\\b(bank|college|university|education|energy|finance|army|air foce|navy|industry|'\n",
    "        r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "        r'environmental|gover(?:nor?|ment)|state|estado|procter|reliance|genome|software|hardware|agency|'\n",
    "        r'(?:sou|nor)th(?:[-\\s]?(?:ea|we)st(?:ern)?)?|[东西南北]方|[东西][南北][方]?|环境|'\n",
    "        r'银行|软件|硬件|大学|机构|研究院|学院|教育|协会|能源|金融|旅馆|医院|石油|电子|工业|[陆海空]军|政府|基因|'\n",
    "        r'global|market(?:ing)?|real estate|system|房地产|系统|兴业)\\b',\n",
    "        re.IGNORECASE)\n",
    "\n",
    "    # Compile regex pattern for matching common words in company names\n",
    "    COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join(COMPANY_COMMON_WORDS['zh']),\n",
    "            rf\"(\\b|^)({'|'.join(COMPANY_COMMON_WORDS['en'])})(\\.|\\b|$)\"\n",
    "        ), re.I\n",
    "    )\n",
    "    # Define ranges for CJK (Chinese, Japanese, Korean) characters\n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "\n",
    "    # Regex to match CJK characters or numeric patterns\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(\n",
    "        rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\"\n",
    "    )\n",
    "\n",
    "    WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "\n",
    "    # Common prepositions\n",
    "    PREPOSITION_WORDS = {\n",
    "        'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as',\n",
    "        'at',\n",
    "        'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "        'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'following',\n",
    "        'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite',\n",
    "        'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'since', 'than', 'through', 'till', 'to',\n",
    "        'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within',\n",
    "        'without'\n",
    "    }\n",
    "\n",
    "    # Common conjunctions\n",
    "    CONJUNCTIONS_WORDS = {\n",
    "        \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "        \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "        \"such\", \"so\", \"that\"\n",
    "    }\n",
    "\n",
    "    # Common articles\n",
    "    ARTICLES = {'a', 'an', 'the'}\n",
    "\n",
    "    # Merge all irrelevant words (prepositions, conjunctions, articles, etc.)\n",
    "    INLINE_WORDS = {\n",
    "                       'also', 'am', 'are', 'did', 'furthermore', 'has', 'hence', 'how', 'however', 'includ.',\n",
    "                       'instead', 'is', 'likewise',\n",
    "                       'long', 'moreover', 'should', 'similar', 'though', 'thus', 'unless', 'was', 'were', 'what',\n",
    "                       'which', 'whichever',\n",
    "                       'why', 'will',\n",
    "                       # Portugese prepositions\n",
    "                       'de', 'di', 'em', 'del', 'des', 'do',\n",
    "                       # 意大利语介词\n",
    "                       'delle',\n",
    "                       # Spanish prepositions\n",
    "                       'con', 'sobre', 'en', 'contra', 'desde', 'entre', 'hacia', 'por', 'la',\n",
    "                       # French prepositions\n",
    "                       'apres', 'avant', 'avec', 'chez', 'contre', 'dans', 'depuis', 'derriere', 'devant', 'durant',\n",
    "                       'envers', 'environ',\n",
    "                       'jusque', 'malgre', 'par', 'parmi', 'pendant', 'pour', 'sans', 'selon', 'sous', 'suivant', 'sur',\n",
    "                       'vers'\n",
    "                   } | PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "    # Common suffixes in company names (e.g. Ltd, Company, etc.)\n",
    "    ORG_SUFFIX_WORDS = {\n",
    "        'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "               'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "        'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                          'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "               'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                            'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp',\n",
    "                            'sro',\n",
    "                            'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "    }\n",
    "    REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "    # Compile regular expressions for matching company suffixes\n",
    "    ORG_SUFFIX_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "            rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "        ),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    def __init__(self, company_data_file: str):\n",
    "        self.company_data_file = company_data_file\n",
    "        self.matcher = NameMatcher(remove_ascii=False, punctuations=False)\n",
    "        self.matching_data = None\n",
    "\n",
    "    def load_company_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load the company data from a JSON Lines (.jsonl) file into a pandas DataFrame.\n",
    "        Ensures aliases and required search strings are properly formatted.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: DataFrame containing company data including names and aliases.\n",
    "        \"\"\"\n",
    "        companies = []\n",
    "        with open(self.company_data_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                company = json.loads(line)\n",
    "                company['aliases'] = company.get('aliases', [])  # 确保别名存在\n",
    "                companies.append(company)\n",
    "        return pd.DataFrame(companies)\n",
    "\n",
    "    def tokenize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenize the input text using Jieba for Chinese words and return the tokenized string.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The text to tokenize (usually the company name).\n",
    "\n",
    "        Returns:\n",
    "        - str: Tokenized text.\n",
    "        \"\"\"\n",
    "        # Use jieba to segment the text and concatenate the results with spaces into a string and return it\n",
    "        return \" \".join(jieba.cut(text))\n",
    "\n",
    "    def search_string_regulator(self, name: str, return_str=True, is_company_name=False) -> str:\n",
    "        \"\"\"\n",
    "        Regulate the search string to remove meaningless words and symbols. Handles both Chinese and English.\n",
    "\n",
    "        :param name: The company name or string to regulate.\n",
    "        :param return_str: If True, return the result as a string; otherwise, return as a list of words.\n",
    "        :param is_company_name: Special flag to avoid removing certain common words for company names.\n",
    "        \"\"\"\n",
    "        # Remove meaningless parts of the company name\n",
    "        extracted_name = re.sub(r'[^\\w\\s]', '', name)  # Adjust this regex as per your needs\n",
    "        if not extracted_name:\n",
    "            return '' if return_str else []\n",
    "\n",
    "        words = []\n",
    "        # Split based on spaces or known separators\n",
    "        for segment in extracted_name.split():\n",
    "            if not segment:\n",
    "                continue\n",
    "            # If the segment is Chinese, treat it as a whole word\n",
    "            if self.contains_chinese(segment):\n",
    "                words.append(segment)\n",
    "            # If it's a meaningful English word, keep it\n",
    "            else:\n",
    "                segment = segment.lower()\n",
    "                if segment not in words or is_company_name:\n",
    "                    words.append(segment)\n",
    "\n",
    "        # Return as a single string or a list\n",
    "        return ' '.join(words) if return_str else words\n",
    "\n",
    "    def regulate_english_asians_mixed_string(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        This function handles the regulation of mixed English and Asian (CJK) strings.\n",
    "        It splits the input string into English, numbers, and CJK segments, then rejoins them.\n",
    "        \"\"\"\n",
    "        # 调用 split_english_number_cjk 方法并传入正确的参数\n",
    "        return ' '.join(self.split_english_number_cjk(text, separate_return=False))\n",
    "\n",
    "    def split_english_number_cjk(self, text: str, separate_return: bool = True):\n",
    "        \"\"\"\n",
    "        A utility function that splits text into English, numbers, and CJK (Chinese, Japanese, Korean) characters.\n",
    "        This is a basic version to simulate the behavior.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        CJK_REGEX = r'[\\u4e00-\\u9fff\\uf900-\\ufaff]+'  # Simplified regex to match CJK characters.\n",
    "        EN_NUM_REGEX = r'[a-zA-Z0-9]+'\n",
    "\n",
    "        cjk_parts = re.findall(CJK_REGEX, text)\n",
    "        en_num_parts = re.findall(EN_NUM_REGEX, text)\n",
    "\n",
    "        if separate_return:\n",
    "            return en_num_parts, cjk_parts  # Return both parts separately\n",
    "        else:\n",
    "            return en_num_parts + cjk_parts  # Concatenate the parts if not separating\n",
    "\n",
    "    def prepare_matching_data(self, companies: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare data for matching, ensuring aliases and required search strings are correctly loaded as individual entries.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for _, company in companies.iterrows():\n",
    "            # Process required search strings, ensure it's a list\n",
    "            required_search_strings = company.get(\"requiredSearchStrings\", [])\n",
    "            if not isinstance(required_search_strings, list):\n",
    "                required_search_strings = []  # If it's not a list, convert to empty list\n",
    "\n",
    "            # Ensure aliases are also a list\n",
    "            aliases = company.get(\"aliases\", [])\n",
    "            if not isinstance(aliases, list):\n",
    "                aliases = []  # If it's not a list, convert to empty list\n",
    "\n",
    "            # Combine required search strings and aliases into one list\n",
    "            full_alias_list = required_search_strings + aliases\n",
    "\n",
    "            # For each alias, create a separate row with all aliases as alias_name\n",
    "            for alias in full_alias_list:\n",
    "                rows.append({\n",
    "                    \"name\": alias,\n",
    "                    \"companyName\": company.get(\"companyName\", \"\"),\n",
    "                    \"alias_name\": full_alias_list,  # Store the full alias list\n",
    "                    \"is_alias\": alias in aliases  # Mark if this is an alias\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def prepare_data_for_matching(self, companies: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Prepare the company data for matching by tokenizing the names and loading them into the matcher.\n",
    "        \"\"\"\n",
    "        companies['name'] = companies['name'].apply(self.tokenize)  # Tokenize company names\n",
    "        self.matcher.load_and_process_master_data(column=\"name\", df_matching_data=companies)\n",
    "\n",
    "    def match_user_input(self, user_input: str, expected_name: str, threshold: int = 70) -> str:\n",
    "        \"\"\"\n",
    "        Match user input against the company names, then validate against expected name.\n",
    "        \"\"\"\n",
    "\n",
    "        # Input is NaN or non-string, convert it to an empty string\n",
    "        if not isinstance(user_input, str) or pd.isna(user_input):\n",
    "            user_input = ''\n",
    "\n",
    "        # Call interpunction_regulator to process the symbols entered by the user\n",
    "        user_input = self.interpunction_regulator(user_input)  # 处理符号\n",
    "        if not user_input:\n",
    "            return ''\n",
    "\n",
    "        # Normalize and tokenize the input\n",
    "        user_input_segmented = self.company_words_regulator(user_input)\n",
    "        user_input_segmented = \" \".join(jieba.cut(user_input_segmented))\n",
    "        to_be_matched = pd.DataFrame({\"name\": [user_input_segmented]})\n",
    "\n",
    "        # Matches the name entered by the user\n",
    "        result = self.matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "        # No match, returns an empty string\n",
    "        if result.empty:\n",
    "            return ''\n",
    "\n",
    "        # Get the best matching results\n",
    "        best_match = result.iloc[0][\"match_name\"]\n",
    "        match_score = result.iloc[0][\"score\"]\n",
    "\n",
    "        # If the match score exceeds the threshold, it is considered a successful match.\n",
    "        if match_score >= threshold:\n",
    "            # 通过 best_match 查找公司名称和别名\n",
    "            matched_row = self.matching_data[self.matching_data['name'] == best_match]\n",
    "            if matched_row.empty:\n",
    "                return ''\n",
    "\n",
    "            # 获取公司名称和别名信息\n",
    "            company_name = matched_row.iloc[0]['companyName']\n",
    "            aliases = matched_row.iloc[0]['alias_name']  # 这现在是一个包含所有别名的列表\n",
    "\n",
    "            # 处理 expected_name，防止 NaN 导致错误\n",
    "            if pd.isna(expected_name):\n",
    "                expected_name = \"\"  # 如果是 NaN，替换为空字符串\n",
    "            else:\n",
    "                expected_name = expected_name.lower()\n",
    "\n",
    "            # 检查 expected_name 是否在别名中\n",
    "            if expected_name and expected_name in [alias.lower() for alias in aliases]:\n",
    "                return expected_name\n",
    "\n",
    "            # 如果 expected_name 和 company_name 匹配，返回公司名称\n",
    "            if expected_name == company_name.lower():\n",
    "                return company_name\n",
    "\n",
    "            # 如果没有匹配到 expected_name，默认返回公司名称\n",
    "            return company_name\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_chinese(text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the input text contains any Chinese characters.\n",
    "        \"\"\"\n",
    "        CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "        return bool(re.search(f\"[{CHINESE_CHARACTERS}]\", text))\n",
    "\n",
    "    def company_words_regulator(self, company_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean up and regulate the company name by applying normalization rules for circumflexes, symbols,\n",
    "        removing unnecessary company suffixes, and common words.\n",
    "        \"\"\"\n",
    "        # Step 1: Normalize circumflex letters and process interpunction\n",
    "        search_str = self.interpunction_regulator(NameNormalizer.circumflex_regulator(company_name))\n",
    "\n",
    "        if not search_str:\n",
    "            return ''\n",
    "\n",
    "        # Step 2: Check if the string contains Chinese and process accordingly\n",
    "        if self.contains_chinese(search_str):\n",
    "            if not re.search(r\"[a-zA-Z]\", search_str):\n",
    "                search_str = search_str.replace(' ', '')\n",
    "\n",
    "        # Step 3: Remove company suffixes like 'inc', 'corp', etc., potentially twice to handle two occurrences\n",
    "        search_str_drop = self.WHITESPACE_REGEX.sub(' ', self.ORG_SUFFIX_REGEX.sub('', self.ORG_SUFFIX_REGEX.sub('',\n",
    "                                                                                                                 search_str).strip())).strip()\n",
    "\n",
    "        # The string becomes empty after removing the suffixes, return an empty string\n",
    "        if not search_str_drop:\n",
    "            return ''\n",
    "\n",
    "        # Step 4: Remove stock exchange abbreviations, e.g., 'SSE plc', 'NYSE', 'NASDAQ'\n",
    "        search_str_drop_bourse = self.COMPANY_COMMON_WORDS_REGEX.sub(' ', search_str_drop).strip()\n",
    "        if search_str_drop_bourse:\n",
    "            search_str_drop = search_str_drop_bourse\n",
    "\n",
    "        # Step 5: Remove common company words like 'technology', 'research', etc.\n",
    "        search_str_drop_common_word = self.WHITESPACE_REGEX.sub(' ', self.COMPANY_COMMON_WORDS_REGEX.sub('',\n",
    "                                                                                                         search_str_drop)).strip()\n",
    "        if search_str_drop_common_word:\n",
    "            search_str_drop = search_str_drop_common_word\n",
    "\n",
    "        # Step 6: Ensure that removing common words doesn't leave just a keyword, e.g., 'Corporation Bank'\n",
    "        if (r_ := self.COMPANY_KEYWORDS_REGEX.search(search_str_drop)) and r_.end() - r_.start() > len(\n",
    "                search_str_drop) - 2:\n",
    "            pass\n",
    "        elif search_str == search_str_drop:\n",
    "            pass\n",
    "        else:\n",
    "            search_str = search_str_drop\n",
    "\n",
    "        # Step 7: Further regulate the search string and convert to lowercase\n",
    "        search_str_regulate = self.search_string_regulator(search_str, is_company_name=True).lower()\n",
    "        if search_str_regulate != search_str:\n",
    "            search_str = search_str_regulate\n",
    "\n",
    "        # Step 8: Remove invalid characters and filter valid words\n",
    "        valid_words = []\n",
    "        for word_ in search_str.split(' '):\n",
    "            word_ = self.REGEX_COMPANY_DROP_INVALID.sub(' ', word_).strip().lower()\n",
    "            if word_:\n",
    "                valid_words.append(word_)\n",
    "\n",
    "        # Return the cleaned and processed company name\n",
    "        return ' '.join(valid_words)\n",
    "\n",
    "    # Direct deletion symbols: '-' and '/'\n",
    "    ERASE_SYMBOLS = re.compile(r'[-/]')\n",
    "\n",
    "\n",
    "\n",
    "    def interpunction_regulator(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Regulates punctuation and symbols in the company name by:\n",
    "        - Removing or replacing certain symbols with spaces.\n",
    "        - Replacing special characters like '&' and '@' with words ('togetherwith', 'locatedat').\n",
    "        - Handling special cases like sequential single letters and 'the' followed by a short word.\n",
    "        \"\"\"\n",
    "        is_chinese = self.contains_chinese(name)\n",
    "\n",
    "        if not is_chinese:\n",
    "            # If name contains specific symbols, split and take the first word for further matching\n",
    "            if name_words := re.split(r'[|]+', name):\n",
    "                name = name_words[0]\n",
    "\n",
    "        # Replace '&' with 'togetherwith' and '@' with 'locatedat'\n",
    "        if and_match := re.search(r'&', name):\n",
    "            name = name.replace('&', 'togetherwith')\n",
    "        if at_match := re.search(r'@', name):\n",
    "            name = name.replace('@', 'locatedat')\n",
    "\n",
    "        # Replace specific symbols with spaces\n",
    "        name = REPLACE_TO_SPACE_SYMBOLS.sub(' ', name)\n",
    "\n",
    "        # Use ERASE_OR_REPLACE_TO_SPACE_SYMBOLS instead of ERASE_SYMBOLS\n",
    "        if is_chinese:\n",
    "            name = ERASE_OR_REPLACE_TO_SPACE_SYMBOLS.sub(' ', name).strip()\n",
    "        else:\n",
    "            name = ERASE_OR_REPLACE_TO_SPACE_SYMBOLS.sub('', name).strip()\n",
    "\n",
    "        # Handle special cases for names starting with 'the' followed by a short word\n",
    "        if the_match := THE_SPECIAL_ENTITY_NAME_REGEX.search(name):\n",
    "            connect_str = the_match.group(1)\n",
    "            name = name.replace(connect_str, re.sub(r'[\\s]+', 'thespecial', connect_str))\n",
    "\n",
    "        # Merge sequential single letters into one\n",
    "        before_merge_name = name\n",
    "        name = ''\n",
    "        while True:\n",
    "            if not (single_letter_sequential := SINGLE_LETTER_SEQUENTIAL_REGEX.search(before_merge_name)):\n",
    "                name += before_merge_name.strip()\n",
    "                break\n",
    "            name += before_merge_name[\n",
    "                    :single_letter_sequential.start()].strip() + ' ' + single_letter_sequential.group().replace(' ',\n",
    "                                                                                                                '') + ' '\n",
    "            before_merge_name = before_merge_name[single_letter_sequential.end():]\n",
    "\n",
    "        # Drop single lowercase letters, but not if they are at the start or end\n",
    "        name = SINGLE_LOWERCASE_REGEX.sub(' ', name)\n",
    "\n",
    "        if is_chinese:\n",
    "            name = self.regulate_english_asians_mixed_string(name)\n",
    "\n",
    "        return name if name else None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to initialize CompanyMatcher, load company data, and perform company name matching.\n",
    "    \"\"\"\n",
    "    # Initialize CompanyMatcher and pass in the path to the company data file\n",
    "    company_matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "\n",
    "    # Load Company Data\n",
    "    companies = company_matcher.load_company_data()\n",
    "\n",
    "    # Prepare matching data and organize company name and alias information into a format that can be matched\n",
    "    matching_data = company_matcher.prepare_matching_data(companies)\n",
    "    company_matcher.matching_data = matching_data\n",
    "\n",
    "    # Prepare company data for matching (e.g. word segmentation)\n",
    "    company_matcher.prepare_data_for_matching(matching_data)\n",
    "\n",
    "    # Get input from the user and match the company name\n",
    "    user_input = input(\"Enter a company name for matching: \")\n",
    "\n",
    "    # Using a matcher to perform company name matching\n",
    "    matched_company = company_matcher.match_user_input(user_input, expected_name=None)\n",
    "\n",
    "    if matched_company:\n",
    "        print(f\"Matched company: {matched_company}\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b254584-7fa6-4255-9778-59e446dab157",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unittest\n",
    "#This is for people who want's to test multiple match_score instead of just one\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from name_matching_for_company import CompanyMatcher\n",
    "class TestCompanyMatcher(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up the CompanyMatcher and load the data once for all tests.\"\"\"\n",
    "        cls.matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "        cls.companies = cls.matcher.load_company_data()\n",
    "        cls.matcher.matching_data = cls.matcher.prepare_matching_data(cls.companies)\n",
    "        cls.matcher.prepare_data_for_matching(cls.matcher.matching_data)\n",
    "\n",
    "    def test_company_name_matching_dynamic_threshold(self):\n",
    "        \"\"\"Test the company name matching with dynamic threshold.\"\"\"\n",
    "        test_data = pd.read_csv('filtered_company_names.csv')\n",
    "\n",
    "        # Loop from match_score >= 0 to match_score >= 100\n",
    "        for threshold in range(0, 101):#This range can change to whatever you want\n",
    "            with self.subTest(threshold=threshold):  # Use unittest subTest for each threshold\n",
    "                # Output CSV file for this specific threshold\n",
    "                output_filename = f'company_name_matching_results_threshold_{threshold}.csv'\n",
    "\n",
    "                # Prepare output file\n",
    "                with open(output_filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "                    writer = csv.writer(output_file)\n",
    "                    writer.writerow(['company name', 'expected company name', 'output', 'true/false', 'time (seconds)'])\n",
    "\n",
    "                    # Iterate over test data\n",
    "                    for index, row in test_data.iterrows():\n",
    "                        input_name = row['company name']\n",
    "                        expected_name = row['expected company name']\n",
    "\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        # Call match_user_input with the dynamic threshold\n",
    "                        output_name = self.matcher.match_user_input(input_name, expected_name, threshold=threshold)\n",
    "\n",
    "                        end_time = time.time()\n",
    "                        elapsed_time = end_time - start_time\n",
    "\n",
    "                        # Ensure expected_name and output_name are strings\n",
    "                        expected_name = '' if pd.isna(expected_name) else expected_name\n",
    "                        output_name = '' if pd.isna(output_name) else output_name\n",
    "\n",
    "                        # Determine if the output matches the expected name\n",
    "                        result = output_name.lower() == expected_name.lower()\n",
    "\n",
    "                        # Write the result to the CSV file\n",
    "                        writer.writerow([input_name, expected_name, output_name, result, elapsed_time])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb92545-6808-4484-9ae1-f4d0b7fcca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#graph for precision vs. recall\n",
    "\"\"\"x is recall\n",
    "   y is precision\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Precision vs Recall\n",
    "data = pd.read_csv('calculate_score.csv')\n",
    "\n",
    "# 绘制 Precision vs Recall 的图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(data['Recall'], data['Precision'], marker='o')\n",
    "\n",
    "# 添加坐标轴标签和标题\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall')\n",
    "\n",
    "# 显示网格\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
