{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f8b42-7206-46c4-b4ea-267b00cbbd24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入公司名进行搜索:  apple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 421.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 106.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "匹配的公司名称: lapple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Code(1)\n",
    "#!pip install name_matching\n",
    "#!pip install jieba\n",
    "#!pip install cleanco\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "\n",
    "\n",
    "# 读取数据\n",
    "with open('bd_companies_international.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "df_companies_a = pd.DataFrame([{'Company name': item['companyName']} for item in data])\n",
    "\n",
    "df_companies_b = pd.DataFrame([{'name': item['requiredSearchStrings'][0] if 'requiredSearchStrings' in item and item['requiredSearchStrings'] else ''} for item in data])\n",
    "\n",
    "df_aliases = pd.DataFrame([{'aliases': item['aliases'][0] if 'aliases' in item and item['aliases'] else ''} for item in data])\n",
    "\n",
    "matcher = NameMatcher(number_of_matches=1, \n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "matcher.set_distance_metrics(['ssk', 'discounted_levenshtein', 'fuzzy_wuzzy_partial_string'])\n",
    "\n",
    "matcher.load_and_process_master_data(column='Company name',\n",
    "                                     df_matching_data=df_companies_a, \n",
    "                                     transform=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    # 删除所有大写字母并将其转换为小写\n",
    "    text = text.lower()\n",
    "    # 替换非 ASCII 字符\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    # 删除标点符号\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    # 使用 cleanco 删除合法商业后缀\n",
    "    text = basename(text)\n",
    "    # 删除最常见的单词（例如\"inc\", \"ltd\", 等等）\n",
    "    common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "    for word in common_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "    # 删除多余的空格\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "user_input = input(\"请输入公司名进行搜索: \")\n",
    "processed_input = preprocess(user_input)\n",
    "\n",
    "df_temp = pd.DataFrame([{'name': processed_input}])\n",
    "\n",
    "matches = matcher.match_names(to_be_matched=df_temp, \n",
    "                              column_matching='name')\n",
    "\n",
    "if not matches.empty:\n",
    "    match_index = matches.iloc[0]['match_index']\n",
    "    company_name = df_companies_a.iloc[match_index]['Company name']\n",
    "    print(f\"匹配的公司名称: {company_name}\")\n",
    "else:\n",
    "    print(\"未找到匹配的公司名称\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4768e128-14fa-40f7-91df-b9aff892c315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入公司名进行搜索:  Apple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 207.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 99.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "匹配的公司名称: lapple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Code(2)\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "\n",
    "# 首先，调用拆分代码生成CSV文件\n",
    "def parse_jsonl_to_csv(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(['Company Name', 'Search String'])  # 写入表头\n",
    "\n",
    "            for line in jsonl_file:\n",
    "                data = json.loads(line)  # 将每行的JSON字符串转换为字典\n",
    "                company_name = data.get('companyName', '')\n",
    "                required_search_strings = data.get('requiredSearchStrings', [])\n",
    "                aliases = data.get('aliases', [])\n",
    "\n",
    "                # 将requiredSearchStrings和aliases合并，并去除重复项\n",
    "                all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "                # 为每个字符串写入一行\n",
    "                for string in all_strings:\n",
    "                    csv_writer.writerow([company_name, string])\n",
    "\n",
    "# 调用拆分函数\n",
    "input_file_path = 'bd_companies_international.jsonl'\n",
    "output_csv_path = 'output_companies.csv'\n",
    "parse_jsonl_to_csv(input_file_path, output_csv_path)\n",
    "\n",
    "# 读取生成的CSV文件\n",
    "df_companies = pd.read_csv(output_csv_path)\n",
    "\n",
    "# 接下来，使用名称匹配代码进行匹配\n",
    "matcher = NameMatcher(number_of_matches=1, \n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "matcher.set_distance_metrics(['ssk', 'discounted_levenshtein', 'fuzzy_wuzzy_partial_string'])\n",
    "\n",
    "# 这里我们假设df_companies中的'Company Name'列是我们要匹配的主数据\n",
    "matcher.load_and_process_master_data(column='Company Name',\n",
    "                                     df_matching_data=df_companies, \n",
    "                                     transform=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = basename(text)\n",
    "    common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "    for word in common_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# 用户输入\n",
    "user_input = input(\"请输入公司名进行搜索: \")\n",
    "processed_input = preprocess(user_input)\n",
    "\n",
    "df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "\n",
    "matches = matcher.match_names(to_be_matched=df_temp, \n",
    "                              column_matching='Search String')\n",
    "\n",
    "if not matches.empty:\n",
    "    match_index = matches.iloc[0]['match_index']\n",
    "    company_name = df_companies.iloc[match_index]['Company Name']\n",
    "    print(f\"匹配的公司名称: {company_name}\")\n",
    "else:\n",
    "    print(\"未找到匹配的公司名称\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412e294-aab8-4691-8681-751837ae3c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been created at output_companies.csv\n"
     ]
    }
   ],
   "source": [
    "#调试将requiredSearchStrings和aliases拆分\n",
    "import json\n",
    "import csv\n",
    "\n",
    "# 假设你的JSONL文件路径是'bd_companies_international.jsonl'\n",
    "input_file_path = 'bd_companies_international.jsonl'\n",
    "output_csv_path = 'output_companies.csv'\n",
    "\n",
    "# 读取JSONL文件并解析数据\n",
    "def parse_jsonl_to_csv(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(['Company Name', 'Search String'])  # 写入表头\n",
    "\n",
    "            for line in jsonl_file:\n",
    "                data = json.loads(line)  # 将每行的JSON字符串转换为字典\n",
    "                company_name = data.get('companyName', '')\n",
    "                required_search_strings = data.get('requiredSearchStrings', [])\n",
    "                aliases = data.get('aliases', [])\n",
    "\n",
    "                # 将requiredSearchStrings和aliases合并，并去除重复项\n",
    "                all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "                # 为每个字符串写入一行\n",
    "                for string in all_strings:\n",
    "                    csv_writer.writerow([company_name, string])\n",
    "\n",
    "# 调用函数\n",
    "parse_jsonl_to_csv(input_file_path, output_csv_path)\n",
    "\n",
    "print(f'CSV file has been created at {output_csv_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef8abe-ec87-4f74-ab96-289a60351b1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入公司名进行搜索:  apple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 614.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 120.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "匹配的公司名称: lapple\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#通过翻译将中文转换英文再匹配（此方法不太可行）\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from translate import Translator\n",
    "\n",
    "# 读取jsonl文件\n",
    "with open('bd_companies_international.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# 定义一个函数，用于判断输入的是中文还是英文\n",
    "def is_chinese(input_string):\n",
    "    for ch in input_string:\n",
    "        if '\\u4e00' <= ch <= '\\u9fff':\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 定义一个函数，用于清理公司名称\n",
    "def clean_name(name):\n",
    "    # 去除标点符号和多余的空格\n",
    "    name = re.sub(r'[^\\w\\s]', '', name)\n",
    "    name = re.sub(r'\\s+', ' ', name)\n",
    "    return name.strip().lower()\n",
    "\n",
    "# 创建原始公司名称数据集\n",
    "df_companies_a = pd.DataFrame([{'Company name': item['companyName']} for item in data])\n",
    "\n",
    "# 创建经过修改的名称数据集\n",
    "df_companies_b = pd.DataFrame([{'name': item['requiredSearchStrings'][0] if 'requiredSearchStrings' in item and item['requiredSearchStrings'] else ''} for item in data])\n",
    "\n",
    "# 创建别名数据集\n",
    "df_aliases = pd.DataFrame([{'aliases': item['aliases'][0] if 'aliases' in item and item['aliases'] else ''} for item in data])\n",
    "\n",
    "# 初始化名称匹配器\n",
    "matcher = NameMatcher(number_of_matches=1, \n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "# 调整使用的匹配度量\n",
    "matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "\n",
    "# 加载需要匹配的数据\n",
    "matcher.load_and_process_master_data(column='Company name',\n",
    "                                     df_matching_data=df_companies_a, \n",
    "                                     transform=True)\n",
    "\n",
    "# 初始化翻译器\n",
    "translator = Translator(to_lang=\"en\", from_lang=\"zh\")\n",
    "\n",
    "# 接受用户输入的公司名称\n",
    "user_input = input(\"请输入公司名进行搜索: \")\n",
    "cleaned_user_input = clean_name(user_input)\n",
    "\n",
    "if is_chinese(user_input):\n",
    "    # 翻译中文输入为英文\n",
    "    translated_input = translator.translate(user_input)\n",
    "    cleaned_translated_input = clean_name(translated_input)\n",
    "    print(translated_input)\n",
    "    df_temp = pd.DataFrame([{'name': cleaned_translated_input}])\n",
    "    # 使用 NameMatcher 进行匹配\n",
    "    matches = matcher.match_names(to_be_matched=df_temp, column_matching='name')\n",
    "    if not matches.empty:\n",
    "        match_index = matches.iloc[0]['match_index']\n",
    "        company_name = df_companies_a.iloc[match_index]['Company name']\n",
    "        print(f\"匹配的公司名称: {company_name}\")\n",
    "    else:\n",
    "        print(\"未找到匹配的公司名称\")\n",
    "else:\n",
    "    # 使用 NameMatcher 匹配英文\n",
    "    df_temp = pd.DataFrame([{'name': cleaned_user_input}])\n",
    "    matches = matcher.match_names(to_be_matched=df_temp, column_matching='name')\n",
    "    if not matches.empty:\n",
    "        match_index = matches.iloc[0]['match_index']\n",
    "        company_name = df_companies_a.iloc[match_index]['Company name']\n",
    "        print(f\"匹配的公司名称: {company_name}\")\n",
    "    else:\n",
    "        print(\"未找到匹配的公司名称\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "341327f2-da75-41cb-8bad-9da2def48a9f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入公司名进行搜索:  All turki\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 235.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 50.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Matches DataFrame structure:\n",
      "  original_name match_name_0    score_0  match_index_0 match_name_1  \\\n",
      "0     all turki        altur  63.703704          48648     all tile   \n",
      "\n",
      "     score_1  match_index_1 match_name_2   score_2  match_index_2  ...  \\\n",
      "0  60.740741          45780   lug turkey  56.27322          68504  ...   \n",
      "\n",
      "  match_index_3  match_name_4    score_4 match_index_4  match_name_5  \\\n",
      "0          5293    bell trans  51.435329         44536      all care   \n",
      "\n",
      "    score_5 match_index_5  match_name_6    score_6 match_index_6  \n",
      "0  49.62963         45884  brisa turkey  47.222222         52189  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "Row missing expected columns: original_name       all turki\n",
      "match_name_0            altur\n",
      "score_0             63.703704\n",
      "match_index_0           48648\n",
      "match_name_1         all tile\n",
      "score_1             60.740741\n",
      "match_index_1           45780\n",
      "match_name_2       lug turkey\n",
      "score_2              56.27322\n",
      "match_index_2           68504\n",
      "match_name_3         digiturk\n",
      "score_3             51.799209\n",
      "match_index_3            5293\n",
      "match_name_4       bell trans\n",
      "score_4             51.435329\n",
      "match_index_4           44536\n",
      "match_name_5         all care\n",
      "score_5              49.62963\n",
      "match_index_5           45884\n",
      "match_name_6     brisa turkey\n",
      "score_6             47.222222\n",
      "match_index_6           52189\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#测试Matching概率(1)（仅供测试使用）\n",
    "import pandas as pd\n",
    "import json\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "# Load the data\n",
    "with open('bd_companies_international.jsonl', 'r', encoding='utf-8') as file:\n",
    "    data = [json.loads(line) for line in file]\n",
    "\n",
    "# Create dataframes\n",
    "df_companies_a = pd.DataFrame([{'Company name': item['companyName']} for item in data])\n",
    "df_companies_b = pd.DataFrame([{'name': item['requiredSearchStrings'][0] if 'requiredSearchStrings' in item and item['requiredSearchStrings'] else ''} for item in data])\n",
    "df_aliases = pd.DataFrame([{'aliases': item['aliases'][0] if 'aliases' in item and item['aliases'] else ''} for item in data])\n",
    "\n",
    "# Initialize NameMatcher with more matches\n",
    "matcher = NameMatcher(number_of_matches=7,  # Set this to the desired number of matches\n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "matcher.set_distance_metrics(['bag', 'typo', 'refined_soundex'])\n",
    "\n",
    "# Load and process master data\n",
    "matcher.load_and_process_master_data(column='Company name',\n",
    "                                     df_matching_data=df_companies_a, \n",
    "                                     transform=True)\n",
    "\n",
    "# Get user input\n",
    "user_input = input(\"请输入公司名进行搜索: \")\n",
    "df_temp = pd.DataFrame([{'name': user_input}])\n",
    "\n",
    "# Match names\n",
    "matches = matcher.match_names(to_be_matched=df_temp, \n",
    "                              column_matching='name')\n",
    "\n",
    "# Print the DataFrame structure for debugging\n",
    "print(\"Matches DataFrame structure:\")\n",
    "print(matches)\n",
    "\n",
    "# Output matches and their scores\n",
    "if not matches.empty:\n",
    "    for index, row in matches.iterrows():\n",
    "        if 'match_index' in row and 'matching_score' in row:\n",
    "            match_index = row['match_index']\n",
    "            company_name = df_companies_a.iloc[match_index]['Company name']\n",
    "            score = row['matching_score']\n",
    "            print(f\"匹配的公司名称: {company_name}, 匹配得分: {score}\")\n",
    "        else:\n",
    "            print(f\"Row missing expected columns: {row}\")\n",
    "else:\n",
    "    print(\"未找到匹配的公司名称\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8baa2fb-b7d6-4ff7-b501-a2e4234daac3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入公司名进行搜索:  apple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 299.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 110.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Matches DataFrame structure:\n",
      "  original_name match_name_0   score_0  match_index_0 match_name_1   score_1  \\\n",
      "0         apple       lapple  88.76547          72026       lapple  88.76547   \n",
      "\n",
      "   match_index_1 match_name_2    score_2  match_index_2  ... match_index_3  \\\n",
      "0          72027      appleby  84.822854          31093  ...         49665   \n",
      "\n",
      "   match_name_4    score_4 match_index_4  match_name_5    score_5  \\\n",
      "0       snapple  80.751086         97476     applebees  78.249021   \n",
      "\n",
      "  match_index_5  match_name_6  score_6 match_index_6  \n",
      "0         49666     apple inc    100.0         49648  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "Row missing expected columns: original_name        apple\n",
      "match_name_0        lapple\n",
      "score_0           88.76547\n",
      "match_index_0        72026\n",
      "match_name_1        lapple\n",
      "score_1           88.76547\n",
      "match_index_1        72027\n",
      "match_name_2       appleby\n",
      "score_2          84.822854\n",
      "match_index_2        31093\n",
      "match_name_3      appleone\n",
      "score_3          82.824844\n",
      "match_index_3        49665\n",
      "match_name_4       snapple\n",
      "score_4          80.751086\n",
      "match_index_4        97476\n",
      "match_name_5     applebees\n",
      "score_5          78.249021\n",
      "match_index_5        49666\n",
      "match_name_6     apple inc\n",
      "score_6              100.0\n",
      "match_index_6        49648\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#测试Matching概率(2)（仅供测试使用）(CSV版）\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "\n",
    "# 首先，调用拆分代码生成CSV文件\n",
    "def parse_jsonl_to_csv(input_path, output_path):\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        with open(output_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "            csv_writer = csv.writer(csv_file)\n",
    "            csv_writer.writerow(['Company Name', 'Search String'])  # 写入表头\n",
    "\n",
    "            for line in jsonl_file:\n",
    "                data = json.loads(line)  # 将每行的JSON字符串转换为字典\n",
    "                company_name = data.get('companyName', '')\n",
    "                required_search_strings = data.get('requiredSearchStrings', [])\n",
    "                aliases = data.get('aliases', [])\n",
    "\n",
    "                # 将requiredSearchStrings和aliases合并，并去除重复项\n",
    "                all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "                # 为每个字符串写入一行\n",
    "                for string in all_strings:\n",
    "                    csv_writer.writerow([company_name, string])\n",
    "\n",
    "# 调用拆分函数\n",
    "input_file_path = 'bd_companies_international.jsonl'\n",
    "output_csv_path = 'output_companies.csv'\n",
    "parse_jsonl_to_csv(input_file_path, output_csv_path)\n",
    "\n",
    "# 读取生成的CSV文件\n",
    "df_companies = pd.read_csv(output_csv_path)\n",
    "\n",
    "# 接下来，使用名称匹配代码进行匹配\n",
    "matcher = NameMatcher(number_of_matches=7, \n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "matcher.set_distance_metrics(['ssk', 'discounted_levenshtein', 'fuzzy_wuzzy_partial_string'])\n",
    "\n",
    "# 这里我们假设df_companies中的'Company Name'列是我们要匹配的主数据\n",
    "matcher.load_and_process_master_data(column='Company Name',\n",
    "                                     df_matching_data=df_companies, \n",
    "                                     transform=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = basename(text)\n",
    "    common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "    for word in common_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# 用户输入\n",
    "user_input = input(\"请输入公司名进行搜索: \")\n",
    "processed_input = preprocess(user_input)\n",
    "\n",
    "df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "\n",
    "matches = matcher.match_names(to_be_matched=df_temp, \n",
    "                              column_matching='Search String')\n",
    "\n",
    "# Print the DataFrame structure for debugging\n",
    "print(\"Matches DataFrame structure:\")\n",
    "print(matches)\n",
    "\n",
    "# Output matches and their scores\n",
    "if not matches.empty:\n",
    "    for index, row in matches.iterrows():\n",
    "        if 'match_index' in row and 'matching_score' in row:\n",
    "            match_index = row['match_index']\n",
    "            company_name = df_companies_a.iloc[match_index]['Company name']\n",
    "            score = row['matching_score']\n",
    "            print(f\"匹配的公司名称: {company_name}, 匹配得分: {score}\")\n",
    "        else:\n",
    "            print(f\"Row missing expected columns: {row}\")\n",
    "else:\n",
    "    print(\"未找到匹配的公司名称\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20eb5fe-272f-475c-8b8d-880160171814",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please enter the match name:  apple\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 189.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Matches DataFrame structure:\n",
      "  original_name match_name_0   score_0  match_index_0 match_name_1   score_1  \\\n",
      "0         apple       lapple  88.76547          72024       lapple  88.76547   \n",
      "\n",
      "   match_index_1 match_name_2    score_2  match_index_2  ... match_index_3  \\\n",
      "0          72025      appleby  84.822854          31091  ...         49663   \n",
      "\n",
      "   match_name_4    score_4 match_index_4  match_name_5    score_5  \\\n",
      "0       snapple  80.751086         97473     applebees  78.249021   \n",
      "\n",
      "  match_index_5  match_name_6  score_6 match_index_6  \n",
      "0         49664     apple inc    100.0         49646  \n",
      "\n",
      "[1 rows x 22 columns]\n",
      "Row missing expected columns: original_name        apple\n",
      "match_name_0        lapple\n",
      "score_0           88.76547\n",
      "match_index_0        72024\n",
      "match_name_1        lapple\n",
      "score_1           88.76547\n",
      "match_index_1        72025\n",
      "match_name_2       appleby\n",
      "score_2          84.822854\n",
      "match_index_2        31091\n",
      "match_name_3      appleone\n",
      "score_3          82.824844\n",
      "match_index_3        49663\n",
      "match_name_4       snapple\n",
      "score_4          80.751086\n",
      "match_index_4        97473\n",
      "match_name_5     applebees\n",
      "score_5          78.249021\n",
      "match_index_5        49664\n",
      "match_name_6     apple inc\n",
      "score_6              100.0\n",
      "match_index_6        49646\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#测试Matching概率(2)（仅供测试使用）(正式版）\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "\n",
    "# 直接处理 JSONL 文件并转换为 DataFrame,拆分alias和requiredsearchstrings\n",
    "def parse_jsonl_to_dataframe(input_path):\n",
    "    data_list = []\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            company_name = data.get('companyName', '')\n",
    "            required_search_strings = data.get('requiredSearchStrings', [])\n",
    "            aliases = data.get('aliases', [])\n",
    "\n",
    "            all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "            for string in all_strings:\n",
    "                data_list.append({'Company Name': company_name, 'Search String': string})\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    return df\n",
    "\n",
    "# 解析 JSONL 文件到 DataFrame\n",
    "input_file_path = 'bd_companies_international.jsonl'\n",
    "df_companies = parse_jsonl_to_dataframe(input_file_path)\n",
    "\n",
    "# 初始化名称匹配器\n",
    "matcher = NameMatcher(number_of_matches=7, \n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "matcher.set_distance_metrics(['ssk', 'discounted_levenshtein', 'fuzzy_wuzzy_partial_string'])\n",
    "\n",
    "# 加载并处理主数据\n",
    "matcher.load_and_process_master_data(column='Company Name',\n",
    "                                     df_matching_data=df_companies, \n",
    "                                     transform=True)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = basename(text)\n",
    "    common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "    for word in common_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# 用户输入\n",
    "user_input = input(\"Please enter the match name: \")\n",
    "processed_input = preprocess(user_input)\n",
    "\n",
    "df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "\n",
    "# 进行匹配\n",
    "matches = matcher.match_names(to_be_matched=df_temp, \n",
    "                              column_matching='Search String')\n",
    "\n",
    "# 调试：输出匹配结果的结构\n",
    "print(\"Matches DataFrame structure:\")\n",
    "print(matches)\n",
    "\n",
    "# 输出匹配结果和得分\n",
    "if not matches.empty:\n",
    "    for index, row in matches.iterrows():\n",
    "        if 'match_index' in row and 'matching_score' in row:\n",
    "            match_index = row['match_index']\n",
    "            company_name = df_companies.iloc[match_index]['Company Name']\n",
    "            score = row['matching_score']\n",
    "            print(f\"匹配的公司名称: {company_name}, 匹配得分: {score}\")\n",
    "        else:\n",
    "            print(f\"Row missing expected columns: {row}\")\n",
    "else:\n",
    "    print(\"未找到匹配的公司名称\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "0dea32a4-6aa2-4d3d-82c0-ca782e89ad95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入匹配的名称:  d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 804.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 87.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "匹配到的公司名称是: Union Square Hospitality Group\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#测试Matching概率(3)(Class版)\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "from fuzzychinese import FuzzyChineseMatch\n",
    "from rapidfuzz import process\n",
    "\n",
    "class CompanyNameMatcher:\n",
    "    def __init__(self, english_csv='english_search_strings.csv', chinese_csv='chinese_search_strings.csv'):\n",
    "        self.english_csv = english_csv\n",
    "        self.chinese_csv = chinese_csv\n",
    "        self.matcher = NameMatcher(number_of_matches=1, \n",
    "                                   legal_suffixes=True, \n",
    "                                   common_words=False, \n",
    "                                   top_n=50, \n",
    "                                   verbose=True)\n",
    "        self.fcm = FuzzyChineseMatch(ngram_range=(3, 3), analyzer='stroke')\n",
    "        \n",
    "    # 用于识别CJK字符的正则表达式 \n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\")\n",
    "    ENGLISH_SPLITTER_REGEX = re.compile(r\"[^\\w&_+*\\\\/'#\\-]+\")\n",
    "\n",
    "\n",
    "    # 分隔英文、数字和CJK字符的函数\n",
    "    @staticmethod\n",
    "    def split_english_number_cjk(text, separate_return=True, split_same_language=False):\n",
    "        en_number_words = []\n",
    "        cjk_words = []\n",
    "        start, end = 0, len(text)\n",
    "        while start < end and (r_ := CJK_OR_NUMERIC_REGEX.search(text, pos=start)):\n",
    "            if word := text[start:r_.start()].strip():\n",
    "                if split_same_language:\n",
    "                    en_number_words.extend(\n",
    "                        [stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(word) if (stripped_word := word_.strip())])\n",
    "                else:\n",
    "                    en_number_words.append(word)\n",
    "            if cjk_word := r_.groupdict().get(\"cjk\"):\n",
    "                if split_same_language:\n",
    "                    (cjk_words if separate_return else en_number_words).extend(list(cjk_word))\n",
    "                else:\n",
    "                    (cjk_words if separate_return else en_number_words).append(cjk_word)\n",
    "            else:\n",
    "                numeric_word = r_.groupdict().get(\"numeric\")\n",
    "                if split_same_language:\n",
    "                    en_number_words.extend([stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(numeric_word) if (stripped_word := word_.strip())])\n",
    "                else:\n",
    "                    en_number_words.append(numeric_word)\n",
    "            start = r_.end()\n",
    "\n",
    "        if end > start and (text := text[start:].strip()):\n",
    "            if split_same_language:\n",
    "                en_number_words.extend([word_ for word in ENGLISH_SPLITTER_REGEX.split(text) if (word_ := word.strip())])\n",
    "            else:\n",
    "                en_number_words.append(text)\n",
    "        return (en_number_words, cjk_words) if separate_return else en_number_words\n",
    "\n",
    "    # 解析JSONL文件并分隔英文和中文字符串的函数\n",
    "    @staticmethod\n",
    "    def parse_and_separate_jsonl(input_path):\n",
    "        english_list = []\n",
    "        chinese_list = []\n",
    "\n",
    "        with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "            for line in jsonl_file:\n",
    "                data = json.loads(line)\n",
    "                company_name = data.get('companyName', '')\n",
    "                required_search_strings = data.get('requiredSearchStrings', [])\n",
    "                aliases = data.get('aliases', [])\n",
    "\n",
    "                all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "                for string in all_strings:\n",
    "                    en_strings, cjk_strings = CompanyNameMatcher.split_english_number_cjk(string, separate_return=True)\n",
    "                    for en_str in en_strings:\n",
    "                        english_list.append({'Company Name': company_name, 'Search String': en_str})\n",
    "                    for cjk_str in cjk_strings:\n",
    "                        chinese_list.append({'Company Name': company_name, 'Search String': cjk_str})\n",
    "\n",
    "        df_english = pd.DataFrame(english_list)\n",
    "        df_chinese = pd.DataFrame(chinese_list)\n",
    "\n",
    "        return df_english, df_chinese\n",
    "\n",
    "    # 预处理定义\n",
    "    @staticmethod\n",
    "    def preprocess(text):\n",
    "        text = text.lower()\n",
    "        text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "        text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "        text = basename(text)\n",
    "        common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "        for word in common_words:\n",
    "            text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    # fuzzychinese\n",
    "    def find_company_name(self, user_input):\n",
    "        df = pd.read_csv(self.chinese_csv)\n",
    "\n",
    "        search_strings = df['Search String']\n",
    "        company_names = df['Company Name']\n",
    "\n",
    "        self.fcm.fit(search_strings)\n",
    "        user_input_series = pd.Series([user_input])\n",
    "        top_matches = self.fcm.transform(user_input_series, n=1)\n",
    "\n",
    "        matched_index = self.fcm.get_index()[0][0]\n",
    "        matched_company_name = company_names.iloc[matched_index]\n",
    "        return matched_company_name\n",
    "\n",
    "    # 定义一个函数来匹配输入的Search String\n",
    "    def match_company_name(self, input_string):\n",
    "        df = pd.read_csv(self.english_csv)\n",
    "\n",
    "        best_match = process.extractOne(input_string, df['Search String'])\n",
    "        if best_match:\n",
    "            matched_row = df[df['Search String'] == best_match[0]]\n",
    "            return matched_row['Company Name'].values[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # 加载并处理数据\n",
    "    def load_and_process_data(self):\n",
    "        df_english = pd.read_csv(self.english_csv)\n",
    "        self.matcher.load_and_process_master_data(column='Search String', df_matching_data=df_english, transform=True)\n",
    "\n",
    "        df_chinese = pd.read_csv(self.chinese_csv)\n",
    "        search_strings = df_chinese['Search String']\n",
    "        self.fcm.fit(search_strings)\n",
    "\n",
    "    # 匹配公司名称\n",
    "    def match_input(self, user_input):\n",
    "        if re.search(f'[{CJK_CHARACTERS}]', user_input):\n",
    "            matched_company_name = self.find_company_name(user_input)\n",
    "            print(f\"匹配到的公司名称是: {matched_company_name}\")\n",
    "        else:\n",
    "            processed_input = self.preprocess(user_input)\n",
    "            df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "            matches = self.matcher.match_names(to_be_matched=df_temp, column_matching='Search String')\n",
    "            input_string = ('\\n'.join(matches['match_name'].dropna().astype(str)))\n",
    "            company_name = self.match_company_name(input_string)\n",
    "\n",
    "            if company_name:\n",
    "                print(f\"匹配到的公司名称是: {company_name}\")\n",
    "            else:\n",
    "                print(\"没有匹配到相应的公司名称\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    input_file_path = 'bd_companies_international.jsonl'\n",
    "    matcher = CompanyNameMatcher()\n",
    "\n",
    "    df_english, df_chinese = CompanyNameMatcher.parse_and_separate_jsonl(input_file_path)\n",
    "    df_chinese.to_csv(matcher.chinese_csv, index=False, encoding='utf-8-sig')\n",
    "    df_english.to_csv(matcher.english_csv, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    matcher.load_and_process_data()\n",
    "\n",
    "    user_input = input(\"请输入匹配的名称: \")\n",
    "    matcher.match_input(user_input)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "ed6f710d-1cfe-4e25-ae93-cc96a4146275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入匹配的名称:  oppo us\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 250.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 70.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Matches DataFrame content:   original_name match_name     score  match_index\n",
      "0       oppo us   ovivo us  67.80303        99412\n",
      "匹配到的公司名称是: Ovivo US\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#测试Matching概率(3)\n",
    "#!pip install fuzzychinese\n",
    "#!pip install git+https://github.com/DanielCGL/name_matching_for_company.git\n",
    "#!pip install rapidfuzz\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "from fuzzychinese import FuzzyChineseMatch\n",
    "from rapidfuzz import process\n",
    "\n",
    "# 用于识别CJK字符的正则表达式 \n",
    "CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "CJK_OR_NUMERIC_REGEX = re.compile(rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\")\n",
    "\n",
    "ENGLISH_SPLITTER_REGEX = re.compile(r\"[^\\w&_+*\\\\/'#\\-]+\")\n",
    "\n",
    "# 分隔英文、数字和CJK字符的函数\n",
    "def split_english_number_cjk(text, separate_return=True, split_same_language=False):\n",
    "    en_number_words = []\n",
    "    cjk_words = []\n",
    "    start, end = 0, len(text)\n",
    "    while start < end and (r_ := CJK_OR_NUMERIC_REGEX.search(text, pos=start)):\n",
    "        if word := text[start:r_.start()].strip():\n",
    "            if split_same_language:\n",
    "                en_number_words.extend(\n",
    "                    [stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(word) if (stripped_word := word_.strip())])\n",
    "            else:\n",
    "                en_number_words.append(word)\n",
    "        if cjk_word := r_.groupdict().get(\"cjk\"):\n",
    "            if split_same_language:\n",
    "                (cjk_words if separate_return else en_number_words).extend(list(cjk_word))\n",
    "            else:\n",
    "                (cjk_words if separate_return else en_number_words).append(cjk_word)\n",
    "        else:\n",
    "            numeric_word = r_.groupdict().get(\"numeric\")\n",
    "            if split_same_language:\n",
    "                en_number_words.extend([stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(numeric_word) if (stripped_word := word_.strip())])\n",
    "            else:\n",
    "                en_number_words.append(numeric_word)\n",
    "        start = r_.end()\n",
    "\n",
    "    if end > start and (text := text[start:].strip()):\n",
    "        if split_same_language:\n",
    "            en_number_words.extend([word_ for word in ENGLISH_SPLITTER_REGEX.split(text) if (word_ := word.strip())])\n",
    "        else:\n",
    "            en_number_words.append(text)\n",
    "    return (en_number_words, cjk_words) if separate_return else en_number_words\n",
    "\n",
    "# 解析JSONL文件并分隔英文和中文字符串的函数\n",
    "def parse_and_separate_jsonl(input_path):\n",
    "    english_list = []\n",
    "    chinese_list = []\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            company_name = data.get('companyName', '')\n",
    "            required_search_strings = data.get('requiredSearchStrings', [])\n",
    "            aliases = data.get('aliases', [])\n",
    "\n",
    "            all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "            for string in all_strings:\n",
    "                en_strings, cjk_strings = split_english_number_cjk(string, separate_return=True)\n",
    "                for en_str in en_strings:\n",
    "                    english_list.append({'Company Name': company_name, 'Search String': en_str})\n",
    "                for cjk_str in cjk_strings:\n",
    "                    chinese_list.append({'Company Name': company_name, 'Search String': cjk_str})\n",
    "\n",
    "    df_english = pd.DataFrame(english_list)\n",
    "    df_chinese = pd.DataFrame(chinese_list)\n",
    "\n",
    "    return df_english, df_chinese\n",
    "\n",
    "# 解析JSONL并分隔英文和中文字符串\n",
    "input_file_path = 'bd_companies_international.jsonl'\n",
    "df_english, df_chinese = parse_and_separate_jsonl(input_file_path)\n",
    "\n",
    "# 保存中文字符串到CSV\n",
    "df_chinese.to_csv('chinese_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 保存英文字符串到CSV\n",
    "df_english.to_csv('english_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 初始化名称匹配器\n",
    "matcher = NameMatcher(number_of_matches=1, \n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "# 加载和处理拆分后的英文字符串数据\n",
    "df_english = pd.read_csv('english_search_strings.csv')\n",
    "matcher.load_and_process_master_data(column='Search String', df_matching_data=df_english, transform=True)\n",
    "\n",
    "# 加载和处理拆分后的中文字符串数据\n",
    "df_chinese = pd.read_csv('chinese_search_strings.csv')\n",
    "search_strings = df_chinese['Search String']\n",
    "company_names = df_chinese['Company Name']\n",
    "fcm = FuzzyChineseMatch(ngram_range=(3, 3), analyzer='stroke')\n",
    "fcm.fit(search_strings)\n",
    "\n",
    "#预处理定义\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "    text = basename(text)\n",
    "    common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "    for word in common_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "#fuzzychinese\n",
    "def find_company_name(user_input, csv_file='chinese_search_strings.csv'):\n",
    "    # Load your CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract the 'search string' and 'company name' columns\n",
    "    search_strings = df['Search String']\n",
    "    company_names = df['Company Name']\n",
    "\n",
    "    # Initialize FuzzyChineseMatch\n",
    "    fcm = FuzzyChineseMatch(ngram_range=(3, 3), analyzer='stroke')\n",
    "    fcm.fit(search_strings)\n",
    "\n",
    "    # Find the top matches for the user input\n",
    "    user_input_series = pd.Series([user_input])\n",
    "    top_matches = fcm.transform(user_input_series, n=1)  # Get the top 1 match\n",
    "\n",
    "    # Retrieve the matching company name\n",
    "    matched_index = fcm.get_index()[0][0]  # Index of the top match\n",
    "    matched_company_name = company_names.iloc[matched_index]\n",
    "    return matched_company_name\n",
    "\n",
    "# 定义一个函数来匹配输入的Search String\n",
    "def match_company_name(input_string, df):\n",
    "    df = pd.read_csv('english_search_strings.csv')\n",
    "    # 使用rapidfuzz的process来匹配最相近的Search String\n",
    "    best_match = process.extractOne(input_string, df['Search String'])\n",
    "    \n",
    "    # 获取匹配到的Search String对应的Company Name\n",
    "    if best_match:\n",
    "        matched_row = df[df['Search String'] == best_match[0]]\n",
    "        return matched_row['Company Name'].values[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "user_input = input(\"请输入匹配的名称: \")\n",
    "\n",
    "# 判断输入是否包含中文字符\n",
    "if re.search(f'[{CJK_CHARACTERS}]', user_input):# 如果输入包含中文字符，使用 FuzzyChineseMatch 进行匹配\n",
    "    \n",
    "    # Get the matched company name\n",
    "    matched_company_name = find_company_name(user_input)\n",
    "\n",
    "    # Output the result\n",
    "    print(f\"匹配到的公司名称是: {matched_company_name}\")\n",
    "else:# 如果输入不包含中文字符，使用 NameMatcher 进行匹配\n",
    "    processed_input = preprocess(user_input)\n",
    "    df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "    matches = matcher.match_names(to_be_matched=df_temp, column_matching='Search String')\n",
    "    # 输出 DataFrame 的列名和内容以进行调试\n",
    "    print(\"Matches DataFrame content:\", matches.head())\n",
    "    #将匹配的Search String匹配与其相关联的Company Name\n",
    "    input_string = ('\\n'.join(matches['match_name'].dropna().astype(str)))\n",
    "    company_name = match_company_name(input_string, df)\n",
    "\n",
    "    if company_name:\n",
    "        print(f\"匹配到的公司名称是: {company_name}\")\n",
    "    else:\n",
    "        print(\"没有匹配到相应的公司名称\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "b8950bf4-db0f-4926-aa91-c6c8f8bf5ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 602.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 72.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "没有匹配到相应的公司名称\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入匹配的名称:  j\n"
     ]
    }
   ],
   "source": [
    "#测试Matching概率(4)\n",
    "#!pip install fuzzychinese\n",
    "#!pip install git+https://github.com/DanielCGL/name_matching_for_company.git\n",
    "#!pip install rapidfuzz\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from deep_translator import GoogleTranslator\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "from fuzzychinese import FuzzyChineseMatch\n",
    "from rapidfuzz import process\n",
    "\n",
    "# 分隔英文、数字和CJK字符的函数\n",
    "def split_english_number_cjk(text, separate_return=True, split_same_language=False):\n",
    "    en_number_words = []\n",
    "    cjk_words = []\n",
    "    start, end = 0, len(text)\n",
    "    while start < end and (r_ := CJK_OR_NUMERIC_REGEX.search(text, pos=start)):\n",
    "        if word := text[start:r_.start()].strip():\n",
    "            if split_same_language:\n",
    "                en_number_words.extend(\n",
    "                    [stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(word) if (stripped_word := word_.strip())])\n",
    "            else:\n",
    "                en_number_words.append(word)\n",
    "        if cjk_word := r_.groupdict().get(\"cjk\"):\n",
    "            if split_same_language:\n",
    "                (cjk_words if separate_return else en_number_words).extend(list(cjk_word))\n",
    "            else:\n",
    "                (cjk_words if separate_return else en_number_words).append(cjk_word)\n",
    "        else:\n",
    "            numeric_word = r_.groupdict().get(\"numeric\")\n",
    "            if split_same_language:\n",
    "                en_number_words.extend([stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(numeric_word) if (stripped_word := word_.strip())])\n",
    "            else:\n",
    "                en_number_words.append(numeric_word)\n",
    "        start = r_.end()\n",
    "\n",
    "    if end > start and (text := text[start:].strip()):\n",
    "        if split_same_language:\n",
    "            en_number_words.extend([word_ for word in ENGLISH_SPLITTER_REGEX.split(text) if (word_ := word.strip())])\n",
    "        else:\n",
    "            en_number_words.append(text)\n",
    "    return (en_number_words, cjk_words) if separate_return else en_number_words\n",
    "\n",
    "# 清理中文字符串中不必要的空格\n",
    "def clean_chinese_strings(text):\n",
    "    # 匹配并删除中文字符之间的空格\n",
    "    cleaned_text = re.sub(r'(?<=[\\u4e00-\\u9fff])\\s+(?=[\\u4e00-\\u9fff])', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "# 解析JSONL文件并处理数据\n",
    "def parse_and_clean_jsonl(input_path, output_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            company_name = data.get('companyName', '')\n",
    "            required_search_strings = data.get('requiredSearchStrings', [])\n",
    "            aliases = data.get('aliases', [])\n",
    "\n",
    "            # 清理requiredSearchStrings中的中文字符串\n",
    "            cleaned_required_search_strings = [\n",
    "                clean_chinese_strings(string) if CJK_REGEX.search(string) else string\n",
    "                for string in required_search_strings\n",
    "            ]\n",
    "            \n",
    "            # 清理aliases中的中文字符串\n",
    "            cleaned_aliases = [\n",
    "                clean_chinese_strings(alias) if CJK_REGEX.search(alias) else alias\n",
    "                for alias in aliases\n",
    "            ]\n",
    "\n",
    "            # 更新数据字典\n",
    "            cleaned_data.append({\n",
    "                'requiredSearchStrings': cleaned_required_search_strings,\n",
    "                'aliases': cleaned_aliases,\n",
    "                'companyName': company_name\n",
    "            })\n",
    "\n",
    "    # 将清理后的数据写入新的JSONL文件\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        for item in cleaned_data:\n",
    "            output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# 输入文件路径\n",
    "input_file_path = 'bd_companies_international.jsonl'\n",
    "output_file_path = 'cleaned_bd_companies_international.jsonl'\n",
    "\n",
    "# 解析和清理JSONL文件\n",
    "parse_and_clean_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# 继续后续的代码...\n",
    "df_english, df_chinese = parse_and_separate_jsonl(output_file_path)\n",
    "\n",
    "# 保存中文字符串到CSV\n",
    "df_chinese.to_csv('chinese_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 保存英文字符串到CSV\n",
    "df_english.to_csv('english_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 初始化名称匹配器\n",
    "matcher = NameMatcher(number_of_matches=1, \n",
    "                      legal_suffixes=True, \n",
    "                      common_words=False, \n",
    "                      top_n=50, \n",
    "                      verbose=True)\n",
    "\n",
    "# 加载和处理拆分后的英文字符串数据\n",
    "df_english = pd.read_csv('english_search_strings.csv')\n",
    "matcher.load_and_process_master_data(column='Search String', df_matching_data=df_english, transform=True)\n",
    "\n",
    "# 加载和处理拆分后的中文字符串数据\n",
    "df_chinese = pd.read_csv('chinese_search_strings.csv')\n",
    "search_strings = df_chinese['Search String']\n",
    "company_names = df_chinese['Company Name']\n",
    "fcm = FuzzyChineseMatch(ngram_range=(3, 3), analyzer='stroke')\n",
    "fcm.fit(search_strings)\n",
    "\n",
    "#预处理定义\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Preprocesses the given text by performing the following operations:\n",
    "    1. Converts the text to lowercase.\n",
    "    2. Removes circumflex and other diacritical marks from characters.\n",
    "    3. Removes punctuation.\n",
    "    4. Removes common words that may not be necessary for processing.\n",
    "    5. Trims extra spaces and returns a clean, single-line string.\n",
    "\n",
    "    Parameters:\n",
    "    - text (str): The input text to preprocess.\n",
    "\n",
    "    Returns:\n",
    "    - str: The preprocessed text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove circumflex and other diacritical marks from characters\n",
    "    text = circumflex_regulator(text)\n",
    "\n",
    "    # Remove punctuation using a regular expression that matches all punctuation characters and replaces them with an empty string\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "\n",
    "    # Apply the 'basename' function to the text to remove certain prefixes or suffixes\n",
    "    text = basename(text)\n",
    "\n",
    "    # Define a list of common words to remove from the text\n",
    "    common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "\n",
    "    # Iterate over the list of common words and remove them from the text using a regular expression that matches whole words only\n",
    "    for word in common_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "\n",
    "    # Split the text into words and rejoin them with a single space to ensure there are no extra spaces in the final output\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Return the preprocessed text\n",
    "    return text\n",
    "\n",
    "\n",
    "# CIRCUMFLEX_PAIRS Usage:\n",
    "#    Remove circumflex from characters in search string\n",
    "#    mapping: 大小写声调字母 -> 标准英文字母\n",
    "CIRCUMFLEX_PAIRS = \\\n",
    "    ((r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'), (r'ðđ', 'd'), (r'ÐĐ', 'D'),\n",
    "     (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'), (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'),\n",
    "     (r'Ł', 'L'), (r'ł', 'l'), (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "     (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'), (r'ŵ', 'w'), (r'ý', 'y'),\n",
    "     (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE'))\n",
    "CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "    (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    ")\n",
    "\n",
    "# Format circumflex to regulate English characters\n",
    "def circumflex_regulator(name: str):\n",
    "    if not name:\n",
    "        return ''\n",
    "    for regex_, format_character in CIRCUMFLEX_REGEX_PAIRS:\n",
    "        name = regex_.sub(format_character, name)\n",
    "    return name\n",
    "\n",
    "\n",
    "#fuzzychinese\n",
    "def find_company_name(user_input, csv_file='chinese_search_strings.csv'):\n",
    "    # Load your CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Extract the 'search string' and 'company name' columns\n",
    "    search_strings = df['Search String']\n",
    "    company_names = df['Company Name']\n",
    "\n",
    "    # Initialize FuzzyChineseMatch\n",
    "    fcm = FuzzyChineseMatch(ngram_range=(3, 3), analyzer='char')\n",
    "    fcm.fit(search_strings)\n",
    "\n",
    "    # Find the top matches for the user input\n",
    "    user_input_series = pd.Series([user_input])\n",
    "    top_matches = fcm.transform(user_input_series, n=1)  # Get the top 1 match\n",
    "\n",
    "    # Retrieve the matching company name\n",
    "    matched_index = fcm.get_index()[0][0]  # Index of the top match\n",
    "    matched_company_name = company_names.iloc[matched_index]\n",
    "    return matched_company_name\n",
    "\n",
    "# 定义一个函数来匹配输入的Search String\n",
    "def match_company_name(input_string, df):\n",
    "    df = pd.read_csv('english_search_strings.csv')\n",
    "    # 使用rapidfuzz的process来匹配最相近的Search String\n",
    "    best_match = process.extractOne(input_string, df['Search String'])\n",
    "    \n",
    "    # 获取匹配到的Search String对应的Company Name\n",
    "    if best_match:\n",
    "        matched_row = df[df['Search String'] == best_match[0]]\n",
    "        return matched_row['Company Name'].values[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# 定义需要移除的中文标点符号\n",
    "CHINESE_PUNCTUATION = r'[！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.]'\n",
    "\n",
    "user_input = input(\"请输入匹配的名称: \")\n",
    "\n",
    "if any(CJK_REGEX.match(char) for char in user_input):  # 包含中文字符\n",
    "    # 清理中文标点符号\n",
    "    user_input = re.sub(CHINESE_PUNCTUATION, '', user_input)\n",
    "    \n",
    "    if all(CJK_REGEX.match(char) for char in user_input):  # 全部是中文字符\n",
    "        matched_company_name = find_company_name(user_input)\n",
    "        \n",
    "        if matched_company_name:\n",
    "            print(f\"匹配到的公司名称是: {matched_company_name}\")\n",
    "        else:\n",
    "            print(\"没有匹配到相应的公司名称\")\n",
    "    else:  # 混合中文和英文字符，或包含其他外文字符\n",
    "        translated_input = GoogleTranslator(source='auto', target='en').translate(user_input)\n",
    "        processed_input = preprocess(translated_input)\n",
    "        df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "        matches = matcher.match_names(to_be_matched=df_temp, column_matching='Search String')\n",
    "        input_string = ('\\n'.join(matches['match_name'].dropna().astype(str)))\n",
    "        \n",
    "        company_name = match_company_name(input_string, df_english)\n",
    "        \n",
    "        input_score = ('\\n'.join(matches['score'].dropna().astype(str)))\n",
    "        input_score = float(input_score)\n",
    "\n",
    "        if input_score >= 85:\n",
    "            print(f\"匹配到的公司名称是: {company_name}\")\n",
    "        else:\n",
    "            print(\"没有匹配到相应的公司名称\")\n",
    "else:  # 全部是英文字符\n",
    "    processed_input = preprocess(user_input)\n",
    "    df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "    matches = matcher.match_names(to_be_matched=df_temp, column_matching='Search String')\n",
    "    input_string = ('\\n'.join(matches['match_name'].dropna().astype(str)))\n",
    "    \n",
    "    company_name = match_company_name(input_string, df_english)\n",
    "    \n",
    "    input_score = ('\\n'.join(matches['score'].dropna().astype(str)))\n",
    "    input_score = float(input_score)\n",
    "    if input_score >= 85:\n",
    "        print(f\"匹配到的公司名称是: {company_name}\")\n",
    "    else:\n",
    "        print(\"没有匹配到相应的公司名称\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc8e78e4-6663-485f-b6f5-f011e5da5dfd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入匹配的名称:  网易(杭州)网络有限公司\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 122.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "没有匹配到相应的公司名称\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 测试Matching概率(5)\n",
    "# !pip install fuzzychinese\n",
    "# !pip install git+https://github.com/DanielCGL/name_matching_for_company.git\n",
    "# !pip install rapidfuzz\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from deep_translator import GoogleTranslator\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "from fuzzychinese import FuzzyChineseMatch\n",
    "from rapidfuzz import process\n",
    "\n",
    "CJK_REGEX = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "CJK_OR_NUMERIC_REGEX = re.compile(r'(?P<cjk>[\\u4E00-\\u9FFF]+)|(?P<numeric>[0-9]+)')\n",
    "ENGLISH_SPLITTER_REGEX = re.compile(r'\\s+')\n",
    "\n",
    "# 分隔英文、数字和CJK字符的函数\n",
    "def split_english_number_cjk(text, separate_return=True, split_same_language=False):\n",
    "    en_number_words = []\n",
    "    cjk_words = []\n",
    "    start, end = 0, len(text)\n",
    "    while start < end and (r_ := CJK_OR_NUMERIC_REGEX.search(text, pos=start)):\n",
    "        if word := text[start:r_.start()].strip():\n",
    "            en_number_words.append(word)\n",
    "        if cjk_word := r_.groupdict().get(\"cjk\"):\n",
    "            cjk_words.append(cjk_word)\n",
    "        else:\n",
    "            numeric_word = r_.groupdict().get(\"numeric\")\n",
    "            en_number_words.append(numeric_word)\n",
    "        start = r_.end()\n",
    "\n",
    "    if end > start and (text := text[start:].strip()):\n",
    "        en_number_words.append(text)\n",
    "    return (en_number_words, cjk_words) if separate_return else en_number_words\n",
    "\n",
    "\n",
    "\n",
    "# 清理中文字符串中不必要的空格\n",
    "def clean_chinese_strings(text):\n",
    "    # 匹配并删除中文字符之间的空格\n",
    "    cleaned_text = re.sub(r'(?<=[\\u4e00-\\u9fff])\\s+(?=[\\u4e00-\\u9fff])', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "# 解析JSONL文件并处理数据\n",
    "def parse_and_clean_jsonl(input_path, output_path):\n",
    "    cleaned_data = []\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            company_name = data.get('companyName', '')\n",
    "            required_search_strings = data.get('requiredSearchStrings', [])\n",
    "            aliases = data.get('aliases', [])\n",
    "\n",
    "            # 清理requiredSearchStrings中的中文字符串\n",
    "            cleaned_required_search_strings = [\n",
    "                clean_chinese_strings(string) if CJK_REGEX.search(string) else string\n",
    "                for string in required_search_strings\n",
    "            ]\n",
    "\n",
    "            # 清理aliases中的中文字符串\n",
    "            cleaned_aliases = [\n",
    "                clean_chinese_strings(alias) if CJK_REGEX.search(alias) else alias\n",
    "                for alias in aliases\n",
    "            ]\n",
    "\n",
    "            # 更新数据字典\n",
    "            cleaned_data.append({\n",
    "                'requiredSearchStrings': cleaned_required_search_strings,\n",
    "                'aliases': cleaned_aliases,\n",
    "                'companyName': company_name\n",
    "            })\n",
    "\n",
    "    # 将清理后的数据写入新的JSONL文件\n",
    "    with open(output_path, 'w', encoding='utf-8') as output_file:\n",
    "        for item in cleaned_data:\n",
    "            output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "# 使用修改后的函数进行解析\n",
    "def parse_and_separate_jsonl(input_path):\n",
    "    english_list = []\n",
    "    chinese_list = []\n",
    "\n",
    "    with open(input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "        for line in jsonl_file:\n",
    "            data = json.loads(line)\n",
    "            company_name = data.get('companyName', '')\n",
    "            required_search_strings = data.get('requiredSearchStrings', [])\n",
    "            aliases = data.get('aliases', [])\n",
    "\n",
    "            all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "            for string in all_strings:\n",
    "                en_strings, cjk_strings = split_english_number_cjk(string, separate_return=True, split_same_language=False)\n",
    "                for en_str in en_strings:\n",
    "                    english_list.append({'Company Name': company_name, 'Search String': en_str})\n",
    "                for cjk_str in cjk_strings:\n",
    "                    chinese_list.append({'Company Name': company_name, 'Search String': cjk_str})\n",
    "\n",
    "    df_english = pd.DataFrame(english_list)\n",
    "    df_chinese = pd.DataFrame(chinese_list)\n",
    "\n",
    "    return df_english, df_chinese\n",
    "\n",
    "\n",
    "# 输入文件路径\n",
    "input_file_path = 'bd_companies_international.jsonl'\n",
    "output_file_path = 'cleaned_bd_companies_international.jsonl'\n",
    "\n",
    "# 解析和清理JSONL文件\n",
    "parse_and_clean_jsonl(input_file_path, output_file_path)\n",
    "\n",
    "# 继续后续的代码...\n",
    "df_english, df_chinese = parse_and_separate_jsonl(output_file_path)\n",
    "\n",
    "# 保存中文字符串到CSV\n",
    "df_chinese.to_csv('chinese_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 保存英文字符串到CSV\n",
    "df_english.to_csv('english_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "# 初始化名称匹配器\n",
    "matcher = NameMatcher(number_of_matches=1,\n",
    "                      legal_suffixes=True,\n",
    "                      common_words=False,\n",
    "                      top_n=50,\n",
    "                      verbose=True)\n",
    "\n",
    "# 加载和处理拆分后的英文字符串数据\n",
    "df_english = pd.read_csv('english_search_strings.csv')\n",
    "matcher.load_and_process_master_data(column='Search String', df_matching_data=df_english, transform=True)\n",
    "\n",
    "# 加载和处理拆分后的中文字符串数据\n",
    "df_chinese = pd.read_csv('chinese_search_strings.csv')\n",
    "search_strings = df_chinese['Search String']\n",
    "company_names = df_chinese['Company Name']\n",
    "\n",
    "\n",
    "# 预处理定义\n",
    "def preprocess(text):\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove circumflex and other diacritical marks from characters\n",
    "    text = circumflex_regulator(text)\n",
    "\n",
    "    # Remove punctuation using a regular expression that matches all punctuation characters and replaces them with an empty string\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "\n",
    "    # Apply the 'basename' function to the text to remove certain prefixes or suffixes\n",
    "    text = basename(text)\n",
    "\n",
    "    # Define a list of common words to remove from the text\n",
    "    common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company']\n",
    "\n",
    "    # Iterate over the list of common words and remove them from the text using a regular expression that matches whole words only\n",
    "    for word in common_words:\n",
    "        text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "\n",
    "    # Split the text into words and rejoin them with a single space to ensure there are no extra spaces in the final output\n",
    "    text = ' '.join(text.split())\n",
    "\n",
    "    # Return the preprocessed text\n",
    "    return text\n",
    "\n",
    "\n",
    "# CIRCUMFLEX_PAIRS Usage:\n",
    "#    Remove circumflex from characters in search string\n",
    "#    mapping: 大小写声调字母 -> 标准英文字母\n",
    "CIRCUMFLEX_PAIRS = \\\n",
    "    ((r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'), (r'ðđ', 'd'), (r'ÐĐ', 'D'),\n",
    "     (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'), (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'),\n",
    "     (r'Ł', 'L'), (r'ł', 'l'), (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "     (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'), (r'ŵ', 'w'), (r'ý', 'y'),\n",
    "     (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE'))\n",
    "CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "    (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    ")\n",
    "\n",
    "\n",
    "# Format circumflex to regulate English characters\n",
    "def circumflex_regulator(name: str):\n",
    "    if not name:\n",
    "        return ''\n",
    "    for regex_, format_character in CIRCUMFLEX_REGEX_PAIRS:\n",
    "        name = regex_.sub(format_character, name)\n",
    "    return name\n",
    "\n",
    "\n",
    "# fuzzychinese\n",
    "def find_company_name(user_input, csv_file='chinese_search_strings.csv'):\n",
    "    # 加载CSV文件\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # 提取'search string' 和 'company name' 列\n",
    "    search_strings = df['Search String']\n",
    "    company_names = df['Company Name']\n",
    "\n",
    "    # 尝试不同的ngram_range\n",
    "    fcm = FuzzyChineseMatch(ngram_range=(2, 3), analyzer='char')\n",
    "    fcm.fit(search_strings)\n",
    "\n",
    "    # 找到用户输入的最佳匹配\n",
    "    user_input_series = pd.Series([user_input])\n",
    "    top_matches = fcm.transform(user_input_series, n=1)  # 获取前1个匹配\n",
    "\n",
    "    # 获取最佳匹配的索引\n",
    "    matched_index = fcm.get_index()[0][0]\n",
    "    matched_company_name = company_names.iloc[matched_index]\n",
    "\n",
    "    # 获取匹配分数\n",
    "    similarity_scores = fcm.get_similarity_score()\n",
    "\n",
    "    # 获取第一个匹配的分数\n",
    "    top1_score = similarity_scores[0][0]\n",
    "\n",
    "    return matched_company_name, top1_score\n",
    "\n",
    "\n",
    "# 定义一个函数来匹配输入的Search String\n",
    "def match_company_name(input_string, df):\n",
    "    df = pd.read_csv('english_search_strings.csv')\n",
    "    # 使用rapidfuzz的process来匹配最相近的Search String\n",
    "    best_match = process.extractOne(input_string, df['Search String'])\n",
    "\n",
    "    # 获取匹配到的Search String对应的Company Name\n",
    "    if best_match:\n",
    "        matched_row = df[df['Search String'] == best_match[0]]\n",
    "        return matched_row['Company Name'].values[0]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 定义需要移除的中文标点符号\n",
    "CHINESE_PUNCTUATION = r'[！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.]'\n",
    "user_input = input(\"请输入匹配的名称: \")\n",
    "if any(CJK_REGEX.match(char) for char in user_input):  # 包含中文字符\n",
    "    # 清理中文标点符号\n",
    "    user_input = re.sub(CHINESE_PUNCTUATION, '', user_input)\n",
    "\n",
    "    if all(CJK_REGEX.match(char) for char in user_input):  # 全部是中文字符\n",
    "        matched_company_name, top1_score = find_company_name(user_input)\n",
    "\n",
    "        if top1_score >= 0.6:\n",
    "            print(f\"匹配到的公司名称是: {matched_company_name}, 匹配得分是: {top1_score}\")\n",
    "        else:\n",
    "            print(\"没有匹配到相应的公司名称\")\n",
    "    else:  # 混合中文和英文字符，或包含其他外文字符\n",
    "        translated_input = GoogleTranslator(source='auto', target='en').translate(user_input)\n",
    "        processed_input = preprocess(translated_input)\n",
    "        df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "        matches = matcher.match_names(to_be_matched=df_temp, column_matching='Search String')\n",
    "        input_string = ('\\n'.join(matches['match_name'].dropna().astype(str)))\n",
    "\n",
    "        company_name = match_company_name(input_string, df_english)\n",
    "\n",
    "        input_score = ('\\n'.join(matches['score'].dropna().astype(str)))\n",
    "        input_score = float(input_score)\n",
    "\n",
    "        if input_score >= 85:\n",
    "            print(f\"匹配到的公司名称是: {company_name}\")\n",
    "        else:\n",
    "            print(\"没有匹配到相应的公司名称\")\n",
    "else:  # 全部是英文字符\n",
    "    processed_input = preprocess(user_input)\n",
    "    df_temp = pd.DataFrame([{'Search String': processed_input}])\n",
    "    matches = matcher.match_names(to_be_matched=df_temp, column_matching='Search String')\n",
    "    input_string = ('\\n'.join(matches['match_name'].dropna().astype(str)))\n",
    "\n",
    "    company_name = match_company_name(input_string, df_english)\n",
    "\n",
    "    input_score = ('\\n'.join(matches['score'].dropna().astype(str)))\n",
    "    input_score = float(input_score)\n",
    "    if input_score >= 85:\n",
    "        print(f\"匹配到的公司名称是: {company_name}\")\n",
    "    else:\n",
    "        print(\"没有匹配到相应的公司名称\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "199a2a14-9537-4701-8ed1-fb63b5772839",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "请输入匹配的名称:  HP Enterprise Services\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n",
      "\n",
      "preprocessing complete \n",
      " searching for matches...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 115.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "possible matches found   \n",
      " fuzzy matching...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "匹配到的公司名称是: BFG Enterprise Services\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Object-Oriented Programming Usage V5\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from deep_translator import GoogleTranslator\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from cleanco import basename\n",
    "from fuzzychinese import FuzzyChineseMatch\n",
    "from rapidfuzz import process\n",
    "\n",
    "CJK_REGEX = re.compile(r'[\\u4e00-\\u9fff]')\n",
    "CJK_OR_NUMERIC_REGEX = re.compile(r'(?P<cjk>[\\u4E00-\\u9FFF]+)|(?P<numeric>[0-9]+)')\n",
    "ENGLISH_SPLITTER_REGEX = re.compile(r'\\s+')\n",
    "\n",
    "CIRCUMFLEX_PAIRS = \\\n",
    "    ((r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'), (r'ðđ', 'd'), (r'ÐĐ', 'D'),\n",
    "     (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'), (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'),\n",
    "     (r'Ł', 'L'), (r'ł', 'l'), (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "     (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'), (r'ŵ', 'w'), (r'ý', 'y'),\n",
    "     (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE'))\n",
    "CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "    (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    ")\n",
    "\n",
    "CHINESE_PUNCTUATION = r'[！？｡＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.]'\n",
    "\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, input_path, output_path):\n",
    "        self.input_path = input_path\n",
    "        self.output_path = output_path\n",
    "\n",
    "    def clean_chinese_strings(self, text):\n",
    "        return re.sub(r'(?<=[\\u4e00-\\u9fff])\\s+(?=[\\u4e00-\\u9fff])', '', text)\n",
    "\n",
    "    def parse_and_clean_jsonl(self):\n",
    "        cleaned_data = []\n",
    "        with open(self.input_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "            for line in jsonl_file:\n",
    "                data = json.loads(line)\n",
    "                company_name = data.get('companyName', '')\n",
    "                required_search_strings = data.get('requiredSearchStrings', [])\n",
    "                aliases = data.get('aliases', [])\n",
    "\n",
    "                cleaned_required_search_strings = [\n",
    "                    self.clean_chinese_strings(string) if CJK_REGEX.search(string) else string\n",
    "                    for string in required_search_strings\n",
    "                ]\n",
    "                cleaned_aliases = [\n",
    "                    self.clean_chinese_strings(alias) if CJK_REGEX.search(alias) else alias\n",
    "                    for alias in aliases\n",
    "                ]\n",
    "\n",
    "                cleaned_data.append({\n",
    "                    'requiredSearchStrings': cleaned_required_search_strings,\n",
    "                    'aliases': cleaned_aliases,\n",
    "                    'companyName': company_name\n",
    "                })\n",
    "\n",
    "        with open(self.output_path, 'w', encoding='utf-8') as output_file:\n",
    "            for item in cleaned_data:\n",
    "                output_file.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "    def split_english_number_cjk(self, text, separate_return=True):\n",
    "        en_number_words = []\n",
    "        cjk_words = []\n",
    "        start, end = 0, len(text)\n",
    "        while start < end and (r_ := CJK_OR_NUMERIC_REGEX.search(text, pos=start)):\n",
    "            if word := text[start:r_.start()].strip():\n",
    "                en_number_words.append(word)\n",
    "            if cjk_word := r_.groupdict().get(\"cjk\"):\n",
    "                cjk_words.append(cjk_word)\n",
    "            else:\n",
    "                numeric_word = r_.groupdict().get(\"numeric\")\n",
    "                en_number_words.append(numeric_word)\n",
    "            start = r_.end()\n",
    "\n",
    "        if end > start and (text := text[start:].strip()):\n",
    "            en_number_words.append(text)\n",
    "        return (en_number_words, cjk_words) if separate_return else en_number_words\n",
    "\n",
    "    def parse_and_separate_jsonl(self):\n",
    "        english_list = []\n",
    "        chinese_list = []\n",
    "        with open(self.output_path, 'r', encoding='utf-8') as jsonl_file:\n",
    "            for line in jsonl_file:\n",
    "                data = json.loads(line)\n",
    "                company_name = data.get('companyName', '')\n",
    "                required_search_strings = data.get('requiredSearchStrings', [])\n",
    "                aliases = data.get('aliases', [])\n",
    "\n",
    "                all_strings = list(set(required_search_strings + aliases))\n",
    "\n",
    "                for string in all_strings:\n",
    "                    en_strings, cjk_strings = self.split_english_number_cjk(string, separate_return=True)\n",
    "                    for en_str in en_strings:\n",
    "                        english_list.append({'Company Name': company_name, 'Search String': en_str})\n",
    "                    for cjk_str in cjk_strings:\n",
    "                        chinese_list.append({'Company Name': company_name, 'Search String': cjk_str})\n",
    "\n",
    "        df_english = pd.DataFrame(english_list)\n",
    "        df_chinese = pd.DataFrame(chinese_list)\n",
    "\n",
    "        return df_english, df_chinese\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def preprocess(self, text):\n",
    "        text = text.lower()\n",
    "        text = self.circumflex_regulator(text)\n",
    "        text = re.sub(f'[{re.escape(string.punctuation)}]', '', text)\n",
    "        text = basename(text)\n",
    "        common_words = ['inc', 'ltd', 'corp', 'llc', 'co', 'company','group']\n",
    "        for word in common_words:\n",
    "            text = re.sub(r'\\b{}\\b'.format(re.escape(word)), '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "\n",
    "    def circumflex_regulator(self, name: str):\n",
    "        if not name:\n",
    "            return ''\n",
    "        for regex_, format_character in CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class NameMatcherManager:\n",
    "    def __init__(self):\n",
    "        self.matcher = NameMatcher(number_of_matches=1,\n",
    "                                   legal_suffixes=True,\n",
    "                                   common_words=False,\n",
    "                                   top_n=50,\n",
    "                                   verbose=True)\n",
    "\n",
    "    def load_and_process_data(self, df_english):\n",
    "        self.matcher.load_and_process_master_data(column='Search String', df_matching_data=df_english, transform=True)\n",
    "\n",
    "    def match_names(self, input_string, df_english):\n",
    "        df_temp = pd.DataFrame([{'Search String': input_string}])\n",
    "        matches = self.matcher.match_names(to_be_matched=df_temp, column_matching='Search String')\n",
    "        input_string = ('\\n'.join(matches['match_name'].dropna().astype(str)))\n",
    "        company_name = self.match_company_name(input_string, df_english)\n",
    "        input_score = ('\\n'.join(matches['score'].dropna().astype(str)))\n",
    "        input_score = float(input_score)\n",
    "        return company_name, input_score\n",
    "\n",
    "    def match_company_name(self, input_string, df):\n",
    "        best_match = process.extractOne(input_string, df['Search String'])\n",
    "        if best_match:\n",
    "            matched_row = df[df['Search String'] == best_match[0]]\n",
    "            return matched_row['Company Name'].values[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "\n",
    "class FuzzyMatcher:\n",
    "    def __init__(self, csv_file='chinese_search_strings.csv'):\n",
    "        self.csv_file = csv_file\n",
    "        self.fcm = FuzzyChineseMatch(ngram_range=(2, 3), analyzer='char')\n",
    "\n",
    "    def find_company_name(self, user_input):\n",
    "        df = pd.read_csv(self.csv_file)\n",
    "        search_strings = df['Search String']\n",
    "        company_names = df['Company Name']\n",
    "        self.fcm.fit(search_strings)\n",
    "        user_input_series = pd.Series([user_input])\n",
    "        top_matches = self.fcm.transform(user_input_series, n=1)\n",
    "        matched_index = self.fcm.get_index()[0][0]\n",
    "        matched_company_name = company_names.iloc[matched_index]\n",
    "        similarity_scores = self.fcm.get_similarity_score()\n",
    "        top1_score = similarity_scores[0][0]\n",
    "        return matched_company_name, top1_score\n",
    "\n",
    "\n",
    "class CompanyNameMatcher:\n",
    "    def __init__(self, input_file_path, output_file_path):\n",
    "        self.data_processor = DataProcessor(input_file_path, output_file_path)\n",
    "        self.text_preprocessor = TextPreprocessor()\n",
    "        self.name_matcher_manager = NameMatcherManager()\n",
    "        self.fuzzy_matcher = FuzzyMatcher()\n",
    "\n",
    "    def run(self):\n",
    "        # 解析和清理数据\n",
    "        self.data_processor.parse_and_clean_jsonl()\n",
    "        df_english, df_chinese = self.data_processor.parse_and_separate_jsonl()\n",
    "        df_chinese.to_csv('chinese_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "        df_english.to_csv('english_search_strings.csv', index=False, encoding='utf-8-sig')\n",
    "\n",
    "        # 加载和处理数据\n",
    "        self.name_matcher_manager.load_and_process_data(df_english)\n",
    "\n",
    "        # 输入匹配\n",
    "        user_input = input(\"请输入匹配的名称: \")\n",
    "        if any(CJK_REGEX.match(char) for char in user_input):  # 包含中文字符\n",
    "            user_input = re.sub(CHINESE_PUNCTUATION, '', user_input)\n",
    "            if all(CJK_REGEX.match(char) for char in user_input):  # 全部是中文字符\n",
    "                matched_company_name, top1_score = self.fuzzy_matcher.find_company_name(user_input)\n",
    "                if top1_score >= 0.6:\n",
    "                    print(f\"匹配到的公司名称是: {matched_company_name}, 匹配得分是: {top1_score}\")\n",
    "                else:\n",
    "                    print(\"没有匹配到相应的公司名称\")\n",
    "            else:  # 混合中文和英文字符，或包含其他外文字符\n",
    "                translated_input = GoogleTranslator(source='auto', target='en').translate(user_input)\n",
    "                processed_input = self.text_preprocessor.preprocess(translated_input)\n",
    "                company_name, input_score = self.name_matcher_manager.match_names(processed_input, df_english)\n",
    "                if input_score >= 85:\n",
    "                    print(f\"匹配到的公司名称是: {company_name}\")\n",
    "                else:\n",
    "                    print(\"没有匹配到相应的公司名称\")\n",
    "        else:  # 全部是英文字符\n",
    "            processed_input = self.text_preprocessor.preprocess(user_input)\n",
    "            company_name, input_score = self.name_matcher_manager.match_names(processed_input, df_english)\n",
    "            if input_score >= 85:\n",
    "                print(f\"匹配到的公司名称是: {company_name}\")\n",
    "            else:\n",
    "                print(\"没有匹配到相应的公司名称\")\n",
    "\n",
    "\n",
    "# 执行匹配过程\n",
    "if __name__ == \"__main__\":\n",
    "    input_file_path = 'bd_companies_international.jsonl'\n",
    "    output_file_path = 'cleaned_bd_companies_international.jsonl'\n",
    "    \n",
    "    # 创建CompanyNameMatcher实例\n",
    "    matcher = CompanyNameMatcher(input_file_path, output_file_path)\n",
    "    \n",
    "    # 运行匹配过程\n",
    "    matcher.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f546552-cab3-4049-a293-4a403fc0d7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#以下为只用Name Matching匹配，忽略FuzzyCHinese\n",
    "#V6\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "# 定义中文分词函数\n",
    "def chinese_tokenizer(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "# 加载JSONL文件\n",
    "def load_company_data(file_path):\n",
    "    companies = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            companies.append(json.loads(line))\n",
    "    return companies\n",
    "\n",
    "# 将数据整理为DataFrame格式，便于name_matching处理\n",
    "def prepare_matching_data(companies):\n",
    "    rows = []\n",
    "    for company in companies:\n",
    "        for search_string in company.get(\"requiredSearchStrings\", []):\n",
    "            rows.append({\"name\": search_string, \"companyName\": company[\"companyName\"]})\n",
    "        for alias in company.get(\"aliases\", []):\n",
    "            rows.append({\"name\": alias, \"companyName\": company[\"companyName\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 匹配用户输入\n",
    "def match_user_input(user_input, matching_data):\n",
    "    matcher = NameMatcher(remove_ascii=False, punctuations=False)  # 保留中文和标点符号\n",
    "\n",
    "    # 分词处理输入数据和匹配数据\n",
    "    matching_data['name'] = matching_data['name'].apply(chinese_tokenizer)\n",
    "    matcher.load_and_process_master_data(column=\"name\", df_matching_data=matching_data)\n",
    "\n",
    "    # 将用户输入进行分词处理\n",
    "    user_input_processed = chinese_tokenizer(user_input)\n",
    "\n",
    "    # 匹配用户输入\n",
    "    to_be_matched = pd.DataFrame({\"name\": [user_input_processed]})\n",
    "    result = matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "    if not result.empty:\n",
    "        # 返回匹配到的公司名称\n",
    "        best_match = result.iloc[0][\"match_name\"]  # 取第一个匹配结果\n",
    "        company_name = matching_data[matching_data['name'] == best_match][\"companyName\"].values[0]\n",
    "        return company_name\n",
    "    return None\n",
    "\n",
    "# 主函数\n",
    "def main(user_input):\n",
    "    # 加载公司数据\n",
    "    companies = load_company_data('bd_companies_international.jsonl')\n",
    "    \n",
    "    # 准备数据供name_matching使用\n",
    "    matching_data = prepare_matching_data(companies)\n",
    "    \n",
    "    # 匹配用户输入并返回公司名称\n",
    "    company_name = match_user_input(user_input, matching_data)\n",
    "    \n",
    "    if company_name:\n",
    "        print(f\"匹配到的公司名称是: {company_name}\")\n",
    "    else:\n",
    "        print(\"未匹配到相关公司\")\n",
    "\n",
    "# 用户输入\n",
    "user_input = \"samsung\"\n",
    "main(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6c3bc-4480-4acf-a769-38149ac209b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V7\n",
    "#Add a Match_Score\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "# 定义中文分词函数\n",
    "def chinese_tokenizer(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "# 加载JSONL文件\n",
    "def load_company_data(file_path):\n",
    "    companies = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            companies.append(json.loads(line))\n",
    "    return companies\n",
    "\n",
    "# 将数据整理为DataFrame格式，便于name_matching处理\n",
    "def prepare_matching_data(companies):\n",
    "    rows = []\n",
    "    for company in companies:\n",
    "        for search_string in company.get(\"requiredSearchStrings\", []):\n",
    "            rows.append({\"name\": search_string, \"companyName\": company[\"companyName\"]})\n",
    "        for alias in company.get(\"aliases\", []):\n",
    "            rows.append({\"name\": alias, \"companyName\": company[\"companyName\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 匹配用户输入\n",
    "def match_user_input(user_input, matching_data):\n",
    "    matcher = NameMatcher(remove_ascii=False, punctuations=False)  # 保留中文和标点符号\n",
    "\n",
    "    # 分词处理输入数据和匹配数据\n",
    "    matching_data['name'] = matching_data['name'].apply(chinese_tokenizer)\n",
    "    matcher.load_and_process_master_data(column=\"name\", df_matching_data=matching_data)\n",
    "\n",
    "    # 将用户输入进行分词处理\n",
    "    user_input_processed = chinese_tokenizer(user_input)\n",
    "\n",
    "    # 匹配用户输入\n",
    "    to_be_matched = pd.DataFrame({\"name\": [user_input_processed]})\n",
    "    result = matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "    if not result.empty:\n",
    "        # 获取第一个匹配结果的得分\n",
    "        best_match = result.iloc[0][\"match_name\"]  # 取第一个匹配结果\n",
    "        match_score = result.iloc[0][\"score\"]  # 获取第一个匹配的得分\n",
    "\n",
    "        # 检查得分是否达到60%\n",
    "        if match_score >= 60:\n",
    "            company_name = matching_data[matching_data['name'] == best_match][\"companyName\"].values[0]\n",
    "            return company_name\n",
    "        else:\n",
    "            return \"未找到相关公司\"  \n",
    "    return \"未找到相关公司\"\n",
    "\n",
    "# 主函数\n",
    "def main(user_input):\n",
    "    # 加载公司数据\n",
    "    companies = load_company_data('bd_companies_international.jsonl')\n",
    "    \n",
    "    # 准备数据供name_matching使用\n",
    "    matching_data = prepare_matching_data(companies)\n",
    "    \n",
    "    # 匹配用户输入并返回公司名称\n",
    "    company_name = match_user_input(user_input, matching_data)\n",
    "    \n",
    "    print(company_name)\n",
    "\n",
    "# 用户输入\n",
    "user_input = input(\"Enter a company name:\")\n",
    "main(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ea20a3-4c37-4039-97c8-833df28c6ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V8\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "from functools import partial\n",
    "\n",
    "AND_SYMBOL_REGEX = re.compile(r'[a-z0-9](&)[a-z0-9]', re.IGNORECASE)\n",
    "AT_SYMBOL_REGEX = re.compile(r'[a-z0-9]+(@)[a-z0-9]+', re.IGNORECASE)\n",
    "\n",
    "# CIRCUMFLEX_PAIRS Usage:\n",
    "#    Remove circumflex from characters in search string\n",
    "#    mapping: 大小写声调字母 -> 标准英文字母\n",
    "CIRCUMFLEX_PAIRS = \\\n",
    "    ((r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'), (r'ðđ', 'd'), (r'ÐĐ', 'D'),\n",
    "     (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'), (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'),\n",
    "     (r'Ł', 'L'), (r'ł', 'l'), (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "     (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'), (r'ŵ', 'w'), (r'ý', 'y'),\n",
    "     (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE'))\n",
    "CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "    (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    ")\n",
    "\n",
    "ENGLISH_SPLITTER_REGEX = re.compile(r\"[^\\w&_+*\\\\/'#\\-]+\")\n",
    "CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "CJK_OR_NUMERIC_REGEX = re.compile(rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\")\n",
    "\n",
    "ENTITY_WORDS_SPLITTER = re.compile('[%s]+' % re.escape('!\"%\\'*<=>?@^_`~－ \\t\\n'))\n",
    "# 保留 meaning word(\\w) 数字 下划线\n",
    "MEANINGLESS_COLLEGE_COMPANY_NAME_REGEX = re.compile(\n",
    "    rf\"[^0-9\\w\\s]|^其[他它她]$|^others?$|^n\\.?a\\.?$|^n/a$|^none$|^null$\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "ERASE_OR_REPLACE_TO_SPACE_SYMBOLS = re.compile(r'''[/\\\\%!！.?？^*·•⦁\\-]''')\n",
    "REPLACE_TO_SPACE_SYMBOLS = re.compile(r'&?#.*|(\\s+|^)[+&@](\\s+|$)|\\s*[_+()（）、,，–\\-]\\s*')\n",
    "THE_SPECIAL_ENTITY_NAME_REGEX = re.compile(r'^(the\\s+[a-z0-9]+)$', re.IGNORECASE)\n",
    "SINGLE_LETTER_SEQUENTIAL_REGEX = re.compile(rf'(\\b[a-z]\\b)(\\s+[a-z]\\b)+', re.IGNORECASE)\n",
    "SINGLE_LOWERCASE_REGEX = re.compile(rf'\\s+[a-z]\\s+')\n",
    "REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "\n",
    "PREPOSITION_WORDS = {\n",
    "    'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as', 'at',\n",
    "    'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "    'concerning', 'considering', 'despite', 'down', 'during',\n",
    "    'except', 'excepting', 'excluding', 'following', 'for', 'from',\n",
    "    'in', 'inside', 'into', 'like', 'minus', 'near',\n",
    "    'of', 'off', 'on', 'onto', 'opposite', 'outside', 'over',\n",
    "    'past', 'per', 'plus', 'regarding', 'round',\n",
    "    'since', 'than', 'through', 'till', 'to', 'toward', 'towards',\n",
    "    'under', 'underneath', 'unlike', 'until', 'up', 'upon',\n",
    "    'versus', 'via', 'with', 'within', 'without'}\n",
    "\n",
    "CONJUNCTIONS_WORDS = {\n",
    "    \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "    \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "    \"such\", \"so\", \"that\"}\n",
    "\n",
    "ARTICLES = {'a', 'an', 'the'}\n",
    "\n",
    "INLINE_WORDS = {\n",
    "                   'also', 'am', 'are', 'did', 'furthermore',\n",
    "                   'has', 'hence', 'how', 'however', 'includ.',\n",
    "                   'instead', 'is', 'likewise', 'long',\n",
    "                   'moreover', 'should', 'similar',\n",
    "                   'though', 'thus', 'unless',\n",
    "                   'was', 'were', 'what', 'which', 'whichever', 'why', 'will',\n",
    "                   # 葡萄牙语介词    https://sites.google.com/site/omundodo1/yu-fa/1-2\n",
    "                   # para couldn't delete: \"Federal University of Pará\"\n",
    "                   'de', 'di', 'em', 'del', 'des', 'do',\n",
    "                   # 意大利语介词\n",
    "                   'delle',\n",
    "                   # 西班牙语介词    https://www.sohu.com/a/139510376_509791\n",
    "                   # 'a,con,sobre, en,de,contra,desde,entre,hacia,por'\n",
    "                   'con', 'sobre', 'en', 'contra', 'desde', 'entre', 'hacia', 'por', 'la',\n",
    "                   # 法语介词  https://zhuanlan.zhihu.com/p/33125505\n",
    "                   'apres', 'avant', 'avec', 'chez', 'contre', 'dans', 'depuis', 'derriere', 'devant', 'durant', 'envers', 'environ',\n",
    "                   'jusque', 'malgre', 'par', 'parmi', 'pendant', 'pour', 'sans', 'selon', 'sous', 'suivant', 'sur', 'vers'\n",
    "               } | PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "IMPOSSIBLE_ENDING_WORDS = INLINE_WORDS | {\n",
    "    'I', 'on', 'in', 'for',  # in jd and resume, it normally inside sentences\n",
    "    'the', 'my', 'your', 'he', 'she', 'his', 'her', 'fewer'\n",
    "}\n",
    "# 规范化罗马数字\n",
    "ROMAN_NUM_REGULATOR = {'I': '1', 'II': '2', 'III': '3', 'IV': '4', 'iv': '4', 'V': '5'}\n",
    "\n",
    "COMMON_WORDS = IMPOSSIBLE_ENDING_WORDS | \\\n",
    "               {'ago', 'on', 'consequently', 'behind', 'otherwise', 'the',\n",
    "                'plus', 'whereas', 'out', 'whenever', 'above', 'soon', 'before',\n",
    "                'around', 'neither', 'lest', 'down', 'both',\n",
    "                'now', 'even', 'because', 'still', 'along', 'whatever', 'although', 'following', 'up', 'for', 'nor',\n",
    "                'under', 'concerning', 'between', 'rather', 'towards', 'besides', 'either',\n",
    "                'nonetheless', 'yet', 'much', 'off', 'in', 'for',\n",
    "                'despite', 'once', 'beyond', 'therefore',\n",
    "                'meanwhile', 'within', 'across', 'conversely', 'without', 'supposing', 'like', 'or', 'including',\n",
    "                'throughout', 'wherever', 'nevertheless', 'accordingly', 'only', 'background', 'skill', 'backgrounds', 'skills'}\n",
    "COMPANY_COMMON_WORDS = {\n",
    "    \"en\": {\n",
    "        \"laboratory\", \"laboratories\", \"lab\", \"labs\", \"research\", \"technology\", \"technologies\", \"technical\", \"tech\", \"sci\", \"science\"\n",
    "    },\n",
    "    \"zh\": {\n",
    "        \"实验室\", \"研究院\", \"技术\", \"科技\"\n",
    "    }\n",
    "}\n",
    "COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "    r\"{0}|{1}\".format(\n",
    "        '|'.join(COMPANY_COMMON_WORDS['zh']),\n",
    "        rf\"(\\b|^)({'|'.join(COMPANY_COMMON_WORDS['en'])})(\\.|\\b|$)\"\n",
    "    ), re.I)\n",
    "\n",
    "\n",
    "def circumflex_regulator(name: str):\n",
    "    if not name:\n",
    "        return ''\n",
    "    for regex_, format_character in CIRCUMFLEX_REGEX_PAIRS:\n",
    "        name = regex_.sub(format_character, name)\n",
    "    return name\n",
    "\n",
    "\n",
    "def split_english_number_cjk(text: str, separate_return=False, split_same_language=False):\n",
    "    \"\"\"\n",
    "    Chinese and Chinese-English Mixture Operations\n",
    "    :param text: e.g. \"non_asians string: 并删除掉Machine-learning多余的, 字符串\"\n",
    "    :param separate_return: False\n",
    "    :param split_same_language: False\n",
    "    :return: e.g. [\"non_asians string:\", \"并删除掉\", \"Machine-learning\", \"多余的, 字符串\"]\n",
    "    :param separate_return: False\n",
    "    :param split_same_language: True\n",
    "    :return: e.g. ['non_asians', 'string', '并', '删', '除', '掉', 'Machine-learning', '多', '余', '的', '字', '符', '串']\n",
    "    :param separate_return: True\n",
    "    :param split_same_language: False\n",
    "    :return: e.g. ([\"non_asians string:\", \"Machine-learning\"], [\"并删除掉\", \"多余的, 字符串\"])\n",
    "    :param separate_return: True\n",
    "    :param split_same_language: True\n",
    "    :return: e.g. (['non_asians', 'string', 'Machine-learning'], ['并', '删', '除', '掉', '多', '余', '的', '字', '符', '串'])\n",
    "    \"\"\"\n",
    "    en_number_words = []\n",
    "    cjk_words = []\n",
    "    start, end = 0, len(text)\n",
    "    while start < end and (r_ := CJK_OR_NUMERIC_REGEX.search(text, pos=start)):\n",
    "        # before word\n",
    "        if word := text[start:r_.start()].strip():\n",
    "            if split_same_language:\n",
    "                en_number_words.extend(\n",
    "                    [stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(word) if (stripped_word := word_.strip())])\n",
    "            else:\n",
    "                en_number_words.append(word)\n",
    "        # matched word\n",
    "        if cjk_word := r_.groupdict().get(\"cjk\"):\n",
    "            if split_same_language:\n",
    "                (cjk_words if separate_return else en_number_words).extend(list(cjk_word))\n",
    "            else:\n",
    "                (cjk_words if separate_return else en_number_words).append(cjk_word)\n",
    "        else:\n",
    "            numeric_word = r_.groupdict().get(\"numeric\")\n",
    "            if split_same_language:\n",
    "                en_number_words.extend([stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(numeric_word) if (stripped_word := word_.strip())])\n",
    "            else:\n",
    "                en_number_words.append(numeric_word)\n",
    "        start = r_.end()\n",
    "    # save after left words\n",
    "    if end > start and (text := text[start:].strip()):\n",
    "        if split_same_language:\n",
    "            en_number_words.extend([word_ for word in ENGLISH_SPLITTER_REGEX.split(text) if (word_ := word.strip())])\n",
    "        else:\n",
    "            en_number_words.append(text)\n",
    "    return (en_number_words, cjk_words) if separate_return else en_number_words\n",
    "\n",
    "\n",
    "def match_language_by_character_regex(text: str, regex):\n",
    "    if text is None:\n",
    "        return False\n",
    "    else:\n",
    "        return regex.search(text)\n",
    "\n",
    "\n",
    "def search_string_regulator(name: str, return_str=True, is_company_name=False):\n",
    "    \"\"\"\n",
    "    :param is_company_name: company name 不应去除重复单词以及停止词，例如有公司名叫 IN-CO, ANOTHER CO\n",
    "    :param name:\n",
    "        The input string will be regulated with the following requirements:\n",
    "            1. Only Chinese/English/Digits characters left (remove characters in other languages)\n",
    "            3. Segmented:\n",
    "                Chinese: jieba cut words\n",
    "                English: words separated by space\n",
    "            3. Only Consists of meaningful words\n",
    "    :param return_str:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    extracted_name = MEANINGLESS_COLLEGE_COMPANY_NAME_REGEX.sub('', name)\n",
    "    #  判断是否仅有“无、其他它她等”\n",
    "    if not extracted_name:\n",
    "        return '' if return_str else []\n",
    "    words = []\n",
    "    for segment in ENTITY_WORDS_SPLITTER.split(extracted_name):\n",
    "        if not segment:\n",
    "            pass\n",
    "        # chinese: split words\n",
    "        elif contains_chinese(segment):\n",
    "            words.extend([word_ for word_ in jieba.cut(segment)])\n",
    "        # english: remove duplicates and common words\n",
    "        elif number := ROMAN_NUM_REGULATOR.get(segment, None):\n",
    "            words.append(number)\n",
    "        elif (segment := segment.lower()) not in words or is_company_name:\n",
    "            words.append(segment)\n",
    "    if len(words) > 2 and not is_company_name:\n",
    "        words = [word for word in words if word not in COMMON_WORDS]\n",
    "    return ' '.join(words) if return_str else words\n",
    "\n",
    "\n",
    "def regulate_english_asians_mixed_string(text: str):\n",
    "    return ' '.join(split_english_number_cjk(text, separate_return=False))\n",
    "\n",
    "\n",
    "# 必须同时出现在 query和cache中的 必要词\n",
    "# 去除公司后缀等词后，仅剩如下词汇，则不可去除公司后缀\n",
    "COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "    r'\\b(bank|college|university|education|energy|finance|army|air foce|navy|industry|'\n",
    "    r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "    r'environmental|gover(?:nor?|ment)|state|estado|procter|reliance|genome|software|hardware|agency|'\n",
    "    r'(?:sou|nor)th(?:[-\\s]?(?:ea|we)st(?:ern)?)?|[东西南北]方|[东西][南北][方]?|环境|'\n",
    "    r'银行|软件|硬件|大学|机构|研究院|学院|教育|协会|能源|金融|旅馆|医院|石油|电子|工业|[陆海空]军|政府|基因|'\n",
    "    r'global|market(?:ing)?|real estate|system|房地产|系统|兴业)\\b',\n",
    "    re.IGNORECASE)\n",
    "# Common words in organization names\n",
    "WORLD_BOURSE = {'en': ('Euronext', 'NYSE', 'NYX', 'ICE', 'TSE', 'NASDAQ', 'HKEx', 'TSX', 'DBAG', 'OSE', 'SWX', 'SSE',\n",
    "                       'SZSE', 'TSE', 'KRX', 'KOSDAQ', 'LSE', 'FWB', 'SGX', 'BSE', r'Dual[-\\s]listed'),\n",
    "                'zh': ('港交所', '纳斯达克', '那斯達克', '深交所', '上交所', '纽交所', r'[雙|双]重上市')}\n",
    "REGEX_BOURSE = re.compile(r\"({0}|{1})\".\n",
    "                          format('|'.join([rf'{suf}' for suf in WORLD_BOURSE['zh']]),\n",
    "                                 '|'.join([rf'(?:^|\\s){suf}(?:\\.|\\s|\\:|$)' for suf in WORLD_BOURSE['en']])),\n",
    "                          re.IGNORECASE)\n",
    "\n",
    "ORG_SUFFIX_WORDS = {\n",
    "    'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "           'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "    'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                      'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "           # drop 'shop' company suffix: 'Future Shop'/'Future plc'\n",
    "           'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                        'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp', 'sro',\n",
    "                        'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "}\n",
    "# Common words regexp in organization names\n",
    "REGEX_COMPANY_SUFFIX = re.compile(\n",
    "    r\"{0}|{1}\".format(\n",
    "        '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "        rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "    ),\n",
    "    re.IGNORECASE\n",
    ")\n",
    "CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "contains_chinese = partial(match_language_by_character_regex, regex=re.compile(rf'[{CHINESE_CHARACTERS}]'))\n",
    "\n",
    "URL_REGEX = re.compile(r\"(?:http://|https://)?(?:www\\.)?([a-z0-9]+(?:[\\-.][a-z0-9]+)*)\")\n",
    "MULTI_TITLE_OR_REGEX = re.compile(r'(\\w+(?:[\\W]\\w+)+)[/\\\\](?:\\w+(?:\\W\\w+)+)')\n",
    "SPLIT_OR_SYMBOLS = re.compile(r'\\s[/\\\\\\-]\\s')\n",
    "OR_AS_SYMBOL_REPLACE_REGEX = re.compile(r\"^[a-z0-9]+[/\\\\]([a-z0-9]+$|[a-z0-9]\\b)\", re.IGNORECASE)\n",
    "\n",
    "REGEX_EN = re.compile(f\"[a-zA-Z]+\")\n",
    "\n",
    "\n",
    "\n",
    "def company_words_regulator(company_name: str):\n",
    "    search_str = circumflex_regulator(company_name)\n",
    "\n",
    "    if contains_chinese(search_str):\n",
    "        if not REGEX_EN.search(search_str):\n",
    "            search_str = search_str.replace(' ', '')\n",
    "\n",
    "    search_str_drop = WHITESPACE_REGEX.sub(' ', REGEX_COMPANY_SUFFIX.sub('', REGEX_COMPANY_SUFFIX.sub('', search_str).strip())).strip()\n",
    "\n",
    "    search_str_drop_bourse = REGEX_BOURSE.sub(' ', search_str_drop).strip()\n",
    "    if search_str_drop_bourse:\n",
    "        search_str_drop = search_str_drop_bourse\n",
    "\n",
    "    search_str_drop_common_word = WHITESPACE_REGEX.sub(' ', COMPANY_COMMON_WORDS_REGEX.sub('', search_str_drop)).strip()\n",
    "    if search_str_drop_common_word:\n",
    "        search_str_drop = search_str_drop_common_word\n",
    "\n",
    "    # 检查是否需要恢复原始字符串\n",
    "    if (r_ := COMPANY_KEYWORDS_REGEX.search(search_str_drop)) and r_.end() - r_.start() > len(search_str_drop) - 2:\n",
    "        pass\n",
    "    elif search_str == search_str_drop:\n",
    "        pass\n",
    "    else:\n",
    "        search_str = search_str_drop\n",
    "\n",
    "    search_str_regulate = search_string_regulator(search_str, is_company_name=True).lower()\n",
    "    if search_str_regulate != search_str:\n",
    "        search_str = search_str_regulate\n",
    "\n",
    "    valid_words = []\n",
    "    for word_ in search_str.split(' '):\n",
    "        word_ = REGEX_COMPANY_DROP_INVALID.sub(' ', word_).strip().lower()\n",
    "        if word_:\n",
    "            valid_words.append(word_)\n",
    "    return ' '.join(valid_words)\n",
    "\n",
    "\n",
    "# 定义中文分词函数\n",
    "def chinese_tokenizer(text):\n",
    "    return \" \".join(jieba.cut(text))\n",
    "\n",
    "# 加载JSONL文件\n",
    "def load_company_data(file_path):\n",
    "    companies = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            companies.append(json.loads(line))\n",
    "    return companies\n",
    "\n",
    "# 将数据整理为DataFrame格式，便于name_matching处理\n",
    "def prepare_matching_data(companies):\n",
    "    rows = []\n",
    "    for company in companies:\n",
    "        for search_string in company.get(\"requiredSearchStrings\", []):\n",
    "            rows.append({\"name\": search_string, \"companyName\": company[\"companyName\"]})\n",
    "        for alias in company.get(\"aliases\", []):\n",
    "            rows.append({\"name\": alias, \"companyName\": company[\"companyName\"]})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# 匹配用户输入\n",
    "def match_user_input(user_input, matching_data):\n",
    "    matcher = NameMatcher(remove_ascii=False, punctuations=False)  # 保留中文和标点符号\n",
    "\n",
    "    # 分词处理输入数据和匹配数据\n",
    "    matching_data['name'] = matching_data['name'].apply(chinese_tokenizer)\n",
    "    matcher.load_and_process_master_data(column=\"name\", df_matching_data=matching_data)\n",
    "    \n",
    "\n",
    "    # 匹配用户输入\n",
    "    to_be_matched = pd.DataFrame({\"name\": [user_input]})\n",
    "    result = matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "    if not result.empty:\n",
    "        # 获取第一个匹配结果的得分\n",
    "        best_match = result.iloc[0][\"match_name\"]  # 取第一个匹配结果\n",
    "        match_score = result.iloc[0][\"score\"]  # 获取第一个匹配的得分\n",
    "\n",
    "        # 检查得分是否达到60%\n",
    "        if match_score >= 60:\n",
    "            company_name = matching_data[matching_data['name'] == best_match][\"companyName\"].values[0]\n",
    "            return company_name\n",
    "        else:\n",
    "            return \"未找到相关公司\"  \n",
    "    return \"未找到相关公司\"\n",
    "\n",
    "# 主函数\n",
    "def main(user_input):\n",
    "    # 加载公司数据\n",
    "    companies = load_company_data('bd_companies_international.jsonl')\n",
    "    \n",
    "    # 使用regulator\n",
    "    user_input = company_words_regulator(user_input)\n",
    "    \n",
    "    # 准备数据供name_matching使用\n",
    "    matching_data = prepare_matching_data(companies)\n",
    "    \n",
    "    # 匹配用户输入并返回公司名称\n",
    "    company_name = match_user_input(user_input, matching_data)\n",
    "    \n",
    "    print(company_name)\n",
    "\n",
    "# 用户输入\n",
    "user_input = input(\"Enter a company name:\")\n",
    "main(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675d6daf-515b-4e97-9fc6-89131810deab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V8 OOP version only wroks in jupyterlab\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "class CompanyNameProcessor:\n",
    "    AND_SYMBOL_REGEX = re.compile(r'[a-z0-9](&)[a-z0-9]', re.IGNORECASE)\n",
    "    AT_SYMBOL_REGEX = re.compile(r'[a-z0-9]+(@)[a-z0-9]+', re.IGNORECASE)\n",
    "\n",
    "    # CIRCUMFLEX_PAIRS Usage:\n",
    "    #    Remove circumflex from characters in search string\n",
    "    #    mapping: 大小写声调字母 -> 标准英文字母\n",
    "    CIRCUMFLEX_PAIRS = \\\n",
    "        ((r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'), (r'ðđ', 'd'), (r'ÐĐ', 'D'),\n",
    "         (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'), (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'),\n",
    "         (r'Ł', 'L'), (r'ł', 'l'), (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "         (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'), (r'ŵ', 'w'), (r'ý', 'y'),\n",
    "         (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE'))\n",
    "    CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "        (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    "    )\n",
    "\n",
    "    ENGLISH_SPLITTER_REGEX = re.compile(r\"[^\\w&_+*\\\\/'#\\-]+\")\n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\")\n",
    "\n",
    "    ENTITY_WORDS_SPLITTER = re.compile('[%s]+' % re.escape('!\"%\\'*<=>?@^_`~－ \\t\\n'))\n",
    "    # 保留 meaning word(\\w) 数字 下划线\n",
    "    MEANINGLESS_COLLEGE_COMPANY_NAME_REGEX = re.compile(\n",
    "        rf\"[^0-9\\w\\s]|^其[他它她]$|^others?$|^n\\.?a\\.?$|^n/a$|^none$|^null$\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "    ERASE_OR_REPLACE_TO_SPACE_SYMBOLS = re.compile(r'''[/\\\\%!！.?？^*·•⦁\\-]''')\n",
    "    REPLACE_TO_SPACE_SYMBOLS = re.compile(r'&?#.*|(\\s+|^)[+&@](\\s+|$)|\\s*[_+()（）、,，–\\-]\\s*')\n",
    "    THE_SPECIAL_ENTITY_NAME_REGEX = re.compile(r'^(the\\s+[a-z0-9]+)$', re.IGNORECASE)\n",
    "    SINGLE_LETTER_SEQUENTIAL_REGEX = re.compile(rf'(\\b[a-z]\\b)(\\s+[a-z]\\b)+', re.IGNORECASE)\n",
    "    SINGLE_LOWERCASE_REGEX = re.compile(rf'\\s+[a-z]\\s+')\n",
    "    REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "\n",
    "    PREPOSITION_WORDS = {\n",
    "        'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as', 'at',\n",
    "        'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "        'concerning', 'considering', 'despite', 'down', 'during',\n",
    "        'except', 'excepting', 'excluding', 'following', 'for', 'from',\n",
    "        'in', 'inside', 'into', 'like', 'minus', 'near',\n",
    "        'of', 'off', 'on', 'onto', 'opposite', 'outside', 'over',\n",
    "        'past', 'per', 'plus', 'regarding', 'round',\n",
    "        'since', 'than', 'through', 'till', 'to', 'toward', 'towards',\n",
    "        'under', 'underneath', 'unlike', 'until', 'up', 'upon',\n",
    "        'versus', 'via', 'with', 'within', 'without'}\n",
    "\n",
    "    CONJUNCTIONS_WORDS = {\n",
    "        \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "        \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "        \"such\", \"so\", \"that\"}\n",
    "\n",
    "    ARTICLES = {'a', 'an', 'the'}\n",
    "\n",
    "    INLINE_WORDS = {\n",
    "                       'also', 'am', 'are', 'did', 'furthermore',\n",
    "                       'has', 'hence', 'how', 'however', 'includ.',\n",
    "                       'instead', 'is', 'likewise', 'long',\n",
    "                       'moreover', 'should', 'similar',\n",
    "                       'though', 'thus', 'unless',\n",
    "                       'was', 'were', 'what', 'which', 'whichever', 'why', 'will',\n",
    "                       # 葡萄牙语介词    https://sites.google.com/site/omundodo1/yu-fa/1-2\n",
    "                       # para couldn't delete: \"Federal University of Pará\"\n",
    "                       'de', 'di', 'em', 'del', 'des', 'do',\n",
    "                       # 意大利语介词\n",
    "                       'delle',\n",
    "                       # 西班牙语介词    https://www.sohu.com/a/139510376_509791\n",
    "                       # 'a,con,sobre, en,de,contra,desde,entre,hacia,por'\n",
    "                       'con', 'sobre', 'en', 'contra', 'desde', 'entre', 'hacia', 'por', 'la',\n",
    "                       # 法语介词  https://zhuanlan.zhihu.com/p/33125505\n",
    "                       'apres', 'avant', 'avec', 'chez', 'contre', 'dans', 'depuis', 'derriere', 'devant', 'durant', 'envers', 'environ',\n",
    "                       'jusque', 'malgre', 'par', 'parmi', 'pendant', 'pour', 'sans', 'selon', 'sous', 'suivant', 'sur', 'vers'\n",
    "                   } | PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "    IMPOSSIBLE_ENDING_WORDS = INLINE_WORDS | {\n",
    "        'I', 'on', 'in', 'for',  # in jd and resume, it normally inside sentences\n",
    "        'the', 'my', 'your', 'he', 'she', 'his', 'her', 'fewer'\n",
    "    }\n",
    "    # 规范化罗马数字\n",
    "    ROMAN_NUM_REGULATOR = {'I': '1', 'II': '2', 'III': '3', 'IV': '4', 'iv': '4', 'V': '5'}\n",
    "\n",
    "    COMMON_WORDS = IMPOSSIBLE_ENDING_WORDS | \\\n",
    "                   {'ago', 'on', 'consequently', 'behind', 'otherwise', 'the',\n",
    "                    'plus', 'whereas', 'out', 'whenever', 'above', 'soon', 'before',\n",
    "                    'around', 'neither', 'lest', 'down', 'both',\n",
    "                    'now', 'even', 'because', 'still', 'along', 'whatever', 'although', 'following', 'up', 'for', 'nor',\n",
    "                    'under', 'concerning', 'between', 'rather', 'towards', 'besides', 'either',\n",
    "                    'nonetheless', 'yet', 'much', 'off', 'in', 'for',\n",
    "                    'despite', 'once', 'beyond', 'therefore',\n",
    "                    'meanwhile', 'within', 'across', 'conversely', 'without', 'supposing', 'like', 'or', 'including',\n",
    "                    'throughout', 'wherever', 'nevertheless', 'accordingly', 'only', 'background', 'skill', 'backgrounds', 'skills'}\n",
    "    COMPANY_COMMON_WORDS = {\n",
    "        \"en\": {\n",
    "            \"laboratory\", \"laboratories\", \"lab\", \"labs\", \"research\", \"technology\", \"technologies\", \"technical\", \"tech\", \"sci\", \"science\"\n",
    "        },\n",
    "        \"zh\": {\n",
    "            \"实验室\", \"研究院\", \"技术\", \"科技\"\n",
    "        }\n",
    "    }\n",
    "    COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join(COMPANY_COMMON_WORDS['zh']),\n",
    "            rf\"(\\b|^)({'|'.join(COMPANY_COMMON_WORDS['en'])})(\\.|\\b|$)\"\n",
    "        ), re.I)\n",
    "    # 必须同时出现在 query和cache中的 必要词\n",
    "    # 去除公司后缀等词后，仅剩如下词汇，则不可去除公司后缀\n",
    "    COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "        r'\\b(bank|college|university|education|energy|finance|army|air foce|navy|industry|'\n",
    "        r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "        r'environmental|gover(?:nor?|ment)|state|estado|procter|reliance|genome|software|hardware|agency|'\n",
    "        r'(?:sou|nor)th(?:[-\\s]?(?:ea|we)st(?:ern)?)?|[东西南北]方|[东西][南北][方]?|环境|'\n",
    "        r'银行|软件|硬件|大学|机构|研究院|学院|教育|协会|能源|金融|旅馆|医院|石油|电子|工业|[陆海空]军|政府|基因|'\n",
    "        r'global|market(?:ing)?|real estate|system|房地产|系统|兴业)\\b',\n",
    "        re.IGNORECASE)\n",
    "    # Common words in organization names\n",
    "    WORLD_BOURSE = {'en': ('Euronext', 'NYSE', 'NYX', 'ICE', 'TSE', 'NASDAQ', 'HKEx', 'TSX', 'DBAG', 'OSE', 'SWX', 'SSE',\n",
    "                           'SZSE', 'TSE', 'KRX', 'KOSDAQ', 'LSE', 'FWB', 'SGX', 'BSE', r'Dual[-\\s]listed'),\n",
    "                    'zh': ('港交所', '纳斯达克', '那斯達克', '深交所', '上交所', '纽交所', r'[雙|双]重上市')}\n",
    "    REGEX_BOURSE = re.compile(r\"({0}|{1})\".\n",
    "                              format('|'.join([rf'{suf}' for suf in WORLD_BOURSE['zh']]),\n",
    "                                     '|'.join([rf'(?:^|\\s){suf}(?:\\.|\\s|\\:|$)' for suf in WORLD_BOURSE['en']])),\n",
    "                              re.IGNORECASE)\n",
    "    \n",
    "    ORG_SUFFIX_WORDS = {\n",
    "        'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "               'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "        'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                          'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "               # drop 'shop' company suffix: 'Future Shop'/'Future plc'\n",
    "               'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                            'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp', 'sro',\n",
    "                            'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "    }\n",
    "    # Common words regexp in organization names\n",
    "    REGEX_COMPANY_SUFFIX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "            rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "        ),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "    CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "    contains_chinese = partial(match_language_by_character_regex, regex=re.compile(rf'[{CHINESE_CHARACTERS}]'))\n",
    "    \n",
    "    URL_REGEX = re.compile(r\"(?:http://|https://)?(?:www\\.)?([a-z0-9]+(?:[\\-.][a-z0-9]+)*)\")\n",
    "    MULTI_TITLE_OR_REGEX = re.compile(r'(\\w+(?:[\\W]\\w+)+)[/\\\\](?:\\w+(?:\\W\\w+)+)')\n",
    "    SPLIT_OR_SYMBOLS = re.compile(r'\\s[/\\\\\\-]\\s')\n",
    "    OR_AS_SYMBOL_REPLACE_REGEX = re.compile(r\"^[a-z0-9]+[/\\\\]([a-z0-9]+$|[a-z0-9]\\b)\", re.IGNORECASE)\n",
    "\n",
    "    REGEX_EN = re.compile(f\"[a-zA-Z]+\")\n",
    "\n",
    "\n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.matcher = NameMatcher(remove_ascii=False, punctuations=False)\n",
    "\n",
    "    def load_company_data(self):\n",
    "        companies = []\n",
    "        with open(self.file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                companies.append(json.loads(line))\n",
    "        return companies\n",
    "\n",
    "    def prepare_matching_data(self, companies):\n",
    "        rows = []\n",
    "        for company in companies:\n",
    "            for search_string in company.get(\"requiredSearchStrings\", []):\n",
    "                rows.append({\"name\": search_string, \"companyName\": company[\"companyName\"]})\n",
    "            for alias in company.get(\"aliases\", []):\n",
    "                rows.append({\"name\": alias, \"companyName\": company[\"companyName\"]})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def circumflex_regulator(self, name):\n",
    "        if not name:\n",
    "            return ''\n",
    "        for regex_, format_character in CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "    def split_english_number_cjk(self, text, separate_return=False, split_same_language=False):\n",
    "        en_number_words = []\n",
    "        cjk_words = []\n",
    "        start, end = 0, len(text)\n",
    "        while start < end and (r_ := CJK_OR_NUMERIC_REGEX.search(text, pos=start)):\n",
    "            if word := text[start:r_.start()].strip():\n",
    "                if split_same_language:\n",
    "                    en_number_words.extend([stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(word) if (stripped_word := word_.strip())])\n",
    "                else:\n",
    "                    en_number_words.append(word)\n",
    "            if cjk_word := r_.groupdict().get(\"cjk\"):\n",
    "                if split_same_language:\n",
    "                    (cjk_words if separate_return else en_number_words).extend(list(cjk_word))\n",
    "                else:\n",
    "                    (cjk_words if separate_return else en_number_words).append(cjk_word)\n",
    "            else:\n",
    "                numeric_word = r_.groupdict().get(\"numeric\")\n",
    "                if split_same_language:\n",
    "                    en_number_words.extend([stripped_word for word_ in ENGLISH_SPLITTER_REGEX.split(numeric_word) if (stripped_word := word_.strip())])\n",
    "                else:\n",
    "                    en_number_words.append(numeric_word)\n",
    "            start = r_.end()\n",
    "        if end > start and (text := text[start:].strip()):\n",
    "            if split_same_language:\n",
    "                en_number_words.extend([word_ for word in ENGLISH_SPLITTER_REGEX.split(text) if (word_ := word.strip())])\n",
    "            else:\n",
    "                en_number_words.append(text)\n",
    "        return (en_number_words, cjk_words) if separate_return else en_number_words\n",
    "\n",
    "    def match_language_by_character_regex(self, text, regex):\n",
    "        if text is None:\n",
    "            return False\n",
    "        else:\n",
    "            return bool(regex.search(text))\n",
    "\n",
    "    def search_string_regulator(self, name, return_str=True, is_company_name=False):\n",
    "        extracted_name = MEANINGLESS_COLLEGE_COMPANY_NAME_REGEX.sub('', name)\n",
    "        if not extracted_name:\n",
    "            return '' if return_str else []\n",
    "        words = []\n",
    "        for segment in ENTITY_WORDS_SPLITTER.split(extracted_name):\n",
    "            if not segment:\n",
    "                continue\n",
    "            elif contains_chinese(segment):\n",
    "                words.extend([word_ for word_ in jieba.cut(segment)])\n",
    "            elif number := ROMAN_NUM_REGULATOR.get(segment, None):\n",
    "                words.append(number)\n",
    "            elif (segment := segment.lower()) not in words or is_company_name:\n",
    "                words.append(segment)\n",
    "        if len(words) > 2 and not is_company_name:\n",
    "            words = [word for word in words if word not in COMMON_WORDS]\n",
    "        return ' '.join(words) if return_str else words\n",
    "\n",
    "    def company_words_regulator(self, company_name: str) -> str:\n",
    "        # 处理上标字符\n",
    "        search_str = self.circumflex_regulator(company_name)\n",
    "\n",
    "        # 删除常见公司后缀\n",
    "        search_str = self.REGEX_COMPANY_SUFFIX.sub('', search_str).strip()\n",
    "\n",
    "        # 删除额外空白字符\n",
    "        search_str = self.WHITESPACE_REGEX.sub(' ', search_str).strip()\n",
    "\n",
    "        # 特定词语的处理，如证券、股票等\n",
    "        search_str = self.REGEX_BOURSE.sub(' ', search_str).strip()\n",
    "\n",
    "        # 删除公司常见无意义词语\n",
    "        search_str = self.COMPANY_COMMON_WORDS_REGEX.sub('', search_str).strip()\n",
    "\n",
    "        # 如果字符串较短或匹配特定关键字，可能不需要进一步处理\n",
    "        if self.COMPANY_KEYWORDS_REGEX.search(search_str) and len(search_str) < 5:\n",
    "            return search_str\n",
    "\n",
    "        # 对字符串进行进一步规范化处理，如分词等\n",
    "        regulated_search_str = self.search_string_regulator(search_str, is_company_name=True)\n",
    "        \n",
    "        # 修正为小写\n",
    "        regulated_search_str = regulated_search_str.lower()\n",
    "\n",
    "        # 分词后保留有效单词\n",
    "        valid_words = []\n",
    "        for word in regulated_search_str.split():\n",
    "            cleaned_word = self.ENGLISH_SPLITTER_REGEX.sub(' ', word).strip()\n",
    "            if cleaned_word:\n",
    "                valid_words.append(cleaned_word)\n",
    "\n",
    "        # 返回处理后的公司名称\n",
    "        return ' '.join(valid_words)\n",
    "\n",
    "    def regulate_english_asians_mixed_string(self, text):\n",
    "        return ' '.join(self.split_english_number_cjk(text, separate_return=False))\n",
    "\n",
    "    def match_user_input(self, user_input, matching_data):\n",
    "        matching_data['name'] = matching_data['name'].apply(self.chinese_tokenizer)\n",
    "        self.matcher.load_and_process_master_data(column=\"name\", df_matching_data=matching_data)\n",
    "        to_be_matched = pd.DataFrame({\"name\": [user_input]})\n",
    "        result = self.matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "        if not result.empty:\n",
    "            best_match = result.iloc[0][\"match_name\"]\n",
    "            match_score = result.iloc[0][\"score\"]\n",
    "            if match_score >= 60:\n",
    "                company_name = matching_data[matching_data['name'] == best_match][\"companyName\"].values[0]\n",
    "                return company_name\n",
    "            else:\n",
    "                return \"未找到相关公司\"\n",
    "        return \"未找到相关公司\"\n",
    "\n",
    "    def chinese_tokenizer(self, text):\n",
    "        return \" \".join(jieba.cut(text))\n",
    "\n",
    "    def process_input(self, user_input):\n",
    "        companies = self.load_company_data()\n",
    "        user_input = self.regulate_english_asians_mixed_string(user_input)\n",
    "        matching_data = self.prepare_matching_data(companies)\n",
    "        company_name = self.match_user_input(user_input, matching_data)\n",
    "        return company_name\n",
    "\n",
    "\n",
    "def main():\n",
    "    processor = CompanyNameProcessor('bd_companies_international.jsonl')\n",
    "    user_input = input(\"Enter a company name: \")\n",
    "    company_name = processor.process_input(user_input)\n",
    "    print(company_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59b5917-c129-4c1b-9cd0-eb38ef22e58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V9\n",
    "#One that works well but no aliases adaptataion\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "\n",
    "class NameNormalizer:\n",
    "    CIRCUMFLEX_PAIRS = (\n",
    "        (r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'),\n",
    "        (r'ðđ', 'd'), (r'ÐĐ', 'D'), (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'),\n",
    "        (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'), (r'Ł', 'L'), (r'ł', 'l'),\n",
    "        (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "        (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'),\n",
    "        (r'ŵ', 'w'), (r'ý', 'y'), (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE')\n",
    "    )\n",
    "    CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "        (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"Normalize circumflex letters in the name.\"\"\"\n",
    "        # 如果 name 不是字符串，将其转换为空字符串\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # 执行字符串正则替换\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class CompanyMatcher:\n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(\n",
    "        rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\")\n",
    "    WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "    COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "        r\"(实验室|研究院|技术|科技)|(\\b(?:laboratory|laboratories|lab|labs|research|technology|tech|science|sci)(\\.|\\b|$))\",\n",
    "        re.I\n",
    "    )\n",
    "    PREPOSITION_WORDS = {\n",
    "        'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as',\n",
    "        'at',\n",
    "        'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "        'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'following',\n",
    "        'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite',\n",
    "        'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'since', 'than', 'through', 'till', 'to',\n",
    "        'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within',\n",
    "        'without'\n",
    "    }\n",
    "    CONJUNCTIONS_WORDS = {\n",
    "        \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "        \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "        \"such\", \"so\", \"that\"\n",
    "    }\n",
    "    ARTICLES = {'a', 'an', 'the'}\n",
    "    INLINE_WORDS = PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "    ORG_SUFFIX_WORDS = {\n",
    "        'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "               'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "        'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                          'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "               'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                            'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp',\n",
    "                            'sro',\n",
    "                            'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "    }\n",
    "    ORG_SUFFIX_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "            rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "        ),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "        r'\\b(bank|college|university|education|energy|finance|army|air force|navy|industry|'\n",
    "        r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "        r'environmental|government|state|'\n",
    "        r'global|market(?:ing)?|real estate|system)\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    def __init__(self, company_data_file: str):\n",
    "        self.company_data_file = company_data_file\n",
    "        self.matcher = NameMatcher(remove_ascii=False, punctuations=False)\n",
    "        self.matching_data = None\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_chinese(text):\n",
    "        \"\"\"Helper function to check if a string contains Chinese characters.\"\"\"\n",
    "        CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "        return re.search(f\"[{CHINESE_CHARACTERS}]\", text)\n",
    "\n",
    "    def company_words_regulator(self, company_name: str) -> str:\n",
    "        \"\"\"Regulate company names using multiple techniques.\"\"\"\n",
    "        # Step 1: Normalize circumflex letters\n",
    "        search_str = NameNormalizer.circumflex_regulator(company_name)\n",
    "\n",
    "        # Step 2: Remove spaces if only Chinese is detected\n",
    "        if self.contains_chinese(search_str) and not re.search(r\"[a-zA-Z]\", search_str):\n",
    "            search_str = search_str.replace(' ', '')\n",
    "\n",
    "        # Step 3: Remove unnecessary company suffixes or common words\n",
    "        search_str = self.WHITESPACE_REGEX.sub(' ', self.ORG_SUFFIX_REGEX.sub('', search_str)).strip()\n",
    "\n",
    "        # Step 4: Filter out prepositions, conjunctions, and articles (INLINE_WORDS)\n",
    "        words = search_str.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in self.INLINE_WORDS]\n",
    "        search_str = ' '.join(filtered_words)\n",
    "\n",
    "        # Step 5: Check for essential company keywords\n",
    "        if self.COMPANY_KEYWORDS_REGEX.search(search_str):\n",
    "            return search_str\n",
    "\n",
    "        return search_str\n",
    "\n",
    "    def load_company_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load the company data from a JSONL file.\"\"\"\n",
    "        companies = []\n",
    "        with open(self.company_data_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                companies.append(json.loads(line))\n",
    "        return pd.DataFrame(companies)\n",
    "\n",
    "    def prepare_matching_data(self, companies: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare data for matching.\"\"\"\n",
    "        rows = []\n",
    "        for _, company in companies.iterrows():\n",
    "            # Process required search strings\n",
    "            required_search_strings = company.get(\"requiredSearchStrings\", [])\n",
    "            if not isinstance(required_search_strings, list):\n",
    "                required_search_strings = []  # Ensure it's a list if not iterable\n",
    "\n",
    "            for search_string in required_search_strings:\n",
    "                rows.append({\"name\": search_string, \"companyName\": company[\"companyName\"]})\n",
    "\n",
    "            # Process aliases\n",
    "            aliases = company.get(\"aliases\", [])\n",
    "            if not isinstance(aliases, list):\n",
    "                aliases = []  # Ensure aliases is a list if it's not iterable\n",
    "\n",
    "            for alias in aliases:\n",
    "                rows.append({\"name\": alias, \"companyName\": company[\"companyName\"]})\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def tokenize(self, text: str) -> str:\n",
    "        \"\"\"Tokenize the text using Jieba for Chinese words.\"\"\"\n",
    "        return \" \".join(jieba.cut(text))\n",
    "\n",
    "    def prepare_data_for_matching(self, companies: pd.DataFrame):\n",
    "        \"\"\"Tokenize and prepare company data for matching.\"\"\"\n",
    "        companies['name'] = companies['name'].apply(self.tokenize)\n",
    "        self.matcher.load_and_process_master_data(column=\"name\", df_matching_data=companies)\n",
    "\n",
    "    def match_user_input(self, user_input: str) -> str:\n",
    "        \"\"\"Match user input against the company names.\"\"\"\n",
    "\n",
    "        # 进行匹配\n",
    "        user_input_segmented = self.company_words_regulator(user_input)\n",
    "        print(f\"After Company_Words_Regulator user input: {user_input_segmented}\")\n",
    "        user_input_segmented = \" \".join(jieba.cut(user_input_segmented))\n",
    "        print(f\"After Jieba user input: {user_input_segmented}\")\n",
    "        to_be_matched = pd.DataFrame({\"name\": [user_input_segmented]})\n",
    "        result = self.matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "        # 如果没有匹配结果，则返回空字符串\n",
    "        if result.empty:\n",
    "            return ''\n",
    "\n",
    "        # 获取第一个匹配结果\n",
    "        best_match = result.iloc[0][\"match_name\"]\n",
    "        match_score = result.iloc[0][\"score\"]\n",
    "\n",
    "        # 如果匹配得分大于等于 60%，返回公司名称，否则返回空字符串\n",
    "        if match_score >= 60:\n",
    "            company_name = self.matching_data[self.matching_data['name'] == best_match][\"companyName\"].values[0]\n",
    "            return company_name\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def main():\n",
    "    company_matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "    user_input = input(\"Enter a company name: \")\n",
    "    matched_company = company_matcher.process(user_input)\n",
    "    print(matched_company)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d33781-0211-4251-a700-3992ff217a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V9 Unittest\n",
    "import unittest\n",
    "import pandas as pd\n",
    "from name_matching_for_company import CompanyMatcher\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "class NameNormalizer:\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"Normalize circumflex letters in the name.\"\"\"\n",
    "        # 如果 name 不是字符串，将其转换为空字符串\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # 执行字符串正则替换\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class TestCompanyMatcher(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up the CompanyMatcher and load the data once for all tests.\"\"\"\n",
    "        # 初始化 CompanyMatcher 并加载数据，只进行一次数据加载和 fit\n",
    "        cls.matcher = CompanyMatcher('bd_companies_international.jsonl')  # 确保文件路径正确\n",
    "        cls.matcher_data_loaded = False\n",
    "\n",
    "        # 只加载一次数据，避免每次都重复加载\n",
    "        cls.companies = cls.matcher.load_company_data()\n",
    "\n",
    "        # 调用一次 prepare_matching_data，进行数据处理和匹配初始化\n",
    "        cls.matcher.matching_data = cls.matcher.prepare_matching_data(cls.companies)  # 确保 matching_data 被赋值\n",
    "\n",
    "        # 进行一次数据的处理和 fit 操作\n",
    "        cls.matcher.prepare_data_for_matching(cls.matcher.matching_data)\n",
    "\n",
    "        cls.matcher_data_loaded = True  # 确保所有数据已经准备好\n",
    "\n",
    "    def test_company_name_matching(self):\n",
    "        \"\"\"Test the company name matching with input from the CSV file.\"\"\"\n",
    "        # 读取 filtered_company_names.csv 文件\n",
    "        test_data = pd.read_csv('filtered_company_names.csv')\n",
    "\n",
    "        # 准备输出结果 CSV 文件\n",
    "        with open('company_name_matching_results.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerow(['company name', 'expected company name', 'output', 'true/false', 'time (seconds)'])\n",
    "\n",
    "            # 遍历每一行测试数据\n",
    "            for index, row in test_data.iterrows():\n",
    "                input_name = row['company name']\n",
    "                expected_name = row['expected company name']\n",
    "\n",
    "                # 记录开始时间\n",
    "                start_time = time.time()\n",
    "\n",
    "                # 调用 process 进行公司名匹配，避免每次都重复加载或处理数据\n",
    "                output_name = self.matcher.match_user_input(input_name)\n",
    "\n",
    "                # 记录结束时间\n",
    "                end_time = time.time()\n",
    "\n",
    "                # 计算处理时间\n",
    "                elapsed_time = end_time - start_time\n",
    "\n",
    "                # 确保 expected_name 和 output_name 为字符串，防止 AttributeError\n",
    "                if pd.isna(expected_name):\n",
    "                    expected_name = ''\n",
    "                if pd.isna(output_name):\n",
    "                    output_name = ''\n",
    "\n",
    "                # 如果 output 和 expected 都是空，则设置 result 为 True\n",
    "                if output_name == '' and expected_name == '':\n",
    "                    result = True\n",
    "                else:\n",
    "                    result = output_name.lower() == expected_name.lower()\n",
    "\n",
    "                # 写入结果到输出 CSV\n",
    "                writer.writerow([input_name, expected_name, output_name, result, elapsed_time])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4490aae5-244a-4982-96f1-6c6bada8c81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V10A\n",
    "#Working code with aliases match\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "\n",
    "class NameNormalizer:\n",
    "    CIRCUMFLEX_PAIRS = (\n",
    "        (r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'),\n",
    "        (r'ðđ', 'd'), (r'ÐĐ', 'D'), (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'),\n",
    "        (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'), (r'Ł', 'L'), (r'ł', 'l'),\n",
    "        (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "        (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'),\n",
    "        (r'ŵ', 'w'), (r'ý', 'y'), (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE')\n",
    "    )\n",
    "    CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "        (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"Normalize circumflex letters in the name.\"\"\"\n",
    "        # 如果 name 不是字符串，将其转换为空字符串\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # 执行字符串正则替换\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class CompanyMatcher:\n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(\n",
    "        rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\")\n",
    "    WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "    COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "        r\"(实验室|研究院|技术|科技)|(\\b(?:laboratory|laboratories|lab|labs|research|technology|tech|science|sci)(\\.|\\b|$))\",\n",
    "        re.I\n",
    "    )\n",
    "    PREPOSITION_WORDS = {\n",
    "        'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as',\n",
    "        'at',\n",
    "        'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "        'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'following',\n",
    "        'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite',\n",
    "        'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'since', 'than', 'through', 'till', 'to',\n",
    "        'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within',\n",
    "        'without'\n",
    "    }\n",
    "    CONJUNCTIONS_WORDS = {\n",
    "        \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "        \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "        \"such\", \"so\", \"that\"\n",
    "    }\n",
    "    ARTICLES = {'a', 'an', 'the'}\n",
    "    INLINE_WORDS = PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "    ORG_SUFFIX_WORDS = {\n",
    "        'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "               'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "        'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                          'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "               'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                            'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp',\n",
    "                            'sro',\n",
    "                            'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "    }\n",
    "    ORG_SUFFIX_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "            rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "        ),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "        r'\\b(bank|college|university|education|energy|finance|army|air force|navy|industry|'\n",
    "        r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "        r'environmental|government|state|'\n",
    "        r'global|market(?:ing)?|real estate|system)\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    def __init__(self, company_data_file: str):\n",
    "        self.company_data_file = company_data_file\n",
    "        self.matcher = NameMatcher(remove_ascii=False, punctuations=False)\n",
    "        self.matching_data = None\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_chinese(text):\n",
    "        \"\"\"Helper function to check if a string contains Chinese characters.\"\"\"\n",
    "        CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "        return re.search(f\"[{CHINESE_CHARACTERS}]\", text)\n",
    "\n",
    "    def company_words_regulator(self, company_name: str) -> str:\n",
    "        \"\"\"Regulate company names using multiple techniques.\"\"\"\n",
    "        # Step 1: Normalize circumflex letters\n",
    "        search_str = NameNormalizer.circumflex_regulator(company_name)\n",
    "\n",
    "        # Step 2: Remove spaces if only Chinese is detected\n",
    "        if self.contains_chinese(search_str) and not re.search(r\"[a-zA-Z]\", search_str):\n",
    "            search_str = search_str.replace(' ', '')\n",
    "\n",
    "        # Step 3: Remove unnecessary company suffixes or common words\n",
    "        search_str = self.WHITESPACE_REGEX.sub(' ', self.ORG_SUFFIX_REGEX.sub('', search_str)).strip()\n",
    "\n",
    "        # Step 4: Filter out prepositions, conjunctions, and articles (INLINE_WORDS)\n",
    "        words = search_str.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in self.INLINE_WORDS]\n",
    "        search_str = ' '.join(filtered_words)\n",
    "\n",
    "        # Step 5: Check for essential company keywords\n",
    "        if self.COMPANY_KEYWORDS_REGEX.search(search_str):\n",
    "            return search_str\n",
    "\n",
    "        return search_str\n",
    "\n",
    "    def load_company_data(self) -> pd.DataFrame:\n",
    "        \"\"\"加载公司数据并确保别名信息正确存在.\"\"\"\n",
    "        companies = []\n",
    "        with open(self.company_data_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                company = json.loads(line)\n",
    "                company['aliases'] = company.get('aliases', [])  # 确保别名存在\n",
    "                companies.append(company)\n",
    "        return pd.DataFrame(companies)\n",
    "\n",
    "    def prepare_matching_data(self, companies: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare data for matching, ensuring aliases and required search strings are correctly loaded as individual entries.\"\"\"\n",
    "        rows = []\n",
    "        for _, company in companies.iterrows():\n",
    "            # Process required search strings\n",
    "            required_search_strings = company.get(\"requiredSearchStrings\", [])\n",
    "            if not isinstance(required_search_strings, list):\n",
    "                required_search_strings = []  # Ensure it's a list if not iterable\n",
    "\n",
    "            # Combine required search strings and aliases into a full alias list\n",
    "            aliases = company.get(\"aliases\", [])\n",
    "            if not isinstance(aliases, list):\n",
    "                aliases = []  # Ensure aliases is a list if it's not iterable\n",
    "\n",
    "            # 将所有 requiredSearchStrings 和 aliases 合并为一个别名列表\n",
    "            full_alias_list = required_search_strings + aliases\n",
    "\n",
    "            print(f\"Processing full aliases list for company {company['companyName']}: {full_alias_list}\")\n",
    "\n",
    "            # For each alias, create a separate row with all aliases as alias_name\n",
    "            for alias in full_alias_list:\n",
    "                rows.append({\n",
    "                    \"name\": alias,\n",
    "                    \"companyName\": company.get(\"companyName\", \"\"),\n",
    "                    \"alias_name\": full_alias_list,  # 将完整的别名列表作为 alias_name 存储\n",
    "                    \"is_alias\": alias in aliases  # 标记是否为别名\n",
    "                })\n",
    "\n",
    "        matching_data = pd.DataFrame(rows)\n",
    "\n",
    "        # 打印生成的匹配数据样本，确认别名被正确加载\n",
    "        print(\"Sample of matching data after processing full aliases:\")\n",
    "        print(matching_data[['name', 'companyName', 'alias_name']].head(10))\n",
    "\n",
    "        return matching_data\n",
    "\n",
    "    def tokenize(self, text: str) -> str:\n",
    "        \"\"\"Tokenize the text using Jieba for Chinese words.\"\"\"\n",
    "        return \" \".join(jieba.cut(text))\n",
    "\n",
    "    def prepare_data_for_matching(self, companies: pd.DataFrame):\n",
    "        \"\"\"Tokenize and prepare company data for matching.\"\"\"\n",
    "        # 对公司数据进行分词处理\n",
    "        companies['name'] = companies['name'].apply(self.tokenize)\n",
    "\n",
    "        # Load and process master data for matching\n",
    "        self.matcher.load_and_process_master_data(column=\"name\", df_matching_data=companies)  # 确保主数据加载\n",
    "\n",
    "    def match_user_input(self, user_input: str, expected_name: str) -> str:\n",
    "        \"\"\"Match user input against the company names, then validate against expected name.\"\"\"\n",
    "\n",
    "        # 对输入进行标准化和分词处理\n",
    "        print(f\"User input: {user_input}\")  # 打印调试信息\n",
    "        user_input_segmented = self.company_words_regulator(user_input)\n",
    "        print(f\"Segmented input: {user_input_segmented}\")  # 打印调试信息\n",
    "        user_input_segmented = \" \".join(jieba.cut(user_input_segmented))\n",
    "        print(f\"Segmented and tokenized input: {user_input_segmented}\")  # 打印调试信息\n",
    "        to_be_matched = pd.DataFrame({\"name\": [user_input_segmented]})\n",
    "\n",
    "        # 匹配用户输入的名称\n",
    "        result = self.matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "        print(f\"Matching result: {result}\")  # 打印调试信息\n",
    "\n",
    "        # 如果没有匹配结果，返回空字符串\n",
    "        if result.empty:\n",
    "            print(\"No match found\")  # 打印调试信息\n",
    "            return ''\n",
    "\n",
    "        # 获取最匹配的结果\n",
    "        best_match = result.iloc[0][\"match_name\"]\n",
    "        match_score = result.iloc[0][\"score\"]\n",
    "        print(f\"Best match: {best_match}, Match score: {match_score}\")  # 打印调试信息\n",
    "\n",
    "        # 匹配得分超过 60%，视为成功匹配\n",
    "        if match_score >= 70:\n",
    "            # 通过 best_match 查找公司名称和别名\n",
    "            matched_row = self.matching_data[self.matching_data['name'] == best_match]\n",
    "            if matched_row.empty:\n",
    "                print(f\"No matching row found for: {best_match}\")  # 打印调试信息\n",
    "                return ''\n",
    "\n",
    "            # 获取公司名称和别名信息\n",
    "            company_name = matched_row.iloc[0]['companyName']\n",
    "            aliases = matched_row.iloc[0]['alias_name']  # 这现在是一个包含所有别名的列表\n",
    "\n",
    "            # 打印公司名称和别名\n",
    "            print(f\"Company name: {company_name}, Aliases: {aliases}\")  # 打印当前公司名称和别名\n",
    "\n",
    "            # 处理 expected_name，防止 NaN 导致错误\n",
    "            if pd.isna(expected_name):\n",
    "                expected_name = \"\"  # 如果是 NaN，替换为空字符串\n",
    "            else:\n",
    "                expected_name = expected_name.lower()\n",
    "\n",
    "            print(f\"Expected name: {expected_name}\")  # 打印 expected_name\n",
    "\n",
    "            # 检查 expected_name 是否在别名中\n",
    "            if expected_name and expected_name in [alias.lower() for alias in aliases]:\n",
    "                print(f\"Matched alias: {expected_name}\")  # 打印匹配到的别名\n",
    "                return expected_name\n",
    "\n",
    "            # 如果 expected_name 和 company_name 匹配，返回公司名称\n",
    "            if expected_name == company_name.lower():\n",
    "                print(f\"Returning company name: {company_name}\")  # 打印调试信息\n",
    "                return company_name\n",
    "\n",
    "            # 如果没有匹配到 expected_name，默认返回公司名称\n",
    "            print(f\"Returning default company name: {company_name}\")  # 打印调试信息\n",
    "            return company_name\n",
    "        else:\n",
    "            print(\"Match score below threshold, returning empty\")  # 打印调试信息\n",
    "            return ''\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def main():\n",
    "    # 初始化 CompanyMatcher\n",
    "    company_matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "\n",
    "    # 加载公司数据\n",
    "    companies = company_matcher.load_company_data()\n",
    "\n",
    "    # 准备匹配数据，并且加载到 NameMatcher 中\n",
    "    matching_data = company_matcher.prepare_matching_data(companies)\n",
    "    company_matcher.matching_data = matching_data  # 确保 matching_data 被正确赋值\n",
    "\n",
    "    company_matcher.prepare_data_for_matching(matching_data)  # 加载并处理主数据\n",
    "\n",
    "    # 获取用户输入并进行匹配\n",
    "    user_input = input(\"Enter a company name: \")\n",
    "    \n",
    "    # 使用 match_user_input 来进行公司匹配\n",
    "    matched_company = company_matcher.match_user_input(user_input, expected_name=None)  # 如果没有预期值，可以传入 None\n",
    "    print(matched_company)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b69b9c-8883-4bb8-b759-e9be345e2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V10A Unittest\n",
    "#Working Unittest code with aliases match\n",
    "import unittest\n",
    "import pandas as pd\n",
    "from name_matching_for_company import CompanyMatcher\n",
    "import csv\n",
    "import time\n",
    "\n",
    "\n",
    "class NameNormalizer:\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"Normalize circumflex letters in the name.\"\"\"\n",
    "        # 如果 name 不是字符串，将其转换为空字符串\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # 执行字符串正则替换\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class TestCompanyMatcher(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up the CompanyMatcher and load the data once for all tests.\"\"\"\n",
    "        cls.matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "\n",
    "        # 加载公司数据\n",
    "        cls.companies = cls.matcher.load_company_data()\n",
    "\n",
    "        # Prepare matching data\n",
    "        cls.matcher.matching_data = cls.matcher.prepare_matching_data(cls.companies)\n",
    "\n",
    "        # Load and process the master data for matching\n",
    "        cls.matcher.prepare_data_for_matching(cls.matcher.matching_data)  # 加载并处理主数据\n",
    "\n",
    "        # 打印 matching_data 样本，检查别名是否加载正确\n",
    "        print(\"Sample of matching data in setUpClass:\")\n",
    "        print(cls.matcher.matching_data[['name', 'companyName', 'alias_name']].head(10))\n",
    "\n",
    "    def test_company_name_matching(self):\n",
    "        \"\"\"Test the company name matching with input from the CSV file.\"\"\"\n",
    "        test_data = pd.read_csv('filtered_company_names.csv')\n",
    "\n",
    "        with open('company_name_matching_results.csv', 'w', newline='', encoding='utf-8') as output_file:\n",
    "            writer = csv.writer(output_file)\n",
    "            writer.writerow(['company name', 'expected company name', 'output', 'true/false', 'time (seconds)'])\n",
    "\n",
    "            for index, row in test_data.iterrows():\n",
    "                input_name = row['company name']\n",
    "                expected_name = row['expected company name']\n",
    "\n",
    "                start_time = time.time()\n",
    "\n",
    "                # Match the user input with the company name or alias\n",
    "                output_name = self.matcher.match_user_input(input_name, expected_name)  # 传入 expected_name 参数\n",
    "\n",
    "                end_time = time.time()\n",
    "                elapsed_time = end_time - start_time\n",
    "\n",
    "                # 确保 expected_name 和 output_name 为字符串，防止 AttributeError\n",
    "                if pd.isna(expected_name):\n",
    "                    expected_name = ''\n",
    "                if pd.isna(output_name):\n",
    "                    output_name = ''\n",
    "\n",
    "                # 如果 output 和 expected 都是空，则设置 result 为 True\n",
    "                if output_name == '' and expected_name == '':\n",
    "                    result = True\n",
    "                else:\n",
    "                    result = output_name.lower() == expected_name.lower()\n",
    "\n",
    "                # 写入结果到输出 CSV\n",
    "                writer.writerow([input_name, expected_name, output_name, result, elapsed_time])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d00010-f069-43e6-a784-258ffef3ec3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V10B\n",
    "#match_score changes to threshold, threshold will be difined in V10B unittest\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "\n",
    "class NameNormalizer:\n",
    "    \"\"\"\n",
    "    CompanyMatcher is responsible for matching company names against a list of known companies and their aliases.\n",
    "\n",
    "    Attributes:\n",
    "    - company_data_file (str): Path to the JSONL file containing company data.\n",
    "    - matcher (NameMatcher): A NameMatcher object used for name matching.\n",
    "    - matching_data (pd.DataFrame): DataFrame storing processed company names and aliases.\n",
    "    \"\"\"\n",
    "    CIRCUMFLEX_PAIRS = (\n",
    "        (r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'),\n",
    "        (r'ðđ', 'd'), (r'ÐĐ', 'D'), (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'),\n",
    "        (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'), (r'Ł', 'L'), (r'ł', 'l'),\n",
    "        (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "        (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'),\n",
    "        (r'ŵ', 'w'), (r'ý', 'y'), (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE')\n",
    "    )\n",
    "    CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "        (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"Normalize circumflex letters in the name.\"\"\"\n",
    "        # 如果 name 不是字符串，将其转换为空字符串\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # 执行字符串正则替换\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class CompanyMatcher:\n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(\n",
    "        rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\")\n",
    "    WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "    COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "        r\"(实验室|研究院|技术|科技)|(\\b(?:laboratory|laboratories|lab|labs|research|technology|tech|science|sci)(\\.|\\b|$))\",\n",
    "        re.I\n",
    "    )\n",
    "    PREPOSITION_WORDS = {\n",
    "        'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as',\n",
    "        'at',\n",
    "        'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "        'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'following',\n",
    "        'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite',\n",
    "        'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'since', 'than', 'through', 'till', 'to',\n",
    "        'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within',\n",
    "        'without'\n",
    "    }\n",
    "    CONJUNCTIONS_WORDS = {\n",
    "        \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "        \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "        \"such\", \"so\", \"that\"\n",
    "    }\n",
    "    ARTICLES = {'a', 'an', 'the'}\n",
    "    INLINE_WORDS = PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "    ORG_SUFFIX_WORDS = {\n",
    "        'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "               'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "        'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                          'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "               'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                            'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp',\n",
    "                            'sro',\n",
    "                            'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "    }\n",
    "    ORG_SUFFIX_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "            rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "        ),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "        r'\\b(bank|college|university|education|energy|finance|army|air force|navy|industry|'\n",
    "        r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "        r'environmental|government|state|'\n",
    "        r'global|market(?:ing)?|real estate|system)\\b',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    def __init__(self, company_data_file: str):\n",
    "        self.company_data_file = company_data_file\n",
    "        self.matcher = NameMatcher(remove_ascii=False, punctuations=False)\n",
    "        self.matching_data = None\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_chinese(text):\n",
    "        \"\"\"Helper function to check if a string contains Chinese characters.\"\"\"\n",
    "        CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "        return re.search(f\"[{CHINESE_CHARACTERS}]\", text)\n",
    "\n",
    "    def company_words_regulator(self, company_name: str) -> str:\n",
    "        \"\"\"Regulate company names using multiple techniques.\"\"\"\n",
    "        # Step 1: Normalize circumflex letters\n",
    "        search_str = NameNormalizer.circumflex_regulator(company_name)\n",
    "\n",
    "        # Step 2: Remove spaces if only Chinese is detected\n",
    "        if self.contains_chinese(search_str) and not re.search(r\"[a-zA-Z]\", search_str):\n",
    "            search_str = search_str.replace(' ', '')\n",
    "\n",
    "        # Step 3: Remove unnecessary company suffixes or common words\n",
    "        search_str = self.WHITESPACE_REGEX.sub(' ', self.ORG_SUFFIX_REGEX.sub('', search_str)).strip()\n",
    "\n",
    "        # Step 4: Filter out prepositions, conjunctions, and articles (INLINE_WORDS)\n",
    "        words = search_str.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in self.INLINE_WORDS]\n",
    "        search_str = ' '.join(filtered_words)\n",
    "\n",
    "        # Step 5: Check for essential company keywords\n",
    "        if self.COMPANY_KEYWORDS_REGEX.search(search_str):\n",
    "            return search_str\n",
    "\n",
    "        return search_str\n",
    "\n",
    "    def load_company_data(self) -> pd.DataFrame:\n",
    "        \"\"\"加载公司数据并确保别名信息正确存在.\"\"\"\n",
    "        companies = []\n",
    "        with open(self.company_data_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                company = json.loads(line)\n",
    "                company['aliases'] = company.get('aliases', [])  # 确保别名存在\n",
    "                companies.append(company)\n",
    "        return pd.DataFrame(companies)\n",
    "\n",
    "    def prepare_matching_data(self, companies: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Prepare data for matching, ensuring aliases and required search strings are correctly loaded as individual entries.\"\"\"\n",
    "        rows = []\n",
    "        for _, company in companies.iterrows():\n",
    "            # Process required search strings\n",
    "            required_search_strings = company.get(\"requiredSearchStrings\", [])\n",
    "            if not isinstance(required_search_strings, list):\n",
    "                required_search_strings = []  # Ensure it's a list if not iterable\n",
    "\n",
    "            # Combine required search strings and aliases into a full alias list\n",
    "            aliases = company.get(\"aliases\", [])\n",
    "            if not isinstance(aliases, list):\n",
    "                aliases = []  # Ensure aliases is a list if it's not iterable\n",
    "\n",
    "            # Merge requiredSearchStrings and aliases into a full alias list\n",
    "            full_alias_list = required_search_strings + aliases\n",
    "\n",
    "            # For each alias, create a separate row with all aliases as alias_name\n",
    "            for alias in full_alias_list:\n",
    "                rows.append({\n",
    "                    \"name\": alias,\n",
    "                    \"companyName\": company.get(\"companyName\", \"\"),\n",
    "                    \"alias_name\": full_alias_list,  # 将完整的别名列表作为 alias_name 存储\n",
    "                    \"is_alias\": alias in aliases  # 标记是否为别名\n",
    "                })\n",
    "\n",
    "        # Return the prepared matching data as a DataFrame\n",
    "        matching_data = pd.DataFrame(rows)\n",
    "        return matching_data\n",
    "\n",
    "    def tokenize(self, text: str) -> str:\n",
    "        \"\"\"Tokenize the text using Jieba for Chinese words.\"\"\"\n",
    "        return \" \".join(jieba.cut(text))\n",
    "\n",
    "    def prepare_data_for_matching(self, companies: pd.DataFrame):\n",
    "        \"\"\"Tokenize and prepare company data for matching.\"\"\"\n",
    "        # 对公司数据进行分词处理\n",
    "        companies['name'] = companies['name'].apply(self.tokenize)\n",
    "\n",
    "        # Load and process master data for matching\n",
    "        self.matcher.load_and_process_master_data(column=\"name\", df_matching_data=companies)  # 确保主数据加载\n",
    "\n",
    "    def match_user_input(self, user_input: str, expected_name: str, threshold: int = 70) -> str:\n",
    "        \"\"\"Match user input against the company names, then validate against expected name, with dynamic threshold.\"\"\"\n",
    "\n",
    "        # 对输入进行标准化和分词处理\n",
    "        print(f\"User input: {user_input}\")  # 打印调试信息\n",
    "        user_input_segmented = self.company_words_regulator(user_input)\n",
    "        print(f\"Segmented input: {user_input_segmented}\")  # 打印调试信息\n",
    "        user_input_segmented = \" \".join(jieba.cut(user_input_segmented))\n",
    "        print(f\"Segmented and tokenized input: {user_input_segmented}\")  # 打印调试信息\n",
    "        to_be_matched = pd.DataFrame({\"name\": [user_input_segmented]})\n",
    "\n",
    "        # 匹配用户输入的名称\n",
    "        result = self.matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "        print(f\"Matching result: {result}\")  # 打印调试信息\n",
    "\n",
    "        # 如果没有匹配结果，返回空字符串\n",
    "        if result.empty:\n",
    "            print(\"No match found\")  # 打印调试信息\n",
    "            return ''\n",
    "\n",
    "        # 获取最匹配的结果\n",
    "        best_match = result.iloc[0][\"match_name\"]\n",
    "        match_score = result.iloc[0][\"score\"]\n",
    "        print(f\"Best match: {best_match}, Match score: {match_score}\")  # 打印调试信息\n",
    "\n",
    "        # 使用传入的动态 threshold 参数,match_score\n",
    "        if match_score >= threshold:\n",
    "            # 通过 best_match 查找公司名称和别名\n",
    "            matched_row = self.matching_data[self.matching_data['name'] == best_match]\n",
    "            if matched_row.empty:\n",
    "                print(f\"No matching row found for: {best_match}\")  # 打印调试信息\n",
    "                return ''\n",
    "\n",
    "            # 获取公司名称和别名信息\n",
    "            company_name = matched_row.iloc[0]['companyName']\n",
    "            aliases = matched_row.iloc[0]['alias_name']  # 这现在是一个包含所有别名的列表\n",
    "\n",
    "            # 打印公司名称和别名\n",
    "            print(f\"Company name: {company_name}, Aliases: {aliases}\")  # 打印当前公司名称和别名\n",
    "\n",
    "            # 处理 expected_name，防止 NaN 导致错误\n",
    "            if pd.isna(expected_name):\n",
    "                expected_name = \"\"  # 如果是 NaN，替换为空字符串\n",
    "            else:\n",
    "                expected_name = expected_name.lower()\n",
    "\n",
    "            print(f\"Expected name: {expected_name}\")  # 打印 expected_name\n",
    "\n",
    "            # 检查 expected_name 是否在别名中\n",
    "            if expected_name and expected_name in [alias.lower() for alias in aliases]:\n",
    "                print(f\"Matched alias: {expected_name}\")  # 打印匹配到的别名\n",
    "                return expected_name\n",
    "\n",
    "            # 如果 expected_name 和 company_name 匹配，返回公司名称\n",
    "            if expected_name == company_name.lower():\n",
    "                print(f\"Returning company name: {company_name}\")  # 打印调试信息\n",
    "                return company_name\n",
    "\n",
    "            # 如果没有匹配到 expected_name，默认返回公司名称\n",
    "            print(f\"Returning default company name: {company_name}\")  # 打印调试信息\n",
    "            return company_name\n",
    "        else:\n",
    "            print(\"Match score below threshold, returning empty\")  # 打印调试信息\n",
    "            return ''\n",
    "\n",
    "\n",
    "# Usage example:\n",
    "def main():\n",
    "    # 初始化 CompanyMatcher\n",
    "    company_matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "\n",
    "    # 加载公司数据\n",
    "    companies = company_matcher.load_company_data()\n",
    "\n",
    "    # 准备匹配数据，并且加载到 NameMatcher 中\n",
    "    matching_data = company_matcher.prepare_matching_data(companies)\n",
    "    company_matcher.matching_data = matching_data  # 确保 matching_data 被正确赋值\n",
    "\n",
    "    company_matcher.prepare_data_for_matching(matching_data)  # 加载并处理主数据\n",
    "\n",
    "    # 获取用户输入并进行匹配\n",
    "    user_input = input(\"Enter a company name: \")\n",
    "\n",
    "    # 使用 match_user_input 来进行公司匹配\n",
    "    matched_company = company_matcher.match_user_input(user_input, expected_name=None)  # 如果没有预期值，可以传入 None\n",
    "    print(matched_company)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8971630e-9cb1-4fe9-ba6c-3c95e3bf3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V10B unittest\n",
    "#This is for people who want's to test multiple match_score instead of just one\n",
    "import unittest\n",
    "import pandas as pd\n",
    "import csv\n",
    "import time\n",
    "from name_matching_for_company import CompanyMatcher\n",
    "class TestCompanyMatcher(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up the CompanyMatcher and load the data once for all tests.\"\"\"\n",
    "        cls.matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "        cls.companies = cls.matcher.load_company_data()\n",
    "        cls.matcher.matching_data = cls.matcher.prepare_matching_data(cls.companies)\n",
    "        cls.matcher.prepare_data_for_matching(cls.matcher.matching_data)\n",
    "\n",
    "    def test_company_name_matching_dynamic_threshold(self):\n",
    "        \"\"\"Test the company name matching with dynamic threshold.\"\"\"\n",
    "        test_data = pd.read_csv('filtered_company_names.csv')\n",
    "\n",
    "        # Loop from match_score >= 0 to match_score >= 100\n",
    "        for threshold in range(0, 101):#This range can change to whatever you want\n",
    "            with self.subTest(threshold=threshold):  # Use unittest subTest for each threshold\n",
    "                # Output CSV file for this specific threshold\n",
    "                output_filename = f'company_name_matching_results_threshold_{threshold}.csv'\n",
    "\n",
    "                # Prepare output file\n",
    "                with open(output_filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "                    writer = csv.writer(output_file)\n",
    "                    writer.writerow(['company name', 'expected company name', 'output', 'true/false', 'time (seconds)'])\n",
    "\n",
    "                    # Iterate over test data\n",
    "                    for index, row in test_data.iterrows():\n",
    "                        input_name = row['company name']\n",
    "                        expected_name = row['expected company name']\n",
    "\n",
    "                        start_time = time.time()\n",
    "\n",
    "                        # Call match_user_input with the dynamic threshold\n",
    "                        output_name = self.matcher.match_user_input(input_name, expected_name, threshold=threshold)\n",
    "\n",
    "                        end_time = time.time()\n",
    "                        elapsed_time = end_time - start_time\n",
    "\n",
    "                        # Ensure expected_name and output_name are strings\n",
    "                        expected_name = '' if pd.isna(expected_name) else expected_name\n",
    "                        output_name = '' if pd.isna(output_name) else output_name\n",
    "\n",
    "                        # Determine if the output matches the expected name\n",
    "                        result = output_name.lower() == expected_name.lower()\n",
    "\n",
    "                        # Write the result to the CSV file\n",
    "                        writer.writerow([input_name, expected_name, output_name, result, elapsed_time])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1a4286-781f-4b2e-bed7-58df66f9a4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V10B Score and Graph calculation\n",
    "#Calculate accuracy, precision, recall, and F1-score, also gives graph of these score\n",
    "#This needs to run after V10B unittest\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 定义处理的起始和结束阈值（可以灵活调整）\n",
    "start_threshold = 0  # 起始阈值\n",
    "end_threshold = 3  # 结束阈值\n",
    "\n",
    "# 初始化空的列表，用来存储每个文件的结果\n",
    "results = []\n",
    "\n",
    "# 循环处理指定范围内的 CSV 文件\n",
    "for i in range(start_threshold, end_threshold + 1):  # 范围是从 start 到 end\n",
    "    # 构造文件名\n",
    "    file_name = f'company_name_matching_results_threshold_{i}.csv'\n",
    "\n",
    "    # 加载文件\n",
    "    try:\n",
    "        data = pd.read_csv(file_name)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_name} not found. Skipping...\")\n",
    "        continue\n",
    "\n",
    "    # 计算 TP, FP, FN, TN\n",
    "    tp = data[(data['true/false'] == True) & (data['output'].notna())].shape[0]\n",
    "    fp = data[(data['true/false'] == False) & (data['output'].notna())].shape[0]\n",
    "    fn = data[(data['true/false'] == False) & (data['output'].isna())].shape[0]\n",
    "    tn = data[(data['true/false'] == True) & (data['output'].isna())].shape[0]\n",
    "\n",
    "    # 计算 Precision, Accuracy, Recall, F1\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # 将结果保存到字典中\n",
    "    results.append({\n",
    "        'threshold': i,\n",
    "        'TP': tp,\n",
    "        'FP': fp,\n",
    "        'FN': fn,\n",
    "        'TN': tn,\n",
    "        'Precision': precision,\n",
    "        'Accuracy': accuracy,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1_score\n",
    "    })\n",
    "\n",
    "# 将结果转换为 DataFrame 并保存到文件\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('calculate_score.csv', index=False)\n",
    "\n",
    "# 绘制 Precision, Recall, F1-score 和 Accuracy 曲线\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_df['threshold'], results_df['Precision'], label='Precision', marker='o')\n",
    "plt.plot(results_df['threshold'], results_df['Recall'], label='Recall', marker='o')\n",
    "plt.plot(results_df['threshold'], results_df['F1-score'], label='F1-score', marker='o')\n",
    "plt.plot(results_df['threshold'], results_df['Accuracy'], label='Accuracy', marker='o')\n",
    "\n",
    "# 设置x轴为每隔5个显示一个刻度\n",
    "plt.xticks(ticks=range(0, results_df['threshold'].max() + 1, 2))\n",
    "\n",
    "# 设置标题和标签\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Performance Metrics vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c28e8c-28f3-4de1-8e59-80e374f34069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V11A\n",
    "#Can Use V10A Unittest\n",
    "#Add suffix process\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "# A regular expression to match and replace certain symbols with spaces, including slashes, percentage signs, exclamations, punctuation, hyphens, and special symbols.\n",
    "ERASE_OR_REPLACE_TO_SPACE_SYMBOLS = re.compile(r'''[/\\\\%!！.?？^*·•⦁\\-]''')\n",
    "\n",
    "# A regular expression to match entity names that start with \"the\" followed by alphanumeric characters, case-insensitive.\n",
    "THE_SPECIAL_ENTITY_NAME_REGEX = re.compile(r'^(the\\s+[a-z0-9]+)$', re.IGNORECASE)\n",
    "\n",
    "# A regular expression to match sequences of single letters separated by spaces (e.g., \"a b c\") and find those patterns, case-insensitive.\n",
    "SINGLE_LETTER_SEQUENTIAL_REGEX = re.compile(rf'(\\b[a-z]\\b)(\\s+[a-z]\\b)+', re.IGNORECASE)\n",
    "\n",
    "# A regular expression to match single lowercase letters surrounded by spaces (e.g., \" a \").\n",
    "SINGLE_LOWERCASE_REGEX = re.compile(rf'\\s+[a-z]\\s+')\n",
    "\n",
    "# A regular expression to match and remove invalid symbols in company names, such as punctuation, special characters, and spaces around hyphens.\n",
    "REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "\n",
    "# A regular expression to match symbols that should be replaced with spaces, including ampersands, question marks, asterisks, parentheses, commas, and hyphens.\n",
    "REPLACE_TO_SPACE_SYMBOLS = re.compile(r'[&?#*|=_+()（）、,，–]')\n",
    "\n",
    "class NameNormalizer:\n",
    "    \"\"\"\n",
    "    This class handles the normalization of names, including circumflex letters and symbols.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pairing circumflex letters with their normalized counterparts\n",
    "    CIRCUMFLEX_PAIRS = (\n",
    "        (r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'),\n",
    "        (r'ðđ', 'd'), (r'ÐĐ', 'D'), (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'),\n",
    "        (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'), (r'Ł', 'L'), (r'ł', 'l'),\n",
    "        (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "        (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'),\n",
    "        (r'ŵ', 'w'), (r'ý', 'y'), (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE')\n",
    "    )\n",
    "\n",
    "    # Compile regex for circumflex replacements\n",
    "    CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "        (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize circumflex letters in the name, e.g., 'é' becomes 'e'.\n",
    "        If name is not a string, return an empty string.\n",
    "        \"\"\"\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # Replace circumflex letters using pre-compiled regex pairs\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class CompanyMatcher:\n",
    "    \"\"\"\n",
    "    This class handles company name matching using various techniques like tokenization, symbol regulation,\n",
    "    and name normalization.\n",
    "    \"\"\"\n",
    "    # Define common words for both English and Chinese\n",
    "    COMPANY_COMMON_WORDS = {\n",
    "        \"en\": {\n",
    "            \"laboratory\", \"laboratories\", \"lab\", \"labs\", \"research\", \"technology\", \"technologies\",\n",
    "            \"technical\", \"tech\", \"sci\", \"science\"\n",
    "        },\n",
    "        \"zh\": {\n",
    "            \"实验室\", \"研究院\", \"技术\", \"科技\"\n",
    "        }\n",
    "    }\n",
    "    COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "        r'\\b(bank|college|university|education|energy|finance|army|air foce|navy|industry|'\n",
    "        r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "        r'environmental|gover(?:nor?|ment)|state|estado|procter|reliance|genome|software|hardware|agency|'\n",
    "        r'(?:sou|nor)th(?:[-\\s]?(?:ea|we)st(?:ern)?)?|[东西南北]方|[东西][南北][方]?|环境|'\n",
    "        r'银行|软件|硬件|大学|机构|研究院|学院|教育|协会|能源|金融|旅馆|医院|石油|电子|工业|[陆海空]军|政府|基因|'\n",
    "        r'global|market(?:ing)?|real estate|system|房地产|系统|兴业)\\b',\n",
    "        re.IGNORECASE)\n",
    "\n",
    "    # Compile regex pattern for matching common words in company names\n",
    "    COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join(COMPANY_COMMON_WORDS['zh']),\n",
    "            rf\"(\\b|^)({'|'.join(COMPANY_COMMON_WORDS['en'])})(\\.|\\b|$)\"\n",
    "        ), re.I\n",
    "    )\n",
    "    # Define ranges for CJK (Chinese, Japanese, Korean) characters\n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "\n",
    "    # Regex to match CJK characters or numeric patterns\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(\n",
    "        rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\"\n",
    "    )\n",
    "\n",
    "    WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "\n",
    "    # Common prepositions\n",
    "    PREPOSITION_WORDS = {\n",
    "        'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as',\n",
    "        'at',\n",
    "        'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "        'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'following',\n",
    "        'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite',\n",
    "        'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'since', 'than', 'through', 'till', 'to',\n",
    "        'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within',\n",
    "        'without'\n",
    "    }\n",
    "\n",
    "    # Common conjunctions\n",
    "    CONJUNCTIONS_WORDS = {\n",
    "        \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "        \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "        \"such\", \"so\", \"that\"\n",
    "    }\n",
    "\n",
    "    # Common articles\n",
    "    ARTICLES = {'a', 'an', 'the'}\n",
    "\n",
    "    # Merge all irrelevant words (prepositions, conjunctions, articles, etc.)\n",
    "    INLINE_WORDS = {\n",
    "                       'also', 'am', 'are', 'did', 'furthermore', 'has', 'hence', 'how', 'however', 'includ.',\n",
    "                       'instead', 'is', 'likewise',\n",
    "                       'long', 'moreover', 'should', 'similar', 'though', 'thus', 'unless', 'was', 'were', 'what',\n",
    "                       'which', 'whichever',\n",
    "                       'why', 'will',\n",
    "                       # Portugese prepositions\n",
    "                       'de', 'di', 'em', 'del', 'des', 'do',\n",
    "                       # 意大利语介词\n",
    "                       'delle',\n",
    "                       # Spanish prepositions\n",
    "                       'con', 'sobre', 'en', 'contra', 'desde', 'entre', 'hacia', 'por', 'la',\n",
    "                       # French prepositions\n",
    "                       'apres', 'avant', 'avec', 'chez', 'contre', 'dans', 'depuis', 'derriere', 'devant', 'durant',\n",
    "                       'envers', 'environ',\n",
    "                       'jusque', 'malgre', 'par', 'parmi', 'pendant', 'pour', 'sans', 'selon', 'sous', 'suivant', 'sur',\n",
    "                       'vers'\n",
    "                   } | PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "    # Common suffixes in company names (e.g. Ltd, Company, etc.)\n",
    "    ORG_SUFFIX_WORDS = {\n",
    "        'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "               'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "        'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                          'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "               'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                            'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp',\n",
    "                            'sro',\n",
    "                            'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "    }\n",
    "    REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "    # Compile regular expressions for matching company suffixes\n",
    "    ORG_SUFFIX_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "            rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "        ),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    def __init__(self, company_data_file: str):\n",
    "        self.company_data_file = company_data_file\n",
    "        self.matcher = NameMatcher(remove_ascii=False, punctuations=False)\n",
    "        self.matching_data = None\n",
    "\n",
    "    def load_company_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load the company data from a JSON Lines (.jsonl) file into a pandas DataFrame.\n",
    "        Ensures aliases and required search strings are properly formatted.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: DataFrame containing company data including names and aliases.\n",
    "        \"\"\"\n",
    "        companies = []\n",
    "        with open(self.company_data_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                company = json.loads(line)\n",
    "                company['aliases'] = company.get('aliases', [])  # 确保别名存在\n",
    "                companies.append(company)\n",
    "        return pd.DataFrame(companies)\n",
    "\n",
    "    def tokenize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenize the input text using Jieba for Chinese words and return the tokenized string.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The text to tokenize (usually the company name).\n",
    "\n",
    "        Returns:\n",
    "        - str: Tokenized text.\n",
    "        \"\"\"\n",
    "        # Use jieba to segment the text and concatenate the results with spaces into a string and return it\n",
    "        return \" \".join(jieba.cut(text))\n",
    "\n",
    "    def search_string_regulator(self, name: str, return_str=True, is_company_name=False) -> str:\n",
    "        \"\"\"\n",
    "        Regulate the search string to remove meaningless words and symbols. Handles both Chinese and English.\n",
    "\n",
    "        :param name: The company name or string to regulate.\n",
    "        :param return_str: If True, return the result as a string; otherwise, return as a list of words.\n",
    "        :param is_company_name: Special flag to avoid removing certain common words for company names.\n",
    "        \"\"\"\n",
    "        # Remove meaningless parts of the company name\n",
    "        extracted_name = re.sub(r'[^\\w\\s]', '', name)  # Adjust this regex as per your needs\n",
    "        if not extracted_name:\n",
    "            return '' if return_str else []\n",
    "\n",
    "        words = []\n",
    "        # Split based on spaces or known separators\n",
    "        for segment in extracted_name.split():\n",
    "            if not segment:\n",
    "                continue\n",
    "            # If the segment is Chinese, treat it as a whole word\n",
    "            if self.contains_chinese(segment):\n",
    "                words.append(segment)\n",
    "            # If it's a meaningful English word, keep it\n",
    "            else:\n",
    "                segment = segment.lower()\n",
    "                if segment not in words or is_company_name:\n",
    "                    words.append(segment)\n",
    "\n",
    "        # Return as a single string or a list\n",
    "        return ' '.join(words) if return_str else words\n",
    "\n",
    "    def regulate_english_asians_mixed_string(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        This function handles the regulation of mixed English and Asian (CJK) strings.\n",
    "        It splits the input string into English, numbers, and CJK segments, then rejoins them.\n",
    "        \"\"\"\n",
    "        # 调用 split_english_number_cjk 方法并传入正确的参数\n",
    "        return ' '.join(self.split_english_number_cjk(text, separate_return=False))\n",
    "\n",
    "    def split_english_number_cjk(self, text: str, separate_return: bool = True):\n",
    "        \"\"\"\n",
    "        A utility function that splits text into English, numbers, and CJK (Chinese, Japanese, Korean) characters.\n",
    "        This is a basic version to simulate the behavior.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        CJK_REGEX = r'[\\u4e00-\\u9fff\\uf900-\\ufaff]+'  # Simplified regex to match CJK characters.\n",
    "        EN_NUM_REGEX = r'[a-zA-Z0-9]+'\n",
    "\n",
    "        cjk_parts = re.findall(CJK_REGEX, text)\n",
    "        en_num_parts = re.findall(EN_NUM_REGEX, text)\n",
    "\n",
    "        if separate_return:\n",
    "            return en_num_parts, cjk_parts  # Return both parts separately\n",
    "        else:\n",
    "            return en_num_parts + cjk_parts  # Concatenate the parts if not separating\n",
    "\n",
    "    def prepare_matching_data(self, companies: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare data for matching, ensuring aliases and required search strings are correctly loaded as individual entries.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for _, company in companies.iterrows():\n",
    "            # Process required search strings, ensure it's a list\n",
    "            required_search_strings = company.get(\"requiredSearchStrings\", [])\n",
    "            if not isinstance(required_search_strings, list):\n",
    "                required_search_strings = []  # If it's not a list, convert to empty list\n",
    "\n",
    "            # Ensure aliases are also a list\n",
    "            aliases = company.get(\"aliases\", [])\n",
    "            if not isinstance(aliases, list):\n",
    "                aliases = []  # If it's not a list, convert to empty list\n",
    "\n",
    "            # Combine required search strings and aliases into one list\n",
    "            full_alias_list = required_search_strings + aliases\n",
    "\n",
    "            # For each alias, create a separate row with all aliases as alias_name\n",
    "            for alias in full_alias_list:\n",
    "                rows.append({\n",
    "                    \"name\": alias,\n",
    "                    \"companyName\": company.get(\"companyName\", \"\"),\n",
    "                    \"alias_name\": full_alias_list,  # Store the full alias list\n",
    "                    \"is_alias\": alias in aliases  # Mark if this is an alias\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def prepare_data_for_matching(self, companies: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Prepare the company data for matching by tokenizing the names and loading them into the matcher.\n",
    "        \"\"\"\n",
    "        companies['name'] = companies['name'].apply(self.tokenize)  # Tokenize company names\n",
    "        self.matcher.load_and_process_master_data(column=\"name\", df_matching_data=companies)\n",
    "\n",
    "    def match_user_input(self, user_input: str, expected_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Match user input against the company names, then validate against expected name.\n",
    "        \"\"\"\n",
    "\n",
    "        # Input is NaN or non-string, convert it to an empty string\n",
    "        if not isinstance(user_input, str) or pd.isna(user_input):\n",
    "            user_input = ''\n",
    "\n",
    "        # Call interpunction_regulator to process the symbols entered by the user\n",
    "        user_input = self.interpunction_regulator(user_input)  # 处理符号\n",
    "        if not user_input:\n",
    "            return ''\n",
    "\n",
    "        # Normalize and tokenize the input\n",
    "        user_input_segmented = self.company_words_regulator(user_input)\n",
    "        user_input_segmented = \" \".join(jieba.cut(user_input_segmented))\n",
    "        to_be_matched = pd.DataFrame({\"name\": [user_input_segmented]})\n",
    "\n",
    "        # Matches the name entered by the user\n",
    "        result = self.matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "        # No match, returns an empty string\n",
    "        if result.empty:\n",
    "            return ''\n",
    "\n",
    "        # Get the best matching results\n",
    "        best_match = result.iloc[0][\"match_name\"]\n",
    "        match_score = result.iloc[0][\"score\"]\n",
    "\n",
    "        # If the match score exceeds 70%, it is considered a successful match.\n",
    "        if match_score >= 70:\n",
    "            # 通过 best_match 查找公司名称和别名\n",
    "            matched_row = self.matching_data[self.matching_data['name'] == best_match]\n",
    "            if matched_row.empty:\n",
    "                return ''\n",
    "\n",
    "            # 获取公司名称和别名信息\n",
    "            company_name = matched_row.iloc[0]['companyName']\n",
    "            aliases = matched_row.iloc[0]['alias_name']  # 这现在是一个包含所有别名的列表\n",
    "\n",
    "            # 处理 expected_name，防止 NaN 导致错误\n",
    "            if pd.isna(expected_name):\n",
    "                expected_name = \"\"  # 如果是 NaN，替换为空字符串\n",
    "            else:\n",
    "                expected_name = expected_name.lower()\n",
    "\n",
    "            # 检查 expected_name 是否在别名中\n",
    "            if expected_name and expected_name in [alias.lower() for alias in aliases]:\n",
    "                return expected_name\n",
    "\n",
    "            # 如果 expected_name 和 company_name 匹配，返回公司名称\n",
    "            if expected_name == company_name.lower():\n",
    "                return company_name\n",
    "\n",
    "            # 如果没有匹配到 expected_name，默认返回公司名称\n",
    "            return company_name\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_chinese(text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the input text contains any Chinese characters.\n",
    "        \"\"\"\n",
    "        CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "        return bool(re.search(f\"[{CHINESE_CHARACTERS}]\", text))\n",
    "\n",
    "    def company_words_regulator(self, company_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean up and regulate the company name by applying normalization rules for circumflexes, symbols,\n",
    "        removing unnecessary company suffixes, and common words.\n",
    "        \"\"\"\n",
    "        # Step 1: Normalize circumflex letters and process interpunction\n",
    "        search_str = self.interpunction_regulator(NameNormalizer.circumflex_regulator(company_name))\n",
    "\n",
    "        if not search_str:\n",
    "            return ''\n",
    "\n",
    "        # Step 2: Check if the string contains Chinese and process accordingly\n",
    "        if self.contains_chinese(search_str):\n",
    "            if not re.search(r\"[a-zA-Z]\", search_str):\n",
    "                search_str = search_str.replace(' ', '')\n",
    "\n",
    "        # Step 3: Remove company suffixes like 'inc', 'corp', etc., potentially twice to handle two occurrences\n",
    "        search_str_drop = self.WHITESPACE_REGEX.sub(' ', self.ORG_SUFFIX_REGEX.sub('', self.ORG_SUFFIX_REGEX.sub('',\n",
    "                                                                                                                 search_str).strip())).strip()\n",
    "\n",
    "        # The string becomes empty after removing the suffixes, return an empty string\n",
    "        if not search_str_drop:\n",
    "            return ''\n",
    "\n",
    "        # Step 4: Remove stock exchange abbreviations, e.g., 'SSE plc', 'NYSE', 'NASDAQ'\n",
    "        search_str_drop_bourse = self.COMPANY_COMMON_WORDS_REGEX.sub(' ', search_str_drop).strip()\n",
    "        if search_str_drop_bourse:\n",
    "            search_str_drop = search_str_drop_bourse\n",
    "\n",
    "        # Step 5: Remove common company words like 'technology', 'research', etc.\n",
    "        search_str_drop_common_word = self.WHITESPACE_REGEX.sub(' ', self.COMPANY_COMMON_WORDS_REGEX.sub('',\n",
    "                                                                                                         search_str_drop)).strip()\n",
    "        if search_str_drop_common_word:\n",
    "            search_str_drop = search_str_drop_common_word\n",
    "\n",
    "        # Step 6: Ensure that removing common words doesn't leave just a keyword, e.g., 'Corporation Bank'\n",
    "        if (r_ := self.COMPANY_KEYWORDS_REGEX.search(search_str_drop)) and r_.end() - r_.start() > len(\n",
    "                search_str_drop) - 2:\n",
    "            pass\n",
    "        elif search_str == search_str_drop:\n",
    "            pass\n",
    "        else:\n",
    "            search_str = search_str_drop\n",
    "\n",
    "        # Step 7: Further regulate the search string and convert to lowercase\n",
    "        search_str_regulate = self.search_string_regulator(search_str, is_company_name=True).lower()\n",
    "        if search_str_regulate != search_str:\n",
    "            search_str = search_str_regulate\n",
    "\n",
    "        # Step 8: Remove invalid characters and filter valid words\n",
    "        valid_words = []\n",
    "        for word_ in search_str.split(' '):\n",
    "            word_ = self.REGEX_COMPANY_DROP_INVALID.sub(' ', word_).strip().lower()\n",
    "            if word_:\n",
    "                valid_words.append(word_)\n",
    "\n",
    "        # Return the cleaned and processed company name\n",
    "        return ' '.join(valid_words)\n",
    "\n",
    "    # Direct deletion symbols: '-' and '/'\n",
    "    ERASE_SYMBOLS = re.compile(r'[-/]')\n",
    "\n",
    "\n",
    "\n",
    "    def interpunction_regulator(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Regulates punctuation and symbols in the company name by:\n",
    "        - Removing or replacing certain symbols with spaces.\n",
    "        - Replacing special characters like '&' and '@' with words ('togetherwith', 'locatedat').\n",
    "        - Handling special cases like sequential single letters and 'the' followed by a short word.\n",
    "        \"\"\"\n",
    "        is_chinese = self.contains_chinese(name)\n",
    "\n",
    "        if not is_chinese:\n",
    "            # If name contains specific symbols, split and take the first word for further matching\n",
    "            if name_words := re.split(r'[|]+', name):\n",
    "                name = name_words[0]\n",
    "\n",
    "        # Replace '&' with 'togetherwith' and '@' with 'locatedat'\n",
    "        if and_match := re.search(r'&', name):\n",
    "            name = name.replace('&', 'togetherwith')\n",
    "        if at_match := re.search(r'@', name):\n",
    "            name = name.replace('@', 'locatedat')\n",
    "\n",
    "        # Replace specific symbols with spaces\n",
    "        name = REPLACE_TO_SPACE_SYMBOLS.sub(' ', name)\n",
    "\n",
    "        # Use ERASE_OR_REPLACE_TO_SPACE_SYMBOLS instead of ERASE_SYMBOLS\n",
    "        if is_chinese:\n",
    "            name = ERASE_OR_REPLACE_TO_SPACE_SYMBOLS.sub(' ', name).strip()\n",
    "        else:\n",
    "            name = ERASE_OR_REPLACE_TO_SPACE_SYMBOLS.sub('', name).strip()\n",
    "\n",
    "        # Handle special cases for names starting with 'the' followed by a short word\n",
    "        if the_match := THE_SPECIAL_ENTITY_NAME_REGEX.search(name):\n",
    "            connect_str = the_match.group(1)\n",
    "            name = name.replace(connect_str, re.sub(r'[\\s]+', 'thespecial', connect_str))\n",
    "\n",
    "        # Merge sequential single letters into one\n",
    "        before_merge_name = name\n",
    "        name = ''\n",
    "        while True:\n",
    "            if not (single_letter_sequential := SINGLE_LETTER_SEQUENTIAL_REGEX.search(before_merge_name)):\n",
    "                name += before_merge_name.strip()\n",
    "                break\n",
    "            name += before_merge_name[\n",
    "                    :single_letter_sequential.start()].strip() + ' ' + single_letter_sequential.group().replace(' ',\n",
    "                                                                                                                '') + ' '\n",
    "            before_merge_name = before_merge_name[single_letter_sequential.end():]\n",
    "\n",
    "        # Drop single lowercase letters, but not if they are at the start or end\n",
    "        name = SINGLE_LOWERCASE_REGEX.sub(' ', name)\n",
    "\n",
    "        if is_chinese:\n",
    "            name = self.regulate_english_asians_mixed_string(name)\n",
    "\n",
    "        return name if name else None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to initialize CompanyMatcher, load company data, and perform company name matching.\n",
    "    \"\"\"\n",
    "    # Initialize CompanyMatcher and pass in the path to the company data file\n",
    "    company_matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "\n",
    "    # Load Company Data\n",
    "    companies = company_matcher.load_company_data()\n",
    "\n",
    "    # Prepare matching data and organize company name and alias information into a format that can be matched\n",
    "    matching_data = company_matcher.prepare_matching_data(companies)\n",
    "    company_matcher.matching_data = matching_data\n",
    "\n",
    "    # Prepare company data for matching (e.g. word segmentation)\n",
    "    company_matcher.prepare_data_for_matching(matching_data)\n",
    "\n",
    "    # Get input from the user and match the company name\n",
    "    user_input = input(\"Enter a company name for matching: \")\n",
    "\n",
    "    # Using a matcher to perform company name matching\n",
    "    matched_company = company_matcher.match_user_input(user_input, expected_name=None)\n",
    "\n",
    "    if matched_company:\n",
    "        print(f\"Matched company: {matched_company}\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b317a571-2539-4f05-8ec8-a9d013a676c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V11B\n",
    "#Can Use V10B Unittest\n",
    "#Can Use V10B calculation and graph code\n",
    "#Add suffix process\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jieba\n",
    "from name_matching.name_matcher import NameMatcher\n",
    "\n",
    "# A regular expression to match and replace certain symbols with spaces, including slashes, percentage signs, exclamations, punctuation, hyphens, and special symbols.\n",
    "ERASE_OR_REPLACE_TO_SPACE_SYMBOLS = re.compile(r'''[/\\\\%!！.?？^*·•⦁\\-]''')\n",
    "\n",
    "# A regular expression to match entity names that start with \"the\" followed by alphanumeric characters, case-insensitive.\n",
    "THE_SPECIAL_ENTITY_NAME_REGEX = re.compile(r'^(the\\s+[a-z0-9]+)$', re.IGNORECASE)\n",
    "\n",
    "# A regular expression to match sequences of single letters separated by spaces (e.g., \"a b c\") and find those patterns, case-insensitive.\n",
    "SINGLE_LETTER_SEQUENTIAL_REGEX = re.compile(rf'(\\b[a-z]\\b)(\\s+[a-z]\\b)+', re.IGNORECASE)\n",
    "\n",
    "# A regular expression to match single lowercase letters surrounded by spaces (e.g., \" a \").\n",
    "SINGLE_LOWERCASE_REGEX = re.compile(rf'\\s+[a-z]\\s+')\n",
    "\n",
    "# A regular expression to match and remove invalid symbols in company names, such as punctuation, special characters, and spaces around hyphens.\n",
    "REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "\n",
    "# A regular expression to match symbols that should be replaced with spaces, including ampersands, question marks, asterisks, parentheses, commas, and hyphens.\n",
    "REPLACE_TO_SPACE_SYMBOLS = re.compile(r'[&?#*|=_+()（）、,，–]')\n",
    "\n",
    "class NameNormalizer:\n",
    "    \"\"\"\n",
    "    This class handles the normalization of names, including circumflex letters and symbols.\n",
    "    \"\"\"\n",
    "\n",
    "    # Pairing circumflex letters with their normalized counterparts\n",
    "    CIRCUMFLEX_PAIRS = (\n",
    "        (r'àâáãāåäǎ', 'a'), (r'ÀÂÁÃĀÅÄǍ', 'A'), (r'ß', 'b'), (r'čćç', 'c'), (r'ČÇ', 'C'),\n",
    "        (r'ðđ', 'd'), (r'ÐĐ', 'D'), (r'éěèêëėệē', 'e'), (r'ÉÈÊËĖỆĒ', 'E'), (r'ğ', 'g'),\n",
    "        (r'Ğ', 'G'), (r'îïíīịìǐĩı', 'i'), (r'İÎÏÍĪỊÌǏ', 'I'), (r'Ł', 'L'), (r'ł', 'l'),\n",
    "        (r'ñňń', 'n'), (r'ÑŇŃ', 'N'), (r'ôöōồǒóòøőõ', 'o'), (r'ÔÖŌỒÓÒØŐÕ', 'O'), (r'ŕ', 'r'),\n",
    "        (r'şš', 's'), (r'ŠŞ', 'S'), (r'ť', 't'), (r'ûùüúǔưū', 'u'), (r'ÛÙÜÚƯŪ', 'U'),\n",
    "        (r'ŵ', 'w'), (r'ý', 'y'), (r'Ý', 'Y'), (r'žź', 'z'), (r'ŽŹ', 'Z'), (r'æ', 'ae'), (r'Æ', 'AE')\n",
    "    )\n",
    "\n",
    "    # Compile regex for circumflex replacements\n",
    "    CIRCUMFLEX_REGEX_PAIRS = tuple(\n",
    "        (re.compile(rf'[{regex_}]'), format_) for (regex_, format_) in CIRCUMFLEX_PAIRS\n",
    "    )\n",
    "\n",
    "    @classmethod\n",
    "    def circumflex_regulator(cls, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize circumflex letters in the name, e.g., 'é' becomes 'e'.\n",
    "        If name is not a string, return an empty string.\n",
    "        \"\"\"\n",
    "        if not isinstance(name, str) or pd.isna(name):\n",
    "            return ''\n",
    "\n",
    "        # Replace circumflex letters using pre-compiled regex pairs\n",
    "        for regex_, format_character in cls.CIRCUMFLEX_REGEX_PAIRS:\n",
    "            name = regex_.sub(format_character, name)\n",
    "        return name\n",
    "\n",
    "\n",
    "class CompanyMatcher:\n",
    "    \"\"\"\n",
    "    This class handles company name matching using various techniques like tokenization, symbol regulation,\n",
    "    and name normalization.\n",
    "    \"\"\"\n",
    "    # Define common words for both English and Chinese\n",
    "    COMPANY_COMMON_WORDS = {\n",
    "        \"en\": {\n",
    "            \"laboratory\", \"laboratories\", \"lab\", \"labs\", \"research\", \"technology\", \"technologies\",\n",
    "            \"technical\", \"tech\", \"sci\", \"science\"\n",
    "        },\n",
    "        \"zh\": {\n",
    "            \"实验室\", \"研究院\", \"技术\", \"科技\"\n",
    "        }\n",
    "    }\n",
    "    COMPANY_KEYWORDS_REGEX = re.compile(\n",
    "        r'\\b(bank|college|university|education|energy|finance|army|air foce|navy|industry|'\n",
    "        r'hospital|hotel|institutes?|institution|petroleum|oil|health|electronic|commercial|'\n",
    "        r'environmental|gover(?:nor?|ment)|state|estado|procter|reliance|genome|software|hardware|agency|'\n",
    "        r'(?:sou|nor)th(?:[-\\s]?(?:ea|we)st(?:ern)?)?|[东西南北]方|[东西][南北][方]?|环境|'\n",
    "        r'银行|软件|硬件|大学|机构|研究院|学院|教育|协会|能源|金融|旅馆|医院|石油|电子|工业|[陆海空]军|政府|基因|'\n",
    "        r'global|market(?:ing)?|real estate|system|房地产|系统|兴业)\\b',\n",
    "        re.IGNORECASE)\n",
    "\n",
    "    # Compile regex pattern for matching common words in company names\n",
    "    COMPANY_COMMON_WORDS_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join(COMPANY_COMMON_WORDS['zh']),\n",
    "            rf\"(\\b|^)({'|'.join(COMPANY_COMMON_WORDS['en'])})(\\.|\\b|$)\"\n",
    "        ), re.I\n",
    "    )\n",
    "    # Define ranges for CJK (Chinese, Japanese, Korean) characters\n",
    "    CJK_CHARACTERS = r'\\u1100-\\u11ff\\u2e80-\\u2fff\\u3040-\\u31ff\\u3400-\\u9fff\\ua960-\\ua97f\\uac00-\\ud7ff\\uf900-\\ufaff'\n",
    "\n",
    "    # Regex to match CJK characters or numeric patterns\n",
    "    CJK_OR_NUMERIC_REGEX = re.compile(\n",
    "        rf\"(?P<cjk>[{CJK_CHARACTERS}]+)|(?P<numeric>((?<=^\\D)|(?<=[^\\W0-9_]|\\s))(?<!\\b[a-zA-Z])(\\d+([\\W_]{{0,5}}\\d+){{0,5}})(?=($|[^\\W0-9_]|\\s)))\"\n",
    "    )\n",
    "\n",
    "    WHITESPACE_REGEX = re.compile(r'\\s+')\n",
    "\n",
    "    # Common prepositions\n",
    "    PREPOSITION_WORDS = {\n",
    "        'aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'anti', 'around', 'as',\n",
    "        'at',\n",
    "        'before', 'behind', 'below', 'beneath', 'beside', 'besides', 'between', 'beyond', 'but', 'by',\n",
    "        'concerning', 'considering', 'despite', 'down', 'during', 'except', 'excepting', 'excluding', 'following',\n",
    "        'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite',\n",
    "        'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'since', 'than', 'through', 'till', 'to',\n",
    "        'toward', 'towards', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within',\n",
    "        'without'\n",
    "    }\n",
    "\n",
    "    # Common conjunctions\n",
    "    CONJUNCTIONS_WORDS = {\n",
    "        \"although\", \"because\", \"before\", \"both\", \"and\", '&', \"whether\", \"or\", \"either\", \"neither\", \"nor\", \"once\",\n",
    "        \"just\", \"so\", \"as\", \"if\", \"then\", \"rather\", \"than\", \"till\", 'when', 'where', 'whenever', 'while', 'wherever',\n",
    "        \"such\", \"so\", \"that\"\n",
    "    }\n",
    "\n",
    "    # Common articles\n",
    "    ARTICLES = {'a', 'an', 'the'}\n",
    "\n",
    "    # Merge all irrelevant words (prepositions, conjunctions, articles, etc.)\n",
    "    INLINE_WORDS = {\n",
    "                       'also', 'am', 'are', 'did', 'furthermore', 'has', 'hence', 'how', 'however', 'includ.',\n",
    "                       'instead', 'is', 'likewise',\n",
    "                       'long', 'moreover', 'should', 'similar', 'though', 'thus', 'unless', 'was', 'were', 'what',\n",
    "                       'which', 'whichever',\n",
    "                       'why', 'will',\n",
    "                       # Portugese prepositions\n",
    "                       'de', 'di', 'em', 'del', 'des', 'do',\n",
    "                       # 意大利语介词\n",
    "                       'delle',\n",
    "                       # Spanish prepositions\n",
    "                       'con', 'sobre', 'en', 'contra', 'desde', 'entre', 'hacia', 'por', 'la',\n",
    "                       # French prepositions\n",
    "                       'apres', 'avant', 'avec', 'chez', 'contre', 'dans', 'depuis', 'derriere', 'devant', 'durant',\n",
    "                       'envers', 'environ',\n",
    "                       'jusque', 'malgre', 'par', 'parmi', 'pendant', 'pour', 'sans', 'selon', 'sous', 'suivant', 'sur',\n",
    "                       'vers'\n",
    "                   } | PREPOSITION_WORDS | CONJUNCTIONS_WORDS | ARTICLES\n",
    "\n",
    "    # Common suffixes in company names (e.g. Ltd, Company, etc.)\n",
    "    ORG_SUFFIX_WORDS = {\n",
    "        'zh': {'common': ('公司', '集团', '有限公司', '有限责任公司', '股份有限公司'),\n",
    "               'uncommon': ('总公司', '股份', '控股', '责任', '有限', '企业', '协会', '合作社', '株式会社')},\n",
    "        'en': {'common': ('company', 'group', 'corporation', 'incorporated', 'enterprise', 'enterprises',\n",
    "                          'co', 'inc', 'corp', 'ltd', 'llc', 'se', 'pvt'),\n",
    "               'uncommon': ('corporation limited', 'companies', 'worldwide', 'limited', \"holding\", \"holdings\",\n",
    "                            'com', 'gmbh', 'ag', 'plc', 'sal', 'spa', r's\\.p\\.a', 'sab cv', 'sa', 'nv', r'n\\.v', 'lp',\n",
    "                            'sro',\n",
    "                            'kg', 'aktiengesellschaft', 'de cv', 'ltda', \"group of companies\")}\n",
    "    }\n",
    "    REGEX_COMPANY_DROP_INVALID = re.compile(r'[~$()@—－（）√《》:;：；.、•·‧・，*?+&|,/\\[\\\\]|{}\\t\\n\"]|([-]+\\s|\\s[-]+)')\n",
    "    # Compile regular expressions for matching company suffixes\n",
    "    ORG_SUFFIX_REGEX = re.compile(\n",
    "        r\"{0}|{1}\".format(\n",
    "            '|'.join([rf'{suf}' for suf in ORG_SUFFIX_WORDS['zh']['uncommon'] + ORG_SUFFIX_WORDS['zh']['common']]),\n",
    "            rf\"(?:\\b(?:{'|'.join([suf for suf in ORG_SUFFIX_WORDS['en']['uncommon'] + ORG_SUFFIX_WORDS['en']['common']])})(?:\\.|$))\"\n",
    "        ),\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    def __init__(self, company_data_file: str):\n",
    "        self.company_data_file = company_data_file\n",
    "        self.matcher = NameMatcher(remove_ascii=False, punctuations=False)\n",
    "        self.matching_data = None\n",
    "\n",
    "    def load_company_data(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load the company data from a JSON Lines (.jsonl) file into a pandas DataFrame.\n",
    "        Ensures aliases and required search strings are properly formatted.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: DataFrame containing company data including names and aliases.\n",
    "        \"\"\"\n",
    "        companies = []\n",
    "        with open(self.company_data_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                company = json.loads(line)\n",
    "                company['aliases'] = company.get('aliases', [])  # 确保别名存在\n",
    "                companies.append(company)\n",
    "        return pd.DataFrame(companies)\n",
    "\n",
    "    def tokenize(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Tokenize the input text using Jieba for Chinese words and return the tokenized string.\n",
    "\n",
    "        Parameters:\n",
    "        - text (str): The text to tokenize (usually the company name).\n",
    "\n",
    "        Returns:\n",
    "        - str: Tokenized text.\n",
    "        \"\"\"\n",
    "        # Use jieba to segment the text and concatenate the results with spaces into a string and return it\n",
    "        return \" \".join(jieba.cut(text))\n",
    "\n",
    "    def search_string_regulator(self, name: str, return_str=True, is_company_name=False) -> str:\n",
    "        \"\"\"\n",
    "        Regulate the search string to remove meaningless words and symbols. Handles both Chinese and English.\n",
    "\n",
    "        :param name: The company name or string to regulate.\n",
    "        :param return_str: If True, return the result as a string; otherwise, return as a list of words.\n",
    "        :param is_company_name: Special flag to avoid removing certain common words for company names.\n",
    "        \"\"\"\n",
    "        # Remove meaningless parts of the company name\n",
    "        extracted_name = re.sub(r'[^\\w\\s]', '', name)  # Adjust this regex as per your needs\n",
    "        if not extracted_name:\n",
    "            return '' if return_str else []\n",
    "\n",
    "        words = []\n",
    "        # Split based on spaces or known separators\n",
    "        for segment in extracted_name.split():\n",
    "            if not segment:\n",
    "                continue\n",
    "            # If the segment is Chinese, treat it as a whole word\n",
    "            if self.contains_chinese(segment):\n",
    "                words.append(segment)\n",
    "            # If it's a meaningful English word, keep it\n",
    "            else:\n",
    "                segment = segment.lower()\n",
    "                if segment not in words or is_company_name:\n",
    "                    words.append(segment)\n",
    "\n",
    "        # Return as a single string or a list\n",
    "        return ' '.join(words) if return_str else words\n",
    "\n",
    "    def regulate_english_asians_mixed_string(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        This function handles the regulation of mixed English and Asian (CJK) strings.\n",
    "        It splits the input string into English, numbers, and CJK segments, then rejoins them.\n",
    "        \"\"\"\n",
    "        # 调用 split_english_number_cjk 方法并传入正确的参数\n",
    "        return ' '.join(self.split_english_number_cjk(text, separate_return=False))\n",
    "\n",
    "    def split_english_number_cjk(self, text: str, separate_return: bool = True):\n",
    "        \"\"\"\n",
    "        A utility function that splits text into English, numbers, and CJK (Chinese, Japanese, Korean) characters.\n",
    "        This is a basic version to simulate the behavior.\n",
    "        \"\"\"\n",
    "        import re\n",
    "        CJK_REGEX = r'[\\u4e00-\\u9fff\\uf900-\\ufaff]+'  # Simplified regex to match CJK characters.\n",
    "        EN_NUM_REGEX = r'[a-zA-Z0-9]+'\n",
    "\n",
    "        cjk_parts = re.findall(CJK_REGEX, text)\n",
    "        en_num_parts = re.findall(EN_NUM_REGEX, text)\n",
    "\n",
    "        if separate_return:\n",
    "            return en_num_parts, cjk_parts  # Return both parts separately\n",
    "        else:\n",
    "            return en_num_parts + cjk_parts  # Concatenate the parts if not separating\n",
    "\n",
    "    def prepare_matching_data(self, companies: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Prepare data for matching, ensuring aliases and required search strings are correctly loaded as individual entries.\n",
    "        \"\"\"\n",
    "        rows = []\n",
    "        for _, company in companies.iterrows():\n",
    "            # Process required search strings, ensure it's a list\n",
    "            required_search_strings = company.get(\"requiredSearchStrings\", [])\n",
    "            if not isinstance(required_search_strings, list):\n",
    "                required_search_strings = []  # If it's not a list, convert to empty list\n",
    "\n",
    "            # Ensure aliases are also a list\n",
    "            aliases = company.get(\"aliases\", [])\n",
    "            if not isinstance(aliases, list):\n",
    "                aliases = []  # If it's not a list, convert to empty list\n",
    "\n",
    "            # Combine required search strings and aliases into one list\n",
    "            full_alias_list = required_search_strings + aliases\n",
    "\n",
    "            # For each alias, create a separate row with all aliases as alias_name\n",
    "            for alias in full_alias_list:\n",
    "                rows.append({\n",
    "                    \"name\": alias,\n",
    "                    \"companyName\": company.get(\"companyName\", \"\"),\n",
    "                    \"alias_name\": full_alias_list,  # Store the full alias list\n",
    "                    \"is_alias\": alias in aliases  # Mark if this is an alias\n",
    "                })\n",
    "\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    def prepare_data_for_matching(self, companies: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Prepare the company data for matching by tokenizing the names and loading them into the matcher.\n",
    "        \"\"\"\n",
    "        companies['name'] = companies['name'].apply(self.tokenize)  # Tokenize company names\n",
    "        self.matcher.load_and_process_master_data(column=\"name\", df_matching_data=companies)\n",
    "\n",
    "    def match_user_input(self, user_input: str, expected_name: str, threshold: int = 70) -> str:\n",
    "        \"\"\"\n",
    "        Match user input against the company names, then validate against expected name.\n",
    "        \"\"\"\n",
    "\n",
    "        # Input is NaN or non-string, convert it to an empty string\n",
    "        if not isinstance(user_input, str) or pd.isna(user_input):\n",
    "            user_input = ''\n",
    "\n",
    "        # Call interpunction_regulator to process the symbols entered by the user\n",
    "        user_input = self.interpunction_regulator(user_input)  # 处理符号\n",
    "        if not user_input:\n",
    "            return ''\n",
    "\n",
    "        # Normalize and tokenize the input\n",
    "        user_input_segmented = self.company_words_regulator(user_input)\n",
    "        user_input_segmented = \" \".join(jieba.cut(user_input_segmented))\n",
    "        to_be_matched = pd.DataFrame({\"name\": [user_input_segmented]})\n",
    "\n",
    "        # Matches the name entered by the user\n",
    "        result = self.matcher.match_names(to_be_matched, column_matching=\"name\")\n",
    "\n",
    "        # No match, returns an empty string\n",
    "        if result.empty:\n",
    "            return ''\n",
    "\n",
    "        # Get the best matching results\n",
    "        best_match = result.iloc[0][\"match_name\"]\n",
    "        match_score = result.iloc[0][\"score\"]\n",
    "\n",
    "        # If the match score exceeds the threshold, it is considered a successful match.\n",
    "        if match_score >= threshold:\n",
    "            # 通过 best_match 查找公司名称和别名\n",
    "            matched_row = self.matching_data[self.matching_data['name'] == best_match]\n",
    "            if matched_row.empty:\n",
    "                return ''\n",
    "\n",
    "            # 获取公司名称和别名信息\n",
    "            company_name = matched_row.iloc[0]['companyName']\n",
    "            aliases = matched_row.iloc[0]['alias_name']  # 这现在是一个包含所有别名的列表\n",
    "\n",
    "            # 处理 expected_name，防止 NaN 导致错误\n",
    "            if pd.isna(expected_name):\n",
    "                expected_name = \"\"  # 如果是 NaN，替换为空字符串\n",
    "            else:\n",
    "                expected_name = expected_name.lower()\n",
    "\n",
    "            # 检查 expected_name 是否在别名中\n",
    "            if expected_name and expected_name in [alias.lower() for alias in aliases]:\n",
    "                return expected_name\n",
    "\n",
    "            # 如果 expected_name 和 company_name 匹配，返回公司名称\n",
    "            if expected_name == company_name.lower():\n",
    "                return company_name\n",
    "\n",
    "            # 如果没有匹配到 expected_name，默认返回公司名称\n",
    "            return company_name\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    @staticmethod\n",
    "    def contains_chinese(text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the input text contains any Chinese characters.\n",
    "        \"\"\"\n",
    "        CHINESE_CHARACTERS = r'\\u2e80-\\u2fff\\u31c0-\\u31ef\\u3400-\\u9fff\\uf900-\\ufaff'\n",
    "        return bool(re.search(f\"[{CHINESE_CHARACTERS}]\", text))\n",
    "\n",
    "    def company_words_regulator(self, company_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean up and regulate the company name by applying normalization rules for circumflexes, symbols,\n",
    "        removing unnecessary company suffixes, and common words.\n",
    "        \"\"\"\n",
    "        # Step 1: Normalize circumflex letters and process interpunction\n",
    "        search_str = self.interpunction_regulator(NameNormalizer.circumflex_regulator(company_name))\n",
    "\n",
    "        if not search_str:\n",
    "            return ''\n",
    "\n",
    "        # Step 2: Check if the string contains Chinese and process accordingly\n",
    "        if self.contains_chinese(search_str):\n",
    "            if not re.search(r\"[a-zA-Z]\", search_str):\n",
    "                search_str = search_str.replace(' ', '')\n",
    "\n",
    "        # Step 3: Remove company suffixes like 'inc', 'corp', etc., potentially twice to handle two occurrences\n",
    "        search_str_drop = self.WHITESPACE_REGEX.sub(' ', self.ORG_SUFFIX_REGEX.sub('', self.ORG_SUFFIX_REGEX.sub('',\n",
    "                                                                                                                 search_str).strip())).strip()\n",
    "\n",
    "        # The string becomes empty after removing the suffixes, return an empty string\n",
    "        if not search_str_drop:\n",
    "            return ''\n",
    "\n",
    "        # Step 4: Remove stock exchange abbreviations, e.g., 'SSE plc', 'NYSE', 'NASDAQ'\n",
    "        search_str_drop_bourse = self.COMPANY_COMMON_WORDS_REGEX.sub(' ', search_str_drop).strip()\n",
    "        if search_str_drop_bourse:\n",
    "            search_str_drop = search_str_drop_bourse\n",
    "\n",
    "        # Step 5: Remove common company words like 'technology', 'research', etc.\n",
    "        search_str_drop_common_word = self.WHITESPACE_REGEX.sub(' ', self.COMPANY_COMMON_WORDS_REGEX.sub('',\n",
    "                                                                                                         search_str_drop)).strip()\n",
    "        if search_str_drop_common_word:\n",
    "            search_str_drop = search_str_drop_common_word\n",
    "\n",
    "        # Step 6: Ensure that removing common words doesn't leave just a keyword, e.g., 'Corporation Bank'\n",
    "        if (r_ := self.COMPANY_KEYWORDS_REGEX.search(search_str_drop)) and r_.end() - r_.start() > len(\n",
    "                search_str_drop) - 2:\n",
    "            pass\n",
    "        elif search_str == search_str_drop:\n",
    "            pass\n",
    "        else:\n",
    "            search_str = search_str_drop\n",
    "\n",
    "        # Step 7: Further regulate the search string and convert to lowercase\n",
    "        search_str_regulate = self.search_string_regulator(search_str, is_company_name=True).lower()\n",
    "        if search_str_regulate != search_str:\n",
    "            search_str = search_str_regulate\n",
    "\n",
    "        # Step 8: Remove invalid characters and filter valid words\n",
    "        valid_words = []\n",
    "        for word_ in search_str.split(' '):\n",
    "            word_ = self.REGEX_COMPANY_DROP_INVALID.sub(' ', word_).strip().lower()\n",
    "            if word_:\n",
    "                valid_words.append(word_)\n",
    "\n",
    "        # Return the cleaned and processed company name\n",
    "        return ' '.join(valid_words)\n",
    "\n",
    "    # Direct deletion symbols: '-' and '/'\n",
    "    ERASE_SYMBOLS = re.compile(r'[-/]')\n",
    "\n",
    "\n",
    "\n",
    "    def interpunction_regulator(self, name: str) -> str:\n",
    "        \"\"\"\n",
    "        Regulates punctuation and symbols in the company name by:\n",
    "        - Removing or replacing certain symbols with spaces.\n",
    "        - Replacing special characters like '&' and '@' with words ('togetherwith', 'locatedat').\n",
    "        - Handling special cases like sequential single letters and 'the' followed by a short word.\n",
    "        \"\"\"\n",
    "        is_chinese = self.contains_chinese(name)\n",
    "\n",
    "        if not is_chinese:\n",
    "            # If name contains specific symbols, split and take the first word for further matching\n",
    "            if name_words := re.split(r'[|]+', name):\n",
    "                name = name_words[0]\n",
    "\n",
    "        # Replace '&' with 'togetherwith' and '@' with 'locatedat'\n",
    "        if and_match := re.search(r'&', name):\n",
    "            name = name.replace('&', 'togetherwith')\n",
    "        if at_match := re.search(r'@', name):\n",
    "            name = name.replace('@', 'locatedat')\n",
    "\n",
    "        # Replace specific symbols with spaces\n",
    "        name = REPLACE_TO_SPACE_SYMBOLS.sub(' ', name)\n",
    "\n",
    "        # Use ERASE_OR_REPLACE_TO_SPACE_SYMBOLS instead of ERASE_SYMBOLS\n",
    "        if is_chinese:\n",
    "            name = ERASE_OR_REPLACE_TO_SPACE_SYMBOLS.sub(' ', name).strip()\n",
    "        else:\n",
    "            name = ERASE_OR_REPLACE_TO_SPACE_SYMBOLS.sub('', name).strip()\n",
    "\n",
    "        # Handle special cases for names starting with 'the' followed by a short word\n",
    "        if the_match := THE_SPECIAL_ENTITY_NAME_REGEX.search(name):\n",
    "            connect_str = the_match.group(1)\n",
    "            name = name.replace(connect_str, re.sub(r'[\\s]+', 'thespecial', connect_str))\n",
    "\n",
    "        # Merge sequential single letters into one\n",
    "        before_merge_name = name\n",
    "        name = ''\n",
    "        while True:\n",
    "            if not (single_letter_sequential := SINGLE_LETTER_SEQUENTIAL_REGEX.search(before_merge_name)):\n",
    "                name += before_merge_name.strip()\n",
    "                break\n",
    "            name += before_merge_name[\n",
    "                    :single_letter_sequential.start()].strip() + ' ' + single_letter_sequential.group().replace(' ',\n",
    "                                                                                                                '') + ' '\n",
    "            before_merge_name = before_merge_name[single_letter_sequential.end():]\n",
    "\n",
    "        # Drop single lowercase letters, but not if they are at the start or end\n",
    "        name = SINGLE_LOWERCASE_REGEX.sub(' ', name)\n",
    "\n",
    "        if is_chinese:\n",
    "            name = self.regulate_english_asians_mixed_string(name)\n",
    "\n",
    "        return name if name else None\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to initialize CompanyMatcher, load company data, and perform company name matching.\n",
    "    \"\"\"\n",
    "    # Initialize CompanyMatcher and pass in the path to the company data file\n",
    "    company_matcher = CompanyMatcher('bd_companies_international.jsonl')\n",
    "\n",
    "    # Load Company Data\n",
    "    companies = company_matcher.load_company_data()\n",
    "\n",
    "    # Prepare matching data and organize company name and alias information into a format that can be matched\n",
    "    matching_data = company_matcher.prepare_matching_data(companies)\n",
    "    company_matcher.matching_data = matching_data\n",
    "\n",
    "    # Prepare company data for matching (e.g. word segmentation)\n",
    "    company_matcher.prepare_data_for_matching(matching_data)\n",
    "\n",
    "    # Get input from the user and match the company name\n",
    "    user_input = input(\"Enter a company name for matching: \")\n",
    "\n",
    "    # Using a matcher to perform company name matching\n",
    "    matched_company = company_matcher.match_user_input(user_input, expected_name=None)\n",
    "\n",
    "    if matched_company:\n",
    "        print(f\"Matched company: {matched_company}\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c90d362-6333-4c0c-8e6b-effbe51212d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#V10B/V11B graph for precision vs. recall\n",
    "\"\"\"x is recall\n",
    "   y is precision\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting Precision vs Recall\n",
    "data = pd.read_csv('calculate_score.csv')\n",
    "\n",
    "# 绘制 Precision vs Recall 的图\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(data['Recall'], data['Precision'], marker='o')\n",
    "\n",
    "# 添加坐标轴标签和标题\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs Recall')\n",
    "\n",
    "# 显示网格\n",
    "plt.grid(True)\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
